<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[反向传播算法（BP）]]></title>
    <url>%2F2019%2F06%2F22%2F%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%2F</url>
    <content type="text"><![CDATA[人工智能领域的算法真是日新月异啊，最近CMU和Google Brain又提出了XLNet。这篇博文还是从基础算法入手，介绍一下Back Propagation（简称BP），主要分为两个部分：反向传播的基本原理和RNN的反向传播算法，最后给出代码实现。 Back Propagation $tag$之前介绍过牛顿迭代法：当我们想要优化一个目标函数时，需要在每次迭代的时候对变量/参数进行更新，而更新参数的最重要的部分就是求目标函数对参数的导数了。下面对最简单的神经网络进行BP算法的公式推导：下图是一个只有两个神经元的网络（图片修改自李弘毅老师的PPT），输入样本为一个二维的向量\vec x，输出为一个数y，现在假设loss函数是l=|| y - \hat y ||_2^2，那么要求的便是\frac {\partial l}{\partial w_i} 以及\frac {\partial l}{\partial b} （把w和b合并成一个向量W也是可以的）。 图中，\sigma表示sigmiod函数。 首先思考一下，这个导数如果让你计算应该怎么做呢？ 因为loss函数里与w, b参数相关的就只有y了，而y由z' 得出，z' 由线性函数wx+b得到…那自然会想到用chain rule来求导了。 整理一下思路： y=\sigma(z') \\ z'=w_{21}a+b_2 \\ a=\sigma(z) \\ z=w_{11}x_1+w_{12}x_2+b_1 \tag {1}那么求导如下： \frac {\partial l}{\partial y} = 2(y-\hat y) \\ \frac {\partial y}{\partial w_{11}} = \frac {\partial y}{\partial z'} \cdot \frac {\partial z'}{\partial a} \cdot \frac {\partial a}{\partial z} \cdot \frac {\partial z}{\partial w_{11}} = \sigma(z')[1-\sigma(z')]\cdot w_{21} \cdot \sigma(z)[1-\sigma(z)] \cdot x_1 \\ \frac {\partial y}{\partial b_1} = \frac {\partial y}{\partial z'} \cdot \frac {\partial z'}{\partial a} \cdot \frac {\partial a}{\partial z} \cdot \frac {\partial z}{\partial b_1} = \sigma(z')[1-\sigma(z')]\cdot w_{21} \cdot \sigma(z)[1-\sigma(z)] \cdot 1 \tag{2}这么求解貌似是可以的，however, 没有任何技巧可言，当网络结构有变化的时候，算法变的异常难写，比如：一层多个神经元的情况，两层之间非全连接的情况，某些神经元共享参数的情况等，想想都可怕，想要写个general的算法几乎不可能，另外还有计算量需要被考虑。 还好聪明人总是有的，考虑到l对w和对b求导过程其实是一样的，下面就仅介绍y对w的BP求导过程。 对于上面的问题，先只看第一层神经元的参数更新（其实对于任意层任意神经元都有下式的关系）： \frac {\partial l}{\partial w} = \frac {\partial l}{\partial z} \frac {\partial z}{\partial w}接下来分别计算\frac {\partial l}{\partial z}和\frac {\partial z}{\partial w}:正向计算： \frac {\partial z}{\partial w_{11}} = x_1 \\ \frac {\partial z}{\partial w_{12}} = x_2 \tag {3}从式（3）可以看出正向计算有个非常好的性质，x_1, x_1就是神经元的值，这都已经在正向传播时计算过了！ 反向计算： \frac {\partial l}{\partial z} = \frac {\partial l}{\partial a} \frac {\partial a}{\partial z} \tag {4}这其中，\frac {\partial a}{\partial z}很简单，sigmiod求导就ok了，而且\sigma(x)'=\sigma(x)[1-\sigma(x)]，nice，又可以用现成的结果了。 而对于\frac {\partial l}{\partial a}的计算比较棘手了，因为： \frac {\partial l}{\partial a} =\frac {\partial l}{\partial z'} \frac {\partial z'}{\partial a} = \frac {\partial l}{\partial z'}\cdot w_{21} \tag {5}其中w_{21}就是神经元间的连接权重参数，对于比较复杂的网络，计算\frac {\partial l}{\partial a} 可以进行递归计算。如果被计算的\frac {\partial l}{\partial z'}是神经网络的最后一层（output layer），问题就变的简单了： \frac {\partial l}{\partial z'}= \frac {\partial l}{\partial y} \frac {\partial y}{\partial z'} \tag{6}结束了，， \frac {\partial l}{\partial z} = \sigma(z)'[\frac {\partial l}{\partial z'}\cdot w_{21}]再看一下反向计算\frac {\partial l}{\partial z}：因为\frac {\partial a}{\partial z}比较好求，是一个常数，sigmoid函数在该神经元上的求导就得到了；\frac {\partial l}{\partial a}呢则可以使用递归方法，只要求出output layer的\frac {\partial l}{\partial z'}就ok。从下图来看，就是input为\frac {\partial l}{\partial z'}，然后按照同样的网络反方向计算一次，和正向传播的不同之处仅仅在于sigmoid（非线性转换/激活函数）变成先求导再相乘，结构清晰，计算量也大幅度下降。 最后总结一下：正向传播和反向传播 相乘就可以计算出所有的参数更新梯度。 插句话：神经网络中常用的优化算法是随机梯度下降法（SGD），实践中常用的是adam优化器（一种自适应步长/学习率的优化算法），理论上梯度下降做自适应步长也很简单，不过需要求Hessian矩阵，而对于上千万甚至上亿个参数的目标函数来说，计算量实在是太大了。 代码实现这里对基础版的反向传播算法进行代码实现，代码来自这里，你也可以点击这里进行查看。核心代码如下： 123456789101112131415161718192021222324252627def feedforward(self, x): # 正向计算 # return the feedforward value for x a = np.copy(x) z_s = [] a_s = [a] for i in range(len(self.weights)): activation_function = self.getActivationFunction(self.activations[i]) z_s.append(self.weights[i].dot(a) + self.biases[i]) a = activation_function(z_s[-1]) a_s.append(a) return (z_s, a_s)def backpropagation(self,y, z_s, a_s): # 反向计算 dw = [] # dl/dW db = [] # dl/db deltas = [None] * len(self.weights) # delta = dl/dz known as error for each layer # insert the last layer error deltas[-1] = ((y-a_s[-1])*(self.getDerivitiveActivationFunction(self.activations[-1]))(z_s[-1])) # Perform BackPropagation for i in reversed(range(len(deltas)-1)): deltas[i] = self.weights[i+1].T.dot(deltas[i+1])*(self.getDerivitiveActivationFunction(self.activations[i])(z_s[i])) #a= [print(d.shape) for d in deltas] batch_size = y.shape[1] db = [d.dot(np.ones((batch_size,1)))/float(batch_size) for d in deltas] dw = [d.dot(a_s[i].T)/float(batch_size) for i,d in enumerate(deltas)] # return the derivitives respect to weight matrix and biases return dw, db RNN怎么做BPRNN的结构和前馈神经网络有很大不同，所以反向传播和上述也略有不同，这里要介绍的算法叫做Backpropagation Through Time (BPTT)。这里假设读者对RNN、LSTM、GRU等算法已经有所了解了，就提纲挈领的介绍一下BPTT特别之处。 主要的不同点还是要看公式，RNN的在t时刻输出神经元的值可以用下式表示：a_{k,t} = \sigma (W_{k}^{h}a_{k,t-1} + W_{k}^{x}a_{k-1,t} +b_{k})式中W_{k}^{h}与上一时刻的计算结果相关 所以\frac {\partial l}{\partial W_{k}^{h}}的计算算是新内容，其他的和之前说的没有什么不一样 总结反向传播算法1974年被提出，不过当时没有受到重视，2010年开始，得益于该算法和GPU算力的提升，人工智能开始迅速发展。 Reference Werbos, P. (1974). Beyond Regression:” New Tools for Prediction and Analysis in the Behavioral Sciences. Ph. D. dissertation, Harvard University. https://medium.com/@a.mirzaei69/implement-a-neural-network-from-scratch-with-python-numpy-backpropagation-e82b70caa9bb https://www.youtube.com/watch?v=UTojaCzMoOs]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分层softmax]]></title>
    <url>%2F2019%2F06%2F15%2F%E5%88%86%E5%B1%82softmax%2F</url>
    <content type="text"><![CDATA[入坑自然语言处理，论文word2vec Parameter Learning Explained基本是必读的，这篇论文中的Hierarchical Softmax，中文叫做分层softmax/层次softmax是比较让人头大的内容，这篇博文试图阐述Hierarchical Softmax算法在word2vec中的应用。 Huffman Tree中文名叫霍夫曼树/霍夫曼编码，是个二叉树（注意不是二叉搜索树），这部分内容比较简单，维基百科上也说的非常清楚，下面搬运一下维基百科上的例子：示例：霍夫曼树常处理符号编写工作。根据整组数据中符号出现的频率高低，决定如何给符号编码。如果符号出现的频率越高，则给符号的码越短，相反符号的号码越长。假设我们要给一个英文单字”F O R G E T”进行霍夫曼编码，而每个英文字母出现的频率分别列在Fig.1。 演算过程： （一）进行霍夫曼编码前，我们先创建一个霍夫曼树。 将每个英文字母依照出现频率由小排到大，最小在左，如Fig.1。 每个字母都代表一个终端节点（叶节点），比较F.O.R.G.E.T六个字母中每个字母的出现频率，将最小的两个字母频率相加合成一个新的节点。如Fig.2所示，发现F与O的频率最小，故相加2+3=5。 比较5.R.G.E.T，发现R与G的频率最小，故相加4+4=8。 比较5.8.E.T，发现5与E的频率最小，故相加5+5=10。 比较8.10.T，发现8与T的频率最小，故相加8+7=15。 最后剩10.15，没有可以比较的对象，相加10+15=25。 最后产生的树状图就是霍夫曼树，参考Fig.2。 （二）进行编码 给霍夫曼树的所有左链接’0’与右链接’1’。 从树根至树叶依序记录所有字母的编码，如Fig.3。 以上便是Huffman Tree的主要内容，在word2vec算法中，这个方法是用来替代softmax层来减少计算量的。至此，需要了解到的信息有以下几点：1.最后树的输出FOERGT是不要特定排序的，排列成FORGET也是可以的，就是画图不是很方便；2.在word2vec中，这里的字母就是单词了，如果单词出现的频率越高，则给单词的码越短（离根节点越近），相反单词的号码越长；3.构建Huffman Tree的中间节点（5, 8, 10, 15, 25）的个数是字典中单词个数减1 softmax in word2vecword2vec Parameter Learning Explained这篇论文中介绍了 Continuous Bag-of-Word Model（连续词袋模型）和skip-gram model（跳字模型），分别对应了词向量的两种训练方法：利用context预测中心词以及利用中心词去预测context。对于连续词袋模型（CBOW）来说，一般的做法（如下图所示）是先对每个单词进行one-of-N编码（one-hot encoded），作为训练网络的输入，接着构建一层hidden layer，最后构建输出层，这一层是一个softmax层，每个context单词到中心单词的事件都被认为是独立的，所以将这些事件发生的概率相乘，最后构建损失函数，即：将输出概率分布和实际选中的词概率分布进行Corss Entropy计算，接下来使用SGD对参数进行更新。这里，hidden layer的训练结果就是最终的word vector了。 需要注意的是：对于任意的单词，Input layer和Hidden Layer之间的权重矩阵W是参数共享的 上述方法看起来是没毛病的，问题是计算量有点大，尤其是进行反向传播更新参数的时候$t$： \frac {\partial \log P(\left. w_c, \right |w_{o_1},w_{o_2},...,w_{o_{2m}})}{\partial v_{o_i}} =\frac {1}{2m}( \vec u_c - \sum_{j \in V}P(\left. w_j \right |w_c)\vec u_j ) \tag {1}式（1）说明，参数更新的时候，对于每一个单词每一次迭代都至少有O|V|的计算量，如此大的计算量是由于softmax引用了词典中的所有单词。 在skip-gram模型中也是一样的： \frac {\partial \log P(\left. w_o \right |w_c)}{\partial v_c} = \vec u_o - \sum_{j \in V}P(\left. w_j \right |w_c)\vec u_j 至此应该了解到：1.浅层的网络就可以学习出来词向量；2.W矩阵对于不同的单词是参数共享的，单词顺序发生变化的时候是不影响结果的；3.参数更新时的计算量非常大。 Hierarchical Softmax in word2vec为了减少计算量，作者提出了两种近似计算方法，第一种叫做Negative Sampling（负采样），该方法就是对词典中的特定属性的单词进行特定分布的采样，将计算的数据量降低了（详见论文）；第二种就是Hierarchical Softmax（分层softmax/层次softmax），该方法将softmax层替换成了分层softmax层。分层softmax的计算过程如下图所示： 图片来自这里 从图中可以看出，hidden layer到output layer的连接原本是一个简单的softmax，有V个神经元和所有的hidden layer两两连接，现在变成了一个树，有V-1个神经元和所有的hidden layer两两连接。计算概率的方法也发生了变化： P(w|w_i)=\prod_{j=1}^{L(w)-1} \sigma ([n(w,j+1) = left\_child(n(w,j)] \cdot \vec U^T_{n(w,j) } \vec V_i) \tag {2}其中，当n(w,j+1) = left\_child(n(w,j)时，中括号内为1，否则为-1，这是用到了一个sigmoid函数的小trick：1-\sigma (x)=\sigma(-x)。所以式（2）的意思就是从根节点到目标单词，有且仅有一条路径可以到达，在这条路径上往左走的概率是\sigma(\vec U^T_{n(w,j) } \vec V_i)，往右走的概论自然就是\sigma(-\vec U^T_{n(w,j) } \vec V_i)，逻辑回归那篇也介绍过，sigmoid函数是用来做二分类的，在这里正好合适；当路径上的所有二分类的概率都连乘后，得到的就是预测单词的概率，可以证明，词典中所有单词被预测到的概率和为1。这也是这个方法被叫做分层softmax的原因了。 如此一来，计算某个单词被预测的概率就仅仅和该单词到hidden layer的神经元连接的唯一路径相关了，更新参数的时候计算量一下子降到了O(log(V))。 仔细想一下，Hierarchical Softmax和CNN的思想其实有点类似，把原本的全连接变成了部分的特定连接。 关于这方面的源码编写可以参考这个美国老哥的博客。 Reference Rong, X. (2014). word2vec parameter learning explained. arXiv preprint arXiv:1411.2738. Morin, F., &amp; Bengio, Y. (2005, January). Hierarchical probabilistic neural network language model. In Aistats (Vol. 5, pp. 246-252). https://www.youtube.com/watch?v=C4X0Cb5_FSo&amp;list=PLLbeS1kM6teJqdFzw1ICHfa4a1y0hg8Ax&amp;index=17&amp;t=0s https://learning.oreilly.com/library/view/python-natural-language/9781787121423/a63263c0-bd79-4c15-88d0-8898186e03a5.xhtml https://zhuanlan.zhihu.com/p/35074402 http://www.trevorsimonton.com/blog/2016/12/15/huffman-tree-in-word2vec.html https://www.quora.com/What-is-hierarchical-softmax]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>深度学习</tag>
        <tag>人工智能</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于矩阵分解的推荐算法]]></title>
    <url>%2F2019%2F06%2F10%2FMatrix-Factorization%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Matrix Factorization算法是推荐系统（Recommendation System）的基础，本篇文章仅介绍一下基于矩阵分解的推荐系统是如何工作的以及Matrix Factorization算法，最后给出一个算法示例。内容比较浅显，深入算法原理还需要阅读更多的论文和资料。 推荐系统简介推荐系统是信息过滤系统的子集，系统任务便是预测用户接下来更可能想要查看的内容，也就是将特定的信息过滤出来提交给用户。这类算法主要应用于商业应用，比如：歌曲推荐、视频推荐、新闻推荐等。推荐系统可以由很多算法实现，而基于用户/基于商品的Collaborative filtering是比较简单直接的。 数据源举个线下的例子：抛开算法，可以先思考一下，超市的售货员阿姨是怎么给你推荐商品的呢？你是学生吗？买回去给谁用？需要买什么价位的？这类问题经常被问及…然后，售货员便根据用户信息来推荐你可能需要的商品了：我们这有xxx，价格xxx，好多学生都买…售货员的信息则是从大量的市场调研/工作经验中学习到的商品信息。 所以，一个推荐系统应该会同时包含用户信息和商品信息，回到线上，当很多用户在某个视频网站上观看电影后，会对电影打个分，除此之外，用户还有年龄、性别、行业、家庭关系等很多信息，这些都是用户信息；而视频本身也有自己的分类，比如：喜剧、动作等，此外还有票房、受众人群等信息，这些便组成了商品信息。 在推荐系统中，这些信息交织在一起，存放于一个大矩阵中，内容为用户对于每一个商品的评价打分，如下图所示（图片来自维基百科）： 需要注意的是，很多人是不点赞、不评论甚至对某些商品完全没有听说过，所以这个数据矩阵是一个巨大的稀疏矩阵，里面很多空白。将这些评价量化后，便得到一个数据源矩阵了。而推荐系统要做的便是从该矩阵中提取信息，为用户/新用户提供items推荐服务。 至此，应该了解到：1.推荐系统基于大量的用户数据；2.数据是一个大型的稀疏矩阵，每一行/列表示某一用户（user）对所有商品（item）的打分信息，中间有很多很多空白处待填写。 算法策略有了数据源矩阵之后，接下来要做的就是在矩阵空白的地方填写数字了，数字代表用户对某一个item的喜好程度，从而推荐用户可能打分比较高的item；如何在空白处填上合理的数字是基于矩阵分解方法的核心。下图示意了算法：$tag$ 图片来自这里 如上图所示，将数据源矩阵分解成用户矩阵W_{u \times k}和商品矩阵W_{k \times I}，得到了用户A对每一个feature的喜好程度；又有每一个item对每一个feature的权重/偏置。那就可以重新构建一个新的没有空白的数据矩阵了，也就是将原来的空白地方进行了填充。但是这里还需要一个约束，根据用户矩阵和item矩阵计算得到的新的被填满的数据矩阵和原来有很多空白的数据源矩阵一定要基本一致（原有数据不能失真）。 需要注意的是，feature的数量k一般是少于user和itme数量的，所以最终如果仅存储用户矩阵和item矩阵，那会比存储数据源矩阵要节省很多存储空间。 Matrix Factorization现在目标就很明确了，将大型稀疏矩阵分解成两个小矩阵，一个是user矩阵，一个是item矩阵。 矩阵分解算法有很多，使用场景也不相同，例如：QR分解在进行迭代计算时常常被使用；SVD分解在进行矩阵降维度，数据去燥方面用的比较多 算法原理和步骤目标： \bf D_{U \times I} \approx User_{u \times k} \times Item_{k \times I} = \hat D_{U \times I}理论上，应该有类似QR、SVD分解这样的方法，去直接求解上式，考虑到上式是一个大型稀疏矩阵，常见的做法是直接使用梯度下降优化算法： 给定初始值：矩阵U和I，设定迭代次数和误差范围； 计算User_{u \times k} \times Item_{k \times I} = \hat D_{U \times I} 计算二范数距离|| D_{U \times I}- \hat D_{U \times I}||_2=e^2，这里加上正则化项也是极好的； 计算梯度\frac {\partial e^2}{\partial U} 以及 \frac {\partial e^2}{\partial I} 更新参数U'=U+\alpha \cdot \frac {\partial e^2}{\partial U}以及I'=I+\alpha \cdot \frac {\partial e^2}{\partial I} python示例点击这里即可查看，或者这里。 总结 为用户生成推荐内容； Matrix Factorization可以节省存储空间； 最重要的是数据处理。 Reference Covington, P., Adams, J., &amp; Sargin, E. (2016, September). Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems (pp. 191-198). ACM. http://www.albertauyeung.com/post/python-matrix-factorization/ Thai-Nghe, N., Nhut-Tu, M., &amp; Nguyen, H. H. (2017, April). An Approach for Multi-Relational Data Context in Recommender Systems. In Asian Conference on Intelligent Information and Database Systems (pp. 709-720). Springer, Cham. https://www.youtube.com/watch?v=ZspR5PZemcs&amp;list=LLoCiddVQCg9ZvZoEa5d5wYw&amp;index=23&amp;t=4s https://www.youtube.com/watch?v=o8PiWO8C3zs]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>推荐系统</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic Regression]]></title>
    <url>%2F2019%2F06%2F09%2FLogistic-Regression%2F</url>
    <content type="text"><![CDATA[对数几率回归/逻辑回归/逻辑斯蒂回归/最大熵模型也即Logistic Regression是深度学习的基础，算法的重要性不言而喻。Logistic Regression虽然叫“Rgression”，但其实与之前介绍的SVM分类（svc）方法一样，同属分类算法。本篇博文对该算法的介绍流程基本参考了李宏毅老师的机器学习课程，文章后半部分主要以问答的形式给出了关于逻辑斯蒂回归的部分理解。 基础知识回顾$ $ Bernoulli distribution伯努利分布/两点分布/0-1分布是一个非常简单的概率模型，事件只有两种，常见于二分类问题，其概率密度函数（PDF）如下： f_X(x)=p^x(1-p)^{1-x}=\begin{cases} p, & \text {if x=1,} \\ q, & \text{if x=0.} \end{cases} \tag{1}sigmoid 函数及其导函数Sigmoid 函数意思就是S型函数，因为长相就是S形的，公式如图左上角所示，下图蓝色表示sigmoid函数，绿色表示其导函数。sigmiod在学习机器学习和深度学习时会时常遇到。 softmax函数softmax可以将一组数字转换成一组概率，并且突出较大数值的优势（使其在概率上有很大的值），如下图所示： 公式推导概率模型对于二分类问题，假设现在有一笔training data（下表中C_1表示数据x^i属于Class 1，C_2表示数据x^i属于Class 2）： 数据 x^1 x^2 x^3 …… x^N 类别 C_1 C_1 C_2 …… C_1 现已知一个新数据x，我们可以构造一个概率密度函数（PDF）来表示x属于C_1的概率P(\left. C_1 \right |x)： f_{w,b}(x) = \sigma (z) = P_{w,z}(\left. C_1 \right |x)\\ z=\sum_iw_ix_i+b \tag{2}这里引入的w_i, b参数和sigmoid函数\sigma(z)并不是凭空捏造出来的，实际上，是可以根据生成模型推导出这个公式的，这部分的内容可以参考这里。如此一来，假设“x_i属于C_1或C_2”这件事情和“x_j属于C_1或C_2”这件事情是相互独立的，那么就可以得出，training data中的所有事件全部发生的概率为： L(w,b) = f_{w,b}(x^1) \cdot f_{w,b}(x^2) \cdot (1-f_{w,b}(x^3)) …… f_{w,b}(x^N) \tag{3} 对于二分类问题，由于x_3属于C_2，所以P_{w,z}(\left. C_2 \right |x^3)=1-P_{w,z}(\left. C_1 \right |x^3)也即1-f_{w,b}(x^3)； 实际数据往往是离散的，这里使用概率密度函数（PDF）也许不准确。 最大似然估计（Maximum Likelihood）为了让PDF可以表示真实数据的distribution，也即让f_{w,b}(x)最为准确，我们希望计算得到的L(w,b)尽可能大，当然，最大值就是1了。接下来目标便是找到一组w^*, b^*使得： \underset{\vec w,b}{\operatorname{arg max}} L(w,b) \tag{4}为了方便（Mathmatic Convenient），将上式改写： \underset{\vec w,b}{\operatorname{arg min}} { -ln[L(w,b)] } \tag{5}并且引入参数\hat y（也是关于x的一个两点分布）： \hat y = \begin{cases} 1, \space \space if \space x\in C_1 \\ 0, \space \space if \space x\in C_2 \end{cases} \tag{6}结合式(5)和式(6)，有： -\ln{f_{w,b}(x^n)}= \hat y^n \ln{f_{w,b}(x^n)} + (1-\hat y^n) \ln{(1-f_{w,b}(x^n))} \tag{7}代入式(5)，得 -ln[L(w,b)] = - \sum_n [ \hat y^n \ln{f_{w,b}(x^n)} + (1-\hat y^n) \ln{(1-f_{w,b}(x^n)}) ] \tag{8}式(8)的右边便是两个二项分布的Cross Entropy，也就是logistic regression的loss function/object function接下来要做的便是对式(8)进行优化/求最小值。 交叉熵（Cross Entropy and KL-divergence)熵：在信息论中（information theroy）离散概率分布函数的熵为 H(p) = - \sum_i{p_i \log{p_i}} \tag{9}KL-divergence: 为了衡量两个离散分布函数的相似度，如果使用传统的二范数距离或者向量的夹角距离，显然不合适，这里定义 D_{KL}(\left. p \right |q) = \sum_i{p_i \log{\frac{p_i}{q_i}}} \tag{10}来衡量两个离散分布函数有多接近/相似，注意D_{KL}(\left. p \right |q) \ne D_{KL}(\left. q \right |p)，当D_{KL}(\left. p \right |q) =0时，p同q分布。Cross Entropy: H(p,q) = H(p)+D_{KL}(\left. p \right |q)=\sum_i{p_i \log{\frac {1}{q_i}}}= -\sum_i{p_i \log{q_i}} \tag{11}所以对应到式(8)中，对于\forall x^n，有： p=\begin{cases} \hat y^n, \space \space \space \space \space \space \space for \space x \in C_1 \\ 1-\hat y^n, \space for \space x \in C_2 \end{cases}, \space \space \space q=\begin{cases} f_{w,b}(x^n), \space \space \space \space \space \space \space for \space x \in C_1 \\ 1-f_{w,b}(x^n), \space for \space x \in C_2 \end{cases} \tag{12}-ln[L(w,b)] = \sum_n H(p,q) = \sum_n H(\hat y^n,f(x^n)) \tag{13}优化有了多个Corss Entropy求和作为loss，接下来便是对目标函数/损失函数进行求解，求解目标是使该函数取最小值，这里可以使用Gradient Descent方法，牛顿迭代法或者其他启发式优化算法理论上也是可以的。 从二分类到多分类上面的推导都是基于binary classification的，如果要进行multiple classification，就需要对上面的情况进行扩展了。这里先给出做法（how），关于sigmoid和softmax分析请继续往下看。 首先\vec x不再是只做一次“\vec w \cdot \vec x + b”变换了，而是做n次，n就是需要分类的类别数，目标的分布也不再是0-1这样，而是多维的0-1分布，比如：对于三分类问题，x^n \in C_1，则\hat y^n=[1,0,0]，而计算概率的函数也由sigmiod函数转变为softmax函数。 方法局限性（limitation）逻辑回归的局限性在于，分类时分割线只是线性的（直线、平面或者超平面），所以对于非线性分类问题，需要另辟蹊径。到这里，有些同学可能有疑问，明明使用了sigmoid或者softmax函数处理仿射变换结果，怎么会还是线性的？？？ 首先，判断一个函数/系统是不是线性的，可以从输入和输出入手来探究，如图一所示，样本x如果增大，显然z1, z2, z3也会成比例增大，而Softmax和Sigmoid在这里的作用仅仅是将结果转换成0-1之间的数值（概率），所以它还是一个线性分类的场景。 非线性可分的解法对于非线性分类问题，可以采用类似于SVM的kernel方法，先对样本做feature map，再进行分类，但是这个map需要依靠人类的智慧去找了，具体的例子可以查看李宏毅的机器学习课。 问答&amp;理解为什么使用Cross Entropy计算loss？这里的机器学习，关键部分可以理解成学习样本数据的分布，由于本质上是使用最大似然方法，要求最终得到的概率密度函数（PDF）可以给出一个新的数据属于某一类的可能性。而对于两个分布的相似度衡量，Cross Entropy目前来看应该还是不二之选。 用Square Error作为Loss行不行？可以，不过训练过程将极其痛苦，按照李宏毅老师课程中所述，使用传统的二范数距离来度量两个分布的相似度的损失函数将会非常平坦，使用梯度下降算法进行优化时，往往不太可能得出比较好的结果。而使用Cross Entropy所得到的损失函数则具有较大的梯度，搜索速度将会比使用Square Error要快很多。 为什么叫Regression？对比Linear Regression方法的计算过程，发现Logistics Regression与其一模一样，包括模型参数的更新公式，形式上都是一样的，虽然Logistics Regression是用来所分类的，还是叫逻辑回归比较”亲切”。 和信息论（information theory）的关系？以上的推导过程都是基于概率的方法，使用了最大似然估计，但是这也完全可以从Information Theory的角度推导出逻辑回归的损失函数，所以，信息论只是认识这个问题的另外一个角度。 和Generation Model/生成模型的关系逻辑回归是一个判别模型（Discrimination Model），我觉得和生成模型比，它是一个不好进行数学解释的模型。实际上，从生成模型到判别模型，参数被抽样化了，底层所依赖的数学理论也同样被抽象化了，一旦模型复杂了，判别模型就基本解释不了了。一般来说，生成模型会对现有数据做一些假设并对样本数据内的信息进行抽象总结，从而给出模型对样本数据的新认识（样本数据里面没有的信息），而判别模型则一般仅会对样本数据之内的信息进行挖掘。所以，对于样本数足够多的时候，使用判别模型一般会由于使用生成模型。 sigmoid V.S. softmax前面已经介绍过，从二分类到多分类，计算概率的函数就会从sigmoid转换为softmax，但实际上softmax就是sigmoid的“扩展板”。如果对于二分类问题也使用softmax函数，可以推导出概率模型和sigmoid给出的一模一样。 Logistic Regression V.S. Neural Network到这里，其实已经很清楚了，DNN中的每一个神经元（Neural）就是一个sigmoid形式的逻辑回归，而对于softmax形式的逻辑回归，它其实就是一个没有hidden layer的NN，一层神经网络。此外，之前讨论的非线性分类问题的解法，就是对原始样本数据先做特征映射，而NN算法在softmax之前都可以看成一个比较复杂的特征映射，并且这个映射的参数是自学习的。这样，也顺便解释了NN算法好于逻辑回归，并对非线性问题有很好的解决。 总结 介绍了逻辑回归算法的推导过程 说明了cross entropy的适用场景 对比了sigmoid和softmax函数 问答的形式给出了一些对逻辑回归算法的理解 Reference https://www.youtube.com/watch?v=hSXFuypLukA&amp;list=LLoCiddVQCg9ZvZoEa5d5wYw&amp;index=5&amp;t=0s https://www.youtube.com/watch?v=fZAZUYEeIMg&amp;list=LLoCiddVQCg9ZvZoEa5d5wYw&amp;index=1 cross-entropy-and-kl-divergence]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>逻辑回归算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s 对外服务之ingress]]></title>
    <url>%2F2019%2F06%2F03%2Fk8s-%E5%AF%B9%E5%A4%96%E6%9C%8D%E5%8A%A1%E4%B9%8Bingress%2F</url>
    <content type="text"><![CDATA[本文首先回顾了一下负载均衡和反向代理，随后介绍ingress，这部分的内容主要参考了YouTube上印度老哥的频道：KodeKloud的视频，直接看视频效果更佳。 7层负载均衡负载均衡（LB）在微服务架构演进中具有非常重要的意义，可以说的内容有很多，这里仅仅讨论四层和七层负载均衡的一些要点和区别，以便于对ingress的理解。所谓四层和七层负载均衡是按照网络层次OSI来划分的负载均衡类型（也可以按照其他的规则来分类，比如：应用的地理结构），简单来说：四层负载均衡表示负载均衡器用ip+port接收请求，再直接转发到后端对应的服务上，工作在传输层( transport layer )；七层负载均衡表示负载均衡器根据虚拟的url或主机名来接收请求，经过处理后再转向相应的后端服务上，工作在应用层( application layer )。 下图表示了4层和7层负载均衡在建立TCP连接上的区别，从图中可以看出，四层负载均衡需要建立的TCP连接其实之有一个，它只做一次转发，client直接和server连接；而7层负载均衡则需要建立两次TCP连接，client到LB，LB根据消息中的内容( 比如URL或者cookie中的信息 )来做出负载均衡的决定，接着建立LB到server的连接。 7层负载均衡有什么好处呢？ 因为存在解包/封包的过程，比4层LB更加CPU‑intensive，但是却极少降低性能； 可以编写更加智能的负载均衡策略，比如根据URL、cookie中的信息等，甚至对接收到的内容做一些优化和修改，比如加密、压缩； 使用buffer的方式来缓解服务器连接慢的问题，从而提高性能 具有7层负载均衡功能的设备通常也被称为反向代理服务器（reverse‑proxy server） 反向代理举个例子：正向代理：在大陆使用VPS访问Google的时候，通常会使用一个本地的代理服务器，浏览器的网络包会先经过本地的代理服务器，代理服务器会通过远在异国它乡的电脑来访问Google并返回消息；这就好比去附近的咖啡店要先问一下手机咖啡店在哪里一样，手机就是一个正向代理服务器。反向代理：当访问的请求到达Google时，Google那边也设置了一个代理服务器，它通过查看请求的URL，发现是想查找视频内容，于时把消息转给了视频搜索服务器（过程是我乱说的），这就好比你去朋友家做客，开门的却是个管家，问你找谁？这时候管家就是一个反向代理了。关于反向代理的好处这里就不多介绍，感兴趣可以看这里 其他OSI层也可以做反向代理 ingressk8s 对外暴露服务（service）主要有两种方式：NotePort, LoadBalance， 此外externalIPs也可以使各类service对外提供服务，但是当集群服务很多的时候，NodePort方式最大的缺点是会占用很多集群机器的端口；LB方式最大的缺点则是每个service一个LB又有点浪费和麻烦，并且需要k8s之外的支持； 而ingress则只需要一个NodePort或者一个LB就可以满足所有service对外服务的需求。 实际上，ingress相当于一个7层的负载均衡器，是k8s对反向代理的一个抽象。大概的工作原理也确实类似于Nginx，可以理解成在 Ingress 里建立一个个映射规则 , ingress Controller 通过监听 Ingress这个api对象里的配置规则并转化成 Nginx 的配置（kubernetes声明式API和控制循环） , 然后对外部提供服务。ingress包括：ingress controller和ingress resources ingress controller：核心是一个deployment，实现方式有很多，比如nginx, Contour, Haproxy, trafik, Istio，需要编写的yaml有：Deployment, Service, ConfigMap, ServiceAccount（Auth），其中service的类型可以是NodePort或者LoadBalancer。 ingress resources：这个就是一个类型为Ingress的k8s api对象了，这部分则是面向开发人员。 假设已经有两个服务部署在了k8s集群内部：12345678$ kubectl get svc,deployNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEsvc/coffee-svc ClusterIP &lt;none&gt; &lt;none&gt; 80/TCP 1msvc/tea-svc ClusterIP &lt;none&gt; &lt;none&gt; 80/TCP 1mNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEdeploy/coffee 2 2 2 2 1mdeploy/tea 1 1 1 1 1m 配置 Ingress resources，即可实现多个service对外暴露服务:方式一：1234567891011121314151617181920apiVersion: extensions/v1beta1kind: Ingressmetadata: name: cafe-ingressspec: rules: # 配置七层域名 - host: foo.bar.com http: paths: # 配置Context Path - path: /tea backend: serviceName: tea-svc servicePort: 80 # 配置Context Path - path: /coffee backend: serviceName: coffee-svc servicePort: 80 接着在hosts文件中添加一条解析规则：${ingress_IP} foo.bar.com，这时通过在浏览器中访问：foo.bar.com/coffee或者foo.bar.com/tea即可访问对应的后端service了。 使用curl时的操作：curl -H &quot;Host: foo.bar.com&quot; http://${ingress_IP}/coffee 方式二：12345678910111213141516171819apiVersion: extensions/v1beta1kind: Ingressmetadata: name: cafe-ingressspec: rules: # 配置七层域名 - host: tea.foo.bar.com http: paths: - backend: serviceName: tea-svc servicePort: 80 - host: coffee.foo.bar.com http: paths: - backend: serviceName: coffee-svc servicePort: 80 这时在hosts文件添加两条解析规则就可以在浏览器中访问了。此外，还可以配置TLS证书实现HTTPS访问，这里不再详述。 前面提到ingress controller的实现有很多方案，除了常用的nginx这类负载均衡器，一些网络插件也会集成相关功能。比较有代表性的如NSX-T，vmware的一款重量级虚拟化网络产品。NSX-T的ncp组件在部署的时候只要在yaml文件中设置default_ingress_class_nsx = True就OK了，并可以选择指定一个external_ip_pools。 Reference YouTube视频 阿里云 https://www.cnblogs.com/danbing/p/7459224.html https://www.nginx.com/resources/glossary/layer-7-load-balancing/ https://www.nginx.com/resources/glossary/reverse-proxy-vs-load-balancer/ https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0]]></content>
      <categories>
        <category>容器云kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>ingress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM调参之股票预测]]></title>
    <url>%2F2019%2F05%2F19%2FSVM%E8%B0%83%E5%8F%82%E4%B9%8B%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[上篇博客讲解了SVM的算法精髓，这篇来牛刀小试一回，主要内容包括：1) 利用SVM分类来预测上证指数的涨跌；2) 试试SVM调参。 预测上证指数涨跌问题描述预测股票涨跌和预测指数涨跌的原理是一样的，都是利用“历史数据”来推测未来的走势。股票数据单纯来看也就是时间序列。这里我们利用分类方法来构建预测模型： 首先，获取过去长时间内上证指数的数据 计算这些数据的一些特征（features），比如：sma、wma、mom等 利用特征数据构建训练数据（train dataset）x_train，保留部分数据作为测试集（test dataset） 为数据打标签y_train：如果交易日当天的收盘价高于上一个交易日的收盘价，则为“+”数据，标签为“+1”，否则为“-1” 利用svm对上述带有标签的数据进行分类 给定测试数据，计算其特征数据，利用分类模型对其进行分类 对比分类结果（“+1”对应“涨”，“-1”对应“跌”）和实际数据的涨跌情况，若一致则说明分类准确/预测准确 这里需要注意的是x_train和y_train的对应关系至少要有一个交易日的时间差，否则，模型将毫无意义。 写代码有了想法后，写代码就比较简单了，这里参考了优矿上一个SVM代码。代码最中主要的部分就是数据处理部分，如下所示：（点击源代码可以查看完整的源码文件） Tips:如果github上无法直接查看ipython notebook，可以点击这里将所要查看的文件的URL拷贝进入即可查看。 123456789101112131415161718for index in range(2,len(close_pri)): # 取数据[-2]表示使用的特征是由今天之前的数据计算得到的 sma_data = talib.SMA(close_pri[:index],timeperiod=7)[-2] wma_data = talib.WMA(close_pri[:index],timeperiod=7)[-2] mom_data = talib.MOM(close_pri[:index],timeperiod=7)[-2] features = [] features.append(sma_data) features.append(wma_data) features.append(mom_data) x_train.append(features) # 对今天的交易进行打标签，涨则标记1，跌则标记-1 if close_pri[index-1] &lt; close_pri[index]: label = 1 else: label = -1 y_train.append(label) SVM调参原本计划以此为例，测试一下不同的参数得到的分类结果，最终会对预测模型的泛化能力产生什么样的影响，这里改成参数sklearn中svm分类算法的参数简介了。 首先来看一下sklearn中对svm分类问题给出的目标函数/loss sklearn中给出的svc公式如下，对比上篇文章的式（18），可以发现，这里的参数$C$是式（18）的倒数： \min_{w,b,\zeta} \frac{1}{2}w^Tw+C\sum_{i=1}^{n} \zeta_i \\ subject to y_i(w^T\phi(x_i)+b) \ge 1-\zeta_i, \\ \zeta_i \ge 0, i=1,...n这里理解起来类似于反演问题的正则化方法，整个目标函数可以看作是一个罚函数/penalty function，C可以看作是惩罚因子。那么，C越大，表示要求惩罚项越小，the vise versa，这里则表示分类越准确，距离是不是最大的稍微有那么一丢丢不那么重要。可以想象如果C是正无穷，则分类准确才是最最重要的，gutter的宽度已经不重要了，这时的模型必然会过拟合。这么一来，还需要分析的参数只有核函数的参数了。sklearn官方的参数如下（额，这么多，，），下面挑几个重要的来看一下：1234SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) decision_function_shape：‘ovo’ OR ‘ovr’，对于多分类问题，ovo表示两两做分类，ovr表示，其中一个类和其他所有做分类degree：就是多项式核函数里面的degree了max_iter：最大迭代次数，针对SMO算法tol：迭代总会有终止条件的gamma：rbf核函数的参数，gamma越大，表示精度越高，因为高斯函数会越高瘦嘛；过拟合也就越严重了，therefore，如果高斯核都搞不定的分类，那就别用svm了吧 安装ta-lib 安装ta-lib时遇到一点点坑，记录一下。 按照官方ta-lib的方法先安装依赖：1234wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gztar -vxf ta-lib-0.4.0-src.tar.gzcd ta-lib/./configure --prefix=/usr # 此处是选取lib文件存放位置 试一下：ipython312In [1]: import talibIn [2]: 如果导入模块报错：libta_lib.so.0: cannot open shared object file: No such file or directory，则考虑是/usr/lib/下的的库文件没有被加载到系统环境变量内，导致导入模块的时候没有找到库文件libta_lib.so.0，添加环境变量即可：1echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib' &gt;&gt; ~/.bashrc 总结 实现了用svm做二分类来预测上证指数 对sklearn的SVC参数做了一定的分析 ta-lib这个模块在CentOS上安装时可能出现一些问题 Reference 优矿SVM教程 sklearn-SVM参数说明]]></content>
      <categories>
        <category>人工智能</category>
        <category>量化投资</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>quant</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机（SVM）]]></title>
    <url>%2F2019%2F05%2F10%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%2F</url>
    <content type="text"><![CDATA[4月份容器云项目投产，天天加班，人也变懒也变胖了，好在项目投产还算顺利，就不吐槽托节奏的队友了。如今NN算法基本可以解决SVM能解决的所有问题，但是学习SVM还是有必要滴这篇文章是SVM算法的开篇，准备先介绍原理和简单的推导，展示出SVM的核心内容及理解，下一篇再以量化交易作为实战来看一下调参的过程。 开篇的公式推导是少不了的，首先需要掌握一些基本的数学知识： 点到直线的距离计算； 向量A到向量B的投影长度计算； 拉格朗日对偶。 线性可分支持向量机现在有两类数据（这里假设是二维数据点），分布在二维平面上，如图1所示，以符号+代表一类数据，-代表另外一类数据。我们要做的事情就是画一条线将这两类数据分开来，而画线的原则是使距离这条线（图中虚线所示）最近的任意数据点到虚线的距离最大，或者说要让图中黑色实线间的宽度最大。需要说明的是，目前这里只考虑线性可分的情况。 我们使用以下公式来判断数据点是否落在+分类中，该式也被称为Decision Rule \vec{w} \cdot \vec{u} + b \ge 0, \text {THEN +} \tag{1}式中$\vec{w}$表示垂直于分隔线（图中虚线）的向量，\vec{u}表示任意数据点的向量表示(unknown)，b表示一个与分割线相关的变量，所以，我们只要确定了\vec w和b就可以据此确定任意一个数据点属于哪个分类了。 这里可以回顾一下解析几何中点到直线的距离公式以及推导过程，方便理解公式。 接下来，我们规定，+数据代入Decision Rule得到的值大于1，而-数据代入Decision Rule得到的值小于-1，这个是可以做到的并且对于之后的操作是有好处的： \begin{cases} \vec{w} \cdot \vec{x_+} + b \ge +1, & \text {+ sample} \\ \vec{w} \cdot \vec{x_-} + b \le -1, & \text{- sample} \end{cases} \tag{2}mathematica convenient 引入变量： \begin{cases} y_i=+1, & \text for \space x_+ \\ y_i=-1, & \text for \space x_- \end{cases} \tag{3}这标记数据其实挺有道理的，另外，数学表达上也更为便利， \begin{cases} y_i(\vec{w} \cdot \vec{x_+} + b) \ge 1 \\ y_i(\vec{w} \cdot \vec{x_-} + b) \ge 1 \end{cases} \tag{4}哎呀～，+数据和-数据表达式上统一了，这就是mathematica convenient了：这里需要说明一下，当数据点恰好落在gutter上（图1中的实黑线）时，有：y_i(\vec{w} \cdot \vec{x_i} + b) = 1 , \space for \space x_i \space in \space the \space gutter \tag{5} 下面，我们想要让两条黑色实线之间的宽度（width of street）越大越好（seperate the samples as wide as possible.）如图1所示，计算宽度：距离分割线最近的+点用向量\vec x_+来表示，距离分割线最近的-点用向量\vec x_-来表示，那么宽度的计算就可以用向量\lbrace \vec x_+-\vec x_- \rbrace在向量\vec w上的投影来表示了，注意\vec w是垂直于分割线的向量，所以： WIDTH = (\vec x_+ - \vec x_-) \cdot \frac {\vec w}{||w||} \tag{6}其中，由公式(5)得到\vec x_+ \cdot \vec w = 1-b 而 - \vec x_- \cdot \vec w = b+1，所以： WIDTH = \frac {2}{||w||} \tag{7}得到了宽度的表达式后，要做的事情就变成了使宽度最大化： max {\frac {2}{||w||}} \to min \space ||w|| \to min \space \frac{1}{2}||w||_2^2 \tag{8}此时问题已经转化为： \begin{cases} min \space \frac{1}{2}||w||_2^2 \\ y_i(\vec{w} \cdot \vec{x_i} + b) = 1, & \text for \space for \space x_i \space in \space the \space gutter \\ y_i(\vec{w} \cdot \vec{x_i} + b) > 1, & \text for \space other \space x_i \end{cases} \tag{9}上述问题的优化可以使用拉格朗日乘数法，定义L: L(\vec w,\vec b,\vec \alpha)=\frac{1}{2}||w||_2^2-\sum_{i=1}^m\alpha_i [y_i(\vec{w} \cdot \vec{x_i} + b) - 1] \tag{10} 有些材料里面还会引入\beta项，似乎看起来没有必要。此外，为啥约束条件是被减去而不是加上呢，可以思考一下 令： \frac {\partial L}{\partial w} = \vec w - \sum_i \alpha_iy_i\vec x_i =0 \implies \vec w = \sum_i \alpha_iy_i\vec x_i \\ \frac {\partial L}{\partial b} = -\sum_i\alpha_iy_i=0 \implies \sum_i\alpha_iy_i=0 \tag{11}这式(11)说明两点信息：1. \vec w只和个别数据相关，因为\alpha_i可能为0；2. \alpha_i之间是有约束的，加权和为0。 将式(11)代回式(10)，得到拉格朗日对偶问题： L(\vec \alpha)=\frac{1}{2}(\sum_i \alpha_iy_i\vec x_i)(\sum_j\alpha_jy_j\vec x_j)-\sum_i \alpha_iy_i\vec x_i\cdot(\sum_i \alpha_jy_j\vec x_j)-\sum_i \alpha_iy_ib + \sum_i \alpha_i \tag{12}下标i,j仅仅是为了区分，式(12)化简之后得到： L(\vec \alpha)=\sum_i \alpha_i- \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j \vec x_i \vec x_j \tag{13}数学家告诉我们，SVM里拉格朗日对偶是满足强对偶条件的，现在的问题已经转化为求max \space L(\vec \alpha)，并且该问题的解（注意需要满足约束条件）便是原始问题的解。在SVM算法中，式(13)的解法主要为SMO算法，由微软研究员提出。 推导至此，我们应当了解到： 所谓支持向量就是这些对分割线（高维空间里是个超平面）产生影响的数据向量，这些数据点其实就是位于“黑色实线”上的点，或者叫做距离分割线的最近点，其他点的\alpha_i=0，对\vec w的确定没有贡献； 强对偶下，对偶问题的解就是原始问题的解，并且对偶问题始终是一个Concave优化问题，所以SVM的解一定是全局唯一解，不会陷入局部最小值； Decision Rule变为：\sum_i \alpha_iy_i\vec x_i \cdot \vec u +b \ge 0, \text {THEN +} 目标函数和Hinge Loss上一部分介绍的是线性可分的支持向量机的推导，如果按照机器学习的“套路”，很难说清楚目标函数、优化算法这类概念。实际上，既然是二分类问题，那目标函数按理说应该是分类的准确性，优化的目标便是提升准确性。所以接下来稍微重构一下刚刚的问题，并简单介绍一下线性不可分（含有噪声信号）的情况如何使用SVM来做分类。 原问题：最小距离取最大 \underset{\vec w,b}{\operatorname{arg max}} { \left \lbrace \frac {1}{||w||} \underset{i}{min}[y_i \cdot (w^T \cdot \vec x_i+b)] \right \rbrace} \tag{14}现在：分类准确是前提，定义Hinge Loss，“最小距离取最大”反而变成了一个约束项： min \space L(f)=\sum_il(f(\vec x_i),y_i)+C||w||_2^2 , C 是常系数 \\ l(f(\vec x_i),y_i)=max(0,1-y_if(\vec x_i)) \\ f(\vec x_i)=\sum_iw\vec x_i+b=\begin{bmatrix} w \\ b \\ \end{bmatrix} \cdot \begin {bmatrix} x_i \\ 1 \\ \end{bmatrix}=W^TX \tag{15}上式中，l便是目标函数/损失函数的关键，如果分类正确l应该为0，如果分类错误l将会变得很大（如下图蓝色实线所示），优化的方向便是使式(15)取最小值。如下图所示，蓝色实线表示Hinge Loss，绿色实线是理想情况下的loss，可以看到Hinge Loss在分类错误的情况下值会变的很大，而在分类正确的情况下还有部分缓冲（横轴在0.0-1.0之间的部分），这就有个好处了—Hinge Loss允许小部分的错误/非严格分类，一些资料里将这一特征称为惩罚部分，这也使得SVM拥有了天然的抗过拟合能力。 接下来引入松弛因子(slack variable)的概念来换一种表达方式: \xi_i= max(0,1-y_if(\vec x_i)) \tag{16}\begin{cases} \xi_i \ge 0 \\ \xi_i \ge 1-y_if(\vec x_i) \implies y_if(\vec x_i) \ge 1-\xi_i \end{cases} \tag{17}当要求的是\xi_i的最小值时式(16)和(17)是等价的，loss function又可以写成下式，式(17)则是约束条件： min \space L(f)=\sum_i \xi_i+C||w||_2^2 \tag{18}核函数技巧 Dr. 李宏毅说SVM其实就是Hinge Loss+Kernel Trick，我认为Kernel Trick才是使SVM真正被广泛使用的关键所在。 拉格朗日对偶和核函数技巧的关系原问题转换为拉格朗日对偶问题还有一个非常强大的好处，由式(13)可知分割平面仅仅和x_i \cdot x_j相关，那么，将x_i 和 x_j做相同的feature mapping是不会影响分类结果的，于是式(13)可以写成如下形式： L(\vec \alpha)=\sum_i \alpha_i- \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j \Phi(\vec x_i) \cdot \Phi(\vec x_j) \tag{19}两个向量的点乘是可以在某种程度上衡量两个向量的相似度的，这里数学家告诉我们只要满足Mercer’s Theory，就可以设计出一个核函数，对于非唯一的feature mapping函数，都有下面的式子成立： K()=\Phi (x_i) \cdot \Phi(x_j)也即，我们不必要知道具体的函数\Phi(x)，只要将x_i和x_j代入核函数就行了，而这样计算明显比先做mapping再做inner product要快很多（虽然结果是等价的），这便是Kernel Trick。这样做的好处是可以对数据点进行升维，在低维空间中不可分的数据点，在高维空间则是可分的（反正总有办法让数据可分），坏处自然就是模型容易过拟合了。 各类核函数多项式核： K(\vec x_i,\vec x_j)= (\vec x_i \cdot \vec x_j+b)^n径向基函数核（RBF）：维度太高，容易过拟合 K(\vec x_i,\vec x_j)=exp(-\frac {1}{2\sigma^2}||\vec x_i-\vec x_j||_2^2)Sigmiod核：可以理解成只有一个Hidden Layer的NN K(\vec x_i,\vec x_j)=tanh(\vec x_i \cdot \vec x_j)SVM 与 SVR用支持向量机来做回归问题其实也是可以的，不过并不是回归问题的主流方法。 总结 所谓支持向量就是这些对分割线（高维空间里是个超平面）产生影响的数据向量，这些数据点其实就是位于“黑色实线”上的点，或者叫做距离分割线的最近点，其他点的\alpha_i=0，对\vec w的确定没有贡献； 强对偶下，对偶问题的解就是原始问题的解，并且对偶问题始终是一个Concave优化问题，所以SVM的解一定是全局唯一解，不会陷入局部最小值； Decision Rule变为：\sum_i \alpha_iy_i\vec x_i \cdot \vec u +b \ge 0, \text {THEN +} 超平面就是data point的线性组合； SVM拥有了天然的去outliers抗过拟合能力； 线性不可分的问题，可以通过核函数的方法将低维问题进行升维来解决； 比较常用的核函数有多项式核函数以及高斯核函数； SVM也可以解决回归问题，对应SVR。 参考资料 Vapnik V., “The nature of statistical learning theory,” Springer-Verlag, New-York, 1995. Vapnik V., “Statistical learning theory,” John Wiley, New-York, 1998. Vapnik V., “The support vector method of function estimation,” In Nonlinear Modeling: advanced black-box techniques, Suykens https://www.youtube.com/watch?v=QSEPStBgwRQ https://www.youtube.com/watch?v=_PwhiWxHK8o&amp;pbjreload=10]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes网络和CNI简介]]></title>
    <url>%2F2019%2F04%2F25%2Fkubernetes%E7%BD%91%E7%BB%9C%E5%92%8CCNI%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[kubernetes提供了出色的容器编排能力，同时开放了网络、存储、运行时的接口，打造了云原生生态的同时，也顺手一统了容器云的天下。网络是云服务的基础和核心之一，就网络插件而言，市面上就有很多，kubernetes提供了CNI为所有网络插件提供接口。下面简单介绍一下kubernetes的容器网络接口。 Pod网络对于任何kubernetes网络方案，都需要满足以下需求： 每个Pod都拥有一个独立的IP地址，而且假定所有的pod都在一个可以直接连通的、扁平的网络空间中； 用户不需要额外考虑如何建立Pod之间的连接，也不需要考虑将容器端口映射到主机端口等问题； 网络要求： 所有的容器都可以在不用NAT的方式下同别的容器通讯 所有节点都可在不用NAT的方式下同所有容器通讯 容器的地址和其他宿主机和容器看到的地址是同一个地址 Pod网络基本原理容器通过使用linux内核提供的Cgroups和Namespaces技术实现了相互之间的网络、存储等资源的隔离与限制。对于网络，kubernetes项目中的Pod则通过让一组用户容器和pause容器/infra容器共享一个network namespace的方式，使一个Pod内的容器都使用了一个网络栈。而一个 Network Namespace 的网络栈包括：网卡（Network Interface）、回环设备（Loopback Device）、路由表（Routing Table）和 iptables 规则。所以，不难想象，Pod的网络和一台虚拟机的网络栈配置起来实际上是类似的，比如同样需要虚拟网卡，IP、MAC地址等，并且每个Pod都有自己唯一的网络栈。当所有的Pod都有了自己的网络栈后，如果想要连接两个Pod进行通信，则类似于其他任何网络架构，需要配置交换机、路由器等，并为其规划IP，路由等信息。如果是对于物理机，我们可以使用网线、交换机、路由器这些设备进行连接，但在Pod中显然需要其他方式。 刚开始接触容器的时候，觉得这种设计好奇怪，本来进程间通信挺容易的，如今用namespace做隔离，再想办法让隔离的进程进行网络通信…~_~!! Pod网络类型kubernetes Pod的网络方案有很多，最典型的就是Flannel的三种后端实现方式了（UDP、VxLan、host-gw），讨论这些则主要是在关注容器跨主机通信的问题。而这里主要讨论的则是Pod的内部的网卡如何创建，又如何将网络数据包在宿主机和容器之间传递。 图片来自这里 虚拟网桥：创建一个虚拟网卡对（veth pair），一头在容器内，一头在宿主机的root namespace内，并且使用Linux bridge（网桥）或者OpenvSwitch（OVS）来连接两个不同namespace内的网卡对。这样一来，容器内发出的网络数据包，可以通过网桥进入宿主机网络栈，而发往容器的网络数据包也可以经过网桥进入容器。例如，docker项目中的docker0、kubernetes项目中的cni0都是网桥设备。 veth pair有个很好的特性，两张虚拟网卡总是成对的出现，并且，从其中一个”网卡”发出的数据包可以直接出现在与它对应的另一张”网卡”上，有点像物理的”虫洞”嚯 多路复用：如图所示，使用一个中间网络设备，暴露多个虚拟网卡接口，容器网卡都可以接入这个中间设备，并通过mac地址/IP地址来区分packet应该转发给哪一个容器设备。 多路复用：物理上，一根光纤内，可以同时跑很多很多不同频率的光波，这就是多路复用的其中一种实现方式。 硬件交换：还有个“比较直接”的方法就是为每个Pod分配一个虚拟网卡，这样一来，Pod与Pod之间的连接关系就会变的非常清晰，因为近乎物理机之间的通信基础。如今大多数网卡都支持SR-IOV功能，该功能将单一的物理网卡虚拟成多个VF接口，每个VF接口都有单独的虚拟PCIe通道，这些虚拟的PCIe通道共用物理网卡的PCIe通道。 kubernetes CNI工作原理简介CNI是Container Network Interface的缩写，它是一个通用的容器网络插件的k8s网络接口，开源社区里已经有了很多实现容器网络的方案，不同的网络实现方案在k8s内都是以插件调用的形式工作，所以这里需要一个统一的标准接口。如果将k8s的Pod视为一台“虚拟机”，那么网络插件的工作就是管理这台虚拟机的网络栈，包括给这台虚拟机插入网卡、配置路由、IP等；而CNI的工作则是对接网络插件和kubelet容器运行时管理工具（对于docker容器运行时来说实际上是dockershim），主要体现在Pod的创建和删除过程： CNI加载目录/etc/cni/net.d/下的配置文件，比如：10-calico.conflist Pod Create 使用macvlan二进制文件创建网卡 调用dhcp二进制文件获取IP 将网卡放入pod network namespace Pod Delete 调用dhcp二进制文件释放IP 调用macvlan二进制文件删除网卡 结束容器 CNI 配置文件，给CRI使用的，比如dockershim1234567891011121314151617181920212223242526272829root@master8088:~# cat /etc/cni/net.d/10-calico.conflist &#123; "name": "k8s-pod-network", "cniVersion": "0.3.0", "plugins": [ &#123; "type": "calico", "log_level": "info", "datastore_type": "kubernetes", "nodename": "master8088", "mtu": 1500, "ipam": &#123; "type": "host-local", "subnet": "usePodCidr" &#125;, "policy": &#123; "type": "k8s" &#125;, "kubernetes": &#123; "kubeconfig": "/etc/cni/net.d/calico-kubeconfig" &#125; &#125;, &#123; "type": "portmap", "snat": true, "capabilities": &#123;"portMappings": true&#125; &#125; ]&#125; 可执行文件CNI官方维护的插件包括以下几个，对于已经搭建好的k8s，cni插件可以在/opt/cni/bin/文件夹下查看：CNI的基础可执行文件按照功能可以划分为三类：Main插件：创建具体网络设备bridge：网桥设备，连接container和hostipvlan：为容器增加ipvlan网卡loopback：lo设备macvlan：为容器创建一个MAC地址ptp：创建一对Veth Pairvlan：分配一个vlan设备host-device：将已存在的设备移入容器内 IPAM插件：负责分配IP地址dhcp：容器向DHCP服务器发起请求，给Pod发放或回收IP地址host-local：使用预先配置的IP地址段来进行分配static：为容器分配一个静态IPv4/IPv6地址，主要用于debug meta插件：并非单独使用bandwidth：使用Token Bucket Filter（TBF）来限流的插件flannel：flannel网络方案的CNI插件，对应于flannel配置文件portmap：通过iptables配置端口映射sbr：为网卡设置source based routingtuning：通过sysctl调整网络设备参数firewall：通过iptables给容器网络的进出流量进行一系列限制 目前kubernetes项目中的Pod只能加载一个CNI的配置，如果需要一个POD使用多网卡多网络方案是不可以的。 实验实验内容：go安装社区维护的CNI相关插件，在linux主机上创建一个network namespace，再利用安装的网络插件给这个linux network namespace配置好网络。实验方法教程参考的是这里 安装cni插件：12345678mkdir -p go/src/github.com/containernetworking/cd go/src/github.com/containernetworking/git clone https://github.com/containernetworking/plugins.git git clone https://github.com/containernetworking/cni.git cd plugins./build_linux.shcd ../cni/cnitoolgo build cnitool.go linux上创建一个network namespace：1sudo ip netns add Mytest 设置网络参数：vi /etc/cni/net.d/10-myptp.conf123456789101112131415&#123; "cniVersion": "0.3.1", "name": "myptp", "type": "ptp", "ipMasq": true, "ipam": &#123; "type": "host-local", "subnet": "172.16.29.0/24", "routes": [ &#123; "dst": "0.0.0.0/0" &#125; ] &#125;&#125; 模拟给容器配置网络：1sudo CNI_PATH=../../plugins/bin ./cnitool add myptp /var/run/netns/Mytest 返回值12345678910111213141516171819202122232425262728&#123; "cniVersion": "0.3.1", "interfaces": [ &#123; "name": "vethe6180c75", "mac": "ea:d0:00:22:ce:33" &#125;, &#123; "name": "eth0", "mac": "56:20:72:2b:5a:7e", "sandbox": "/var/run/netns/Mytest" &#125; ], "ips": [ &#123; "version": "4", "interface": 1, "address": "172.16.29.4/24", "gateway": "172.16.29.1" &#125; ], "routes": [ &#123; "dst": "0.0.0.0/0" &#125; ], "dns": &#123;&#125;&#125; 测试网络是否可用：123456789$ sudo ip -n Mytest addr1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:003: eth0@if12: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 56:20:72:2b:5a:7e brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.16.29.4/24 brd 172.16.29.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::5420:72ff:fe2b:5a7e/64 scope link valid_lft forever preferred_lft forever 进入namespace，ping一下百度的DNS12345678$ sudo ip netns exec Mytest ping -c 2 180.76.76.76PING 180.76.76.76 (180.76.76.76) 56(84) bytes of data.64 bytes from 180.76.76.76: icmp_seq=1 ttl=49 time=27.4 ms64 bytes from 180.76.76.76: icmp_seq=2 ttl=49 time=26.3 ms--- 180.76.76.76 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1001msrtt min/avg/max/mdev = 26.374/26.916/27.458/0.542 ms 清除：12sudo CNI_PATH=../../plugins/bin ./cnitool del myptp /var/run/netns/Mytest`sudo ip netns del Mytest 总结 容器网络实现了容器间、容器和宿主机间、容器和服务间的通信； CNI主要提供了容器运行时和网络插件之间的协作； CNI主要以插件调用的方式工作。 自己实现一个网络方案，除了需要实现网络本身（例如：flanneld），还需要该网络对应的CNI插件（例如：flannel）。 最后多说一句，所谓的云服务其实就是基于网络的服务，好好规划网络可以充分利用数据中心的资源，只有充分利用数据中心的资源，才能称之为云计算。 参考资料 https://github.com/containernetworking/cni/tree/master/cnitool https://thenewstack.io/hackers-guide-kubernetes-networking/ http://www.cnblogs.com/YaoDD/p/7419383.html https://github.com/containernetworking/plugins https://time.geekbang.org/column/article/64948 https://www.zhihu.com/question/24950671]]></content>
      <categories>
        <category>容器云kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>k8s网络插件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s关键组件及其高可用方案]]></title>
    <url>%2F2019%2F04%2F18%2Fk8s%E5%85%B3%E9%94%AE%E7%BB%84%E4%BB%B6%E5%8F%8A%E5%85%B6%E9%AB%98%E5%8F%AF%E7%94%A8%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[如今，业务上云已经不是什么新鲜事物了，容器云也已在大大小小的公司实现落地，而kubernetes（k8s）也已登上容器编排调度的霸主地位… 如何搭建高可用k8s集群如今，业务上云已经不是什么新鲜事物了，容器云也已在大大小小的公司实现落地，而kubernetes（k8s）也已登上容器编排调度的霸主地位。越来越多的企业开始部署k8s集群，无论是业务应用还是中间件服务，甚至分布式计算任务，k8s集群都能轻松驾驭，然而，企业场景下稳定并且高可用的k8s集群部署也并非毫无挑战，这篇文章就根据笔者参与的企业级容器云建设项目，分享几点高可用k8s集群搭建的经验。 何谓高可用高可用对于底层的IT基础设施来说是基本要求，这意味着云基础设施三个方面的需求： 容错：即使出了一些错误（无论是天灾还是人祸），底层系统依旧工作 服务可用：跑在云上的服务必须一直是可用的状态 数据安全：云上的数据确保健全、可用 那么在设计上，我们就得要求“no single point of failure”：路由、防火墙、负载均衡、反向代理以及监控系统等在网络和应用层面上必须全部是冗余设计，以此来保证最佳的可用性。 下面，浅谈一下k8s高可用集群是如何组建的。 k8s关键组件及其高可用方案首先，看一下架构设计（参考资料2），图中已经将高可用k8s集群各个组件的功能以及通讯调用关系清晰的展示出来了，下面我将分别从管理平面、执行平面和数据平面三个部分来简单说明一下该高可用架构方案以及各个组件的功能。 管理平面 apiserver: apiserver是k8s集群的入口，为了使用方便，kubectl作为其客户端供用户使用。为了实现高可用，在3个机器上分别以静态Pod的方式部署了apiserver并挂载在同一个loadbalancer上，如此，其与其他组件的联系都经由这个负载均衡器来做转发（图中黑色连线），这样也保证了每一个用户命令都有且仅有一个apiserver来响应，并且理论上只要还有一个Pod是可用的，该组件的服务就没有问题，再加上k8s的Pod有自愈能力，apiserver高可用可以说是能够保证的。 这里多说两句，k8s的API至关重要，而一些针对k8s做的二次开发其实也主要是围绕着k8s声明式API做一些CRUD，而面向API编程也是你从k8s用户向玩家进阶的必经之路。 controller managers: k8s自愈能力的关键所在，controller managers提供一种reconciliation的功能，简单来说就是该组件会无限循环地去通过apiserver来查看api资源的状态，并将其实际状态转变为api资源声明中的状态。比如，一个deployment设置了replicas为3，而由于某些原因集群中运行了5个这样的Pod时，controller managers就会触发工作并且调用api来删除2个Pod。同样，在k8s的master节点上，每个节点以静态Pod部署一个组件以达到高可用的目的。 scheduler: 该组件负责集群内部Pod的调度，主要根据集群node资源情况来平衡每个node的任务量，此外，还支持用户对Pod调度的自定义限制规则，比如NodeSelector、affinity规则等。该组件的高可用部署方案也是在每一个master节点上部署一个静态Pod。 组件controller managers和scheduler的选主是通过etcd来实现的：当一个副本不能工作时，其余副本会更新endpoint至etcd，而etcd只会接受其中一个更新请求，从而实现leader election。至于为什么需要选主，这里就留作一道思考题好了。 执行平面执行平面针对的就是node/slave节点，这里实际上就没有高可用一说了，即便如此，还是简单介绍一下图中出现的几个组件吧。 container runtime: 每个节点都需要一个容器运行时来执行容器，比如Docker。非pod启动。 kubelet: 用于执行apiserver下达的命令，也可以重启启动失败的pod。 kube-proxy: 通过修改iptables来达到网络代理、负载均衡的效果，在k8s中以Service作为代表。比如在使用NodePort进行对外提供服务时，所有node/slave节点都会生成特定的iptables，当该服务被删除或者节点断网时，iptables也会被清除。 数据平面etcd 对于高可用集群来说，集群的数据至关重要，Kubernetes将etcd作为数据存储中心，其存储了所有集群相关的信息，比如：pod、node、cm… 鉴于底层系统的高可靠性，数据决不能丢。 如图所示，etcd在每个master节点上部署了一个实例，以保证其高可用性，实践证明，etcd挂载本地ssd的方式会大幅提高超大规模（节点大于2000）集群性能（参考资料6）。 etcd官方给的部署模式是奇数个（大于等于3）就好了，推荐部署5个节点，这就不得不提etcd的选主协议/逻辑/算法Raft，这里有个非常生动的动画值得一看。此外，还需要注意的是所谓“脑裂”问题，这里的“脑裂”是指etcd集群出现两个甚至多个leader，如果你也是这样理解脑裂的，那就大可放心使用，因为there is no “split-brain” in etcd。 默认的etcd参数不太适合disk io比较低的场景，由其是在测试环境，所以可以调优一下：12ETCD_ELECTION_TIMEOUT=5000 #default 1000msETCD_HEARTBEAT_INTERVAL=250 #default 100ms 总结 部署高可用k8s集群对于企业级云平台来说是一个根本性的原则； 容错、服务可用和数据安全是高可用基础设施的关键； 文中简单介绍了部分k8s组件，实际上还有一些必须组件，如：网络插件、DNS插件等 对于Business来说，高可用并不仅仅是一个集群就可以做到的，更复杂的还有如多网络域部署甚至异地多数据中心部署。 参考资料 https://www.criticalcase.com/blog/5-reasons-why-you-need-high-availability-for-your-business.html https://elastisys.com/2018/01/25/setting-highly-available-kubernetes-clusters/ https://coreos.com/etcd/docs/latest/op-guide/failures.html https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/ http://thesecretlivesofdata.com/raft/ https://openai.com/blog/scaling-kubernetes-to-2500-nodes/]]></content>
      <categories>
        <category>容器云kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>高可用</tag>
        <tag>云计算架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN网络参数的计算]]></title>
    <url>%2F2019%2F03%2F25%2FCNN%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E7%9A%84%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[前几天听室友给我讲算法岗的面经，其中面试官就问了一个小问题，“给出CNN网络的参数（可学习的）个数如何计算”，今天就来计算一下好了。 问题描述可学习参数顾名思义就是指CNN中需要学习/更新的变量，因为CNN的网络架构设计中会引入很多需要被学习出来的变量，比如：hidden layer中的神经元个数便直接和仿射变换的参数个数相关，而现在的问题是把这些可学习的变量的个数统计出来。 卷积神经网络架构卷积神经网络在计算机视觉领域有比较多的应用，下图便是一个图片识别的网络架构示例图（工业界使用的模型更复杂）。 上图描述了卷积神经网络在进行正向计算/正向传播时的流程/架构，如图所示，当输入一个”小轿车”的图片时，我们希望经过一个函数$f(x)$各种计算后，能够输出“CAR”这个词。那么该如何计算呢？ 输入层（input）：就是读取图片，将图片用数字化的矩阵来表示。 卷积（convolution）：选用卷积核（filter，可以是多个）对图片的多个通道进行卷积操作（element-wise的相乘）。卷积计算会使图片的长宽变小，但是”高度”变大（如图中的图片逐渐变”厚”），这是因为使用的卷积核（filter）较多，使得计算得到的图片通道数（channels）也会增加。 卷积操作其实可以理解为简化版的”连接层”，部分神经元才可以和下一层的部分神经元进行连接。 激活（activation）：该操作主要是对之前的卷积计算结果做非线性处理，万能逼近原理告诉我们这种线性和非线性计算的组合可以拟合所有复杂的函数。常用的非线性处理函数/激活函数有Sigmoid、Relu、Leaky ReLU、tanh等，更多内容可以参考这里。 池化（pooling）：对非线性化后的高维矩阵进行”减采样”，同样以一定步长逐步将矩阵中的”元素块（例如：9 \times 9）”仅使用一个数来代表，比如：取”元素块”中的最大值、平均值等计算方式。 降采样可以减少后续的计算量还可以一定程度防止过拟合。 拉平(Flatten)：将高维矩阵”拉平”，转换为一维矩阵，元素依次排序。 全连接（Fully Connected）:设置下一层神经元的个数，并使用仿射变换y_i = \vec W_i \cdot \vec x+b_i得到下一层神经元的值，因为两层之间的神经元会全部连接起来，所及叫做全连接。 输出层计算分类概率（Softmax）：对最后一层的神经元进行概率输出计算，即：给出各分类标签的概率，比如这里预期”Car”的概率一定要大于其他分类标签的概率值，所以最后一层的神经元个数和分类的标签个数需要一致。下面是softmax函数的表达式：p(y_i|\vec x) = \frac {e^{(\vec w_i \cdot \vec x+b_i)}}{\sum_{k \in K} e^{(\vec w_k \cdot \vec x+b_k)}} Batch-Normalization：这一层其实在每一次卷积、全连接后都可以进行计算，但是图片中没有反映处这个处理过程。感兴趣可以查看我之前的博客卷积神经网络之Batch Normalization（一）：How？ 参数个数计算按照上面的步骤描述，可知： input层是没有引进变量的； 卷积层则引入了”卷积核/filter”，假设卷积核大小为n \times m，图片有l个通道（channels）/维度，而选用的”卷积核/filter”有k个，再加上bias，可以得到引进的参数有：(n*m*l+1)*k个； 激活、池化层仅仅对原来的矩阵做了一个变换，不会引进新的参数； 拉平操作仅仅对矩阵进行了reshape，也不会引进新的变量； 全连接层就是对前后神经元做了仿射变换\vec w_i \cdot \vec x+b_i，引进的参数有权重和偏置，假设n个神经元连接m个神经元，则引入的参数有(n+1)*m； 输出层其实和全连接层没啥区别，只是输出的神经元个数要求是分类的标签个数，所以引入的变量也是(n+1)*m，这里m是分类的标签个数； BN层引入的参数则和输入层神经元个数相关，假设输入神经元个数为n，则该层引进的参数为2n 综合以上，计算一个CNN架构的所有可学习参数/变量的个数可以分解成每一个步骤的参数变化量和引入参数个数两个相关的小问题，就像这样：123456789# name size parameters--- -------- ------------------------- ------------------------ 0 input 1x28x28 0 1 conv2d1 (28-(5-1))=24 -&gt; 32x24x24 (5*5*1+1)*32 = 832 2 maxpool1 32x12x12 0 3 conv2d2 (12-(3-1))=10 -&gt; 32x10x10 (3*3*32+1)*32 = 9'248 4 maxpool2 32x5x5 0 5 dense 256 (32*5*5+1)*256 = 205'056 6 output 10 (256+1)*10 = 2'570 最后将每一个步骤的参数相加便得到所有参数的个数。 参考资料 https://stackoverflow.com/questions/42786717/how-to-calculate-the-number-of-parameters-for-convolutional-neural-network https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>深度学习</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优化算法--牛顿迭代法]]></title>
    <url>%2F2019%2F03%2F17%2F%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-%E7%89%9B%E9%A1%BF%E8%BF%AD%E4%BB%A3%E6%B3%95%2F</url>
    <content type="text"><![CDATA[牛顿法给出了任意方程求根的数值解法，而最优化问题一般会转换为求函数之间在”赋范线性空间”的距离最小点，所以，利用牛顿法去求解任意目标函数的极值点是个不错的思路。 方程求根对于一元二次方程，求根其实很简单，只要套用求根公式就行了，但找到一个方程的求根公式（解析解）其实是很困难的，可以证明5次方程以上便没有解析解了，参考维基百科五次方程。其他的复杂方程如偏微分方程求解更是超级困难。好在随着计算机技术的发展，解析解变的不再那么重要（至少是在工程上），取而代之的方法便是数值解法，牛顿法便是众多数值解法中的一个。数值法求解又叫做数值分析，主要利用逼近的思想来使数值解通过迭代计算不断接近解析解，而得出来得解就叫做数值解，在工程上，数值解只要是在精度要求范围内满足方程便是有用的。 牛顿迭代法 先考虑一个小问题：求解方程x^2-2=0的根，也即求解\sqrt 2。牛顿迭代法的思想从几何的角度很好理解，如上图所示（画图的脚本在这里）方程的根就是函数y=x^2-2与x轴的交点处横坐标的值。从图中x_n点出发，计算函数在x_n点处的切线，再计算切线和x轴的交点得到x_{n+1}，再计算函数在x_{n+1}点处的切线… 一直这样迭代下去，可以发现x_{n}会越来越接近方程的根。 上述思路的数学表达：由x_{n}计算y_{n} f(x_n)=y_n得到切线方程： y-y_n= \left. f(x)' \right | _{x=x_n}(x-x_n)切线和x轴的交点，也即，当y=0时， 0-y_n=\left. f(x)' \right | _{x=x_n}(x-x_n)\frac{-y_n}{\left. f'(x) \right | _{x=x_n}} = (x-x_n)当\left. f'(x) \right | _{x=x_n} \neq 0时， x = x_n- \frac{y_n}{\left. f'(x) \right | _{x=x_n}}由y_n = f(x_n)，得到： x = x_n- \frac{f(x_n)}{\left. f'(x) \right | _{x=x_n}}令x=x_n，继续迭代，则得到迭代公式： x_{n+1} = x_n- \frac{f(x_n)}{\left. f'(x) \right | _{x=x_n}}推导过程还可以从函数泰勒展开的角度去理解，这在很多博客里有写，这里就不赘述了。 根据上面的迭代公式，可以计算方程x^2-2=0的根了： 猜一个初始值，因为根大概是1点多吧，那就给个x_0=2好了； 计算x_1：x_{1} = x_0- \frac{f(x_0)}{\left. f'(x) \right | _{x=x_0}}= 2- \frac{f(2)}{\left. f'(x) \right | _{x=2}}=1.5x_{2} = 1.5- \frac{f(1.5)}{\left. f'(x) \right | _{x=1.5}}=1.416667x_{3} = 1.416667- \frac{f(1.416667)}{\left. f'(x) \right | _{x=1.416667}}=1.414216 算法优缺点分析牛顿法的优点当然就是提供了一种方程求根的数值解方法。而缺点也有几点： 首先算法是要求函数处处可导的，如果对于优化问题还需要导函数连续（因为要求处处存在二阶导数），否则算法就不能计算函数的根了，比如f(x)=x^{1/3}就不能收敛，虽然函数的根为0，但是它在0处的导数是不存在的； 求出的解可能仅仅是众多解中的一个，这个比较依赖于初始值的选取，比如上面的问题，初始值为2，则收敛到了方程的正数解，要想得到负数解，则需要将初始值选在负数中，现实中的问题，很难去估计解的大小范围； 如果初始的估计值与根的距离太远收敛就会变的比较慢； 要求每次迭代是得到的切线导数不能为0，如推导过程所示； 如果方程本来就没有根，那牛顿法是不能收敛的； 优化问题求解优化问题从泛函的角度理解起来，就是计算函数之间的距离最小。对于距离的定义有很多，比较常用的是二范数，使二范数距离最小的求解过程就叫做最小二乘。对于Gm=data_{predict}这样的线性问题（非线程问题可以通过泰勒展开转换成线性问题），可以定义距离为\phi (m)=||Gm-data_{observation}||_2，为了求距离最小值点，需要先求极值点，问题便转换为求解\phi '(m)=0的根，这时候牛顿法便派上了用场。与之前问题不同的是，这里需要求\phi '(m)的导数，也即求解\phi "(m)，也即Hessian矩阵。假设，此处的参数m是n维向量，则Hessian矩阵为： H = \begin{pmatrix} \frac{\partial ^2f}{\partial m_1^2} & \frac{\partial ^2f}{\partial m_1 \partial m_2} & \cdots & \frac{\partial ^2f}{\partial m_1 \partial m_n} \\ \frac{\partial ^2f}{\partial m_2 \partial m_1} & \frac{\partial ^2f}{\partial m_2^2} & \cdots & \frac{\partial ^2f}{\partial m_2 \partial m_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial ^2f}{\partial m_n \partial m_1} & \frac{\partial ^2f}{\partial m_n \partial m_2} & \cdots & \frac{\partial ^2f}{\partial m_n^2} \\ \end{pmatrix}所以，牛顿法求解最优化问题，需要先求目标函数的Jacobian矩阵和Hessian矩阵，计算量比较大的便是计算Hessian矩阵了，因为二阶导计算量成指数增长。 注意，这里若二阶导数是连续的，则$H$是对称矩阵。 算法步骤步骤1： 给定误差阈值0\leq\epsilon]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
        <tag>牛顿迭代法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据采集之Python爬虫实验]]></title>
    <url>%2F2019%2F03%2F11%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BPython%E7%88%AC%E8%99%AB%E5%AE%9E%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[如果说算法是程序的灵魂，那么数据就是算法的灵魂。数据采集是数据工程的第一步，为了提高效率，目前在互联网上采集数据比较高效的方法就是爬虫了。今天就来爬一下《惊奇队长》豆瓣上的影评/review。 爬虫简介数据可以挖掘潜在的价值，但是在挖掘之前需要有数据，否则再牛的算法也不会work。在一些具体的问题上，比如，我们可以收集各大网站上观众对于该电影的评价来判断电影的火热程度，结合其他的数据还可以估计票房等信息；又或者根据社交网络上人们对于股票市场行情的态度以及相关新闻，通过NLP处理来预测股价。那么，首先需要解决的问题就是如何获取数据。网络爬虫就是获取数据的一个重要手段，爬虫就好像一个全自动的机器人一直在浏览网页并将重要的信息收集起来，并按照一定的规则存储在相应的数据库或者文件内。数据收集好了，算法设计人员才可以做进一步的工作。 如果有网站数据的API接口，就不用写爬虫了；爬虫获取的数据都是公开的数据，很多有价值的数据还是需要花重金购买。 开始实践本次内容就是编写一个最简单的爬虫，实现豆瓣上电影的影评（或者叫做review），思路就是使用urllib库去模拟浏览器访问相关网页，再利用re库和正则表达式提取关键信息。 URL分析首先，看一下豆瓣影评页的URL是否存在规律：浏览器打开豆瓣电影，逐个点击《惊奇队长》—&gt; 惊奇队长的影评 · · · · · · ( 全部 1160 条 )，查看网址为：https://movie.douban.com/subject/26213252/reviews，网址中出现的数字应该是电影的编号，但是没有页码的信息，点击”后页”按钮，网址变成了https://movie.douban.com/subject/26213252/reviews?start=20，所以猜测start=20这个参数是控制页码的，多试几次，发现确实是这个规律。那么，在代码中设置翻页的操作就可以通过for循环来写了: 123urlorg = "https://movie.douban.com/subject/26213252/reviews?start="for i in range(0, 100, 20): url = urlorg + str(i) # 这里的url即为下一页的url了 网页源码分析查看《惊奇队长》影评页的网页源码，发现每一个影评都对应了一个id，而且每个id都对应了一个新的URL，想要查看完整的影评内容需要跳转到这个新的网页上进行查看；这些新的URL的构造也很有规律：https://movie.douban.com/review/10034121/最后的数字10034121就是这个review的id。通过网页源码的搜索（如下所示），可以发现data-rid=&quot;9371928&quot; title=&quot;有用&quot;&gt;可以唯一对应这个id，我们可以据此来设置正则表达式。 html 源码： 1&lt;a href="javascript:;" class="action-btn up" data-rid="9371928" title="有用"&gt; 使用python 的re包接收正则表达式，(.*?)表示匹配项： 1pat = 'data-rid="(.*?)" title="有用"' #(.*?)部分即为id 得到了每一个review的id之后，把它存在一个list内，我们就可以构造出每一个review的URL：https://movie.douban.com/review/+str(id)，打开其中一个review继续分析（我们的目标是把影评的内容拿到）；再次查看影评的网页源码，可以发现影评的题目可以通过&lt;meta name=&quot;description&quot; content=&quot;影评题目&quot; /&gt;唯一确定，而影评的内容则在data-original=&quot;1&quot;&gt;和&lt;div class=&quot;copyright&quot;&gt;之间，如下所示：123456789&lt;meta name="description" content="影评题目" /&gt;.........data-original="1"&gt; &lt;p&gt;内容.... ....内容&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt; &lt;div class="copyright"&gt; 如此以来，我们便可以设置正则将这两个主要内容提取出来。python的re包接收正则表达式： 12pat2 = '&lt;meta name="description" content="(.*?)" /&gt;' #影评的题目pat3 = 'data-original="1"&gt;(.*?)&lt;div class="copyright"&gt;' #影评的内容 编写代码根据上面的信息，爬虫的主要代码部分已经呼之欲出了，完整代码请点击这里查看，爬取之前可以做一下浏览器模拟请求，最后爬下来的影评内容存放在一个文件中，以供后续使用。 12345678910111213141516171819urlorg = "https://movie.douban.com/subject/26213252/reviews?start="#每页显示20个评论，所以间隔20算作一页for i in range(0, 100, 20): #如果需要爬取所有影评可以将100设置为更大的数字 url = urlorg + str(i) every_page = ur.urlopen(url).read().decode("utf-8") pat = 'data-rid="(.*?)" title="有用"' review_id = re.compile(pat).findall(every_page) for j in range(0, 20): review_url = "https://movie.douban.com/review/" + str(review_id[j]) review = ur.urlopen(review_url).read().decode("utf-8") pat2 = '&lt;meta name="description" content="(.*?)" /&gt;' pat3 = 'data-original="1"&gt;(.*?)&lt;div class="copyright"&gt;' review_title = re.compile(pat2).findall(review) review_content = re.compile(pat3, re.S).findall(review) print(review_title) print("-----------------------------") print(review_content) 高级爬虫这里所谓的高级爬虫就是网络爬虫在遇到各种各样的问题时，依旧可以正常工作。下图表示一个爬虫的工作流程，首先进行网页的源码分析，看看是否存在规律，接着编写爬虫代码，最后将爬取的数据（适合存在在数据库的数据）保存在数据库内。下面简单的分析几个问题，以下问题都是有解的。 网页抓包分析静态网页直接查看网页源码就可以爬取内容了，而动态网页则是利用一些前端技术（比如js、css等）进行动态展示。此时需要对浏览器进行抓包分析，将真正的网址抓取出来，得到关键信息。抓包的工具有Fiddler等。 网站数据的加密、验证码很多网站采取了防爬虫的措施，比如对网页源码进行加密（典型的例子：网易云音乐的评论）、访问过频繁需要填写验证码等。对于网页加密，需要仔细的对网站进行抓包分析，将加密方式破解就可以了（一般还是js文件在搞事情）；对于填写验证码，可以让爬虫在爬取时加一个sleep时间（或者使用图片识别技术）。 异常处理爬虫的工作依赖网络，所以网络万一出现问题，爬虫得有相应的处理机制，例如尝试连接某个网页未果后要及时爬取下一个，而不是异常退出了；再比如爬虫异常推出重启后得在重启之前得位置接着爬取数据，而不是重头再来。 多线程对于爬虫这种io密集型的操作，多线程确实会提高效率，但是python的GIL设计使其多线程其实是假的多线程，所以如果比较在乎效率，这里比较推崇的做法是go语言写的爬虫。 使用框架使用框架的好处是可以省去很多麻烦的设置，少考虑一些简单的问题，相互传阅代码也比较通俗易懂，python中的scrapy框架使用的比较多。 数据库表的设计如果数据需要存入数据库，则需要设计一下数据库、表；这样做的好处是后续的数据供大家使用非常方便快捷。 小结 网络上有很多数据值得深入的挖掘，获取数据是数据挖掘的第一步； 爬虫可以对网络的资源进行自动化获取，爬取数据之前需要对资源的获取进行仔细分析； 高级/工业级的爬虫需要考虑更多的问题，如：对加密网页内容的破解、抓包分析、爬虫的异常处理、数据存库等。]]></content>
      <categories>
        <category>数据工程</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>爬虫</tag>
        <tag>数据采集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络之Batch-Normalization（二）：Why？]]></title>
    <url>%2F2019%2F03%2F09%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8BBatch-Normalization%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9AWhy%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[上一篇主要介绍了Batch-Normalization（下面简称BN）是如何工作的，即在连接层和激活函数之间加了一个BN层，这些参数参与了整个网络的正向和反向传播。这篇博文主要介绍为什么BN算法可以work，内容主要参考了两篇论文，包括18年的一篇NIPS论文。 问题的提出和解决在引入BN之前，以前的model training有一些系统性的问题，导致很多算法收敛速度都非常慢，甚至根本就不能工作，尤其在使用sigmoid激活函数时。其中一个比较著名的问题就是每层神经元是会受到它之前所有神经元影响的，因为每一层的输出都是下一层的输入，所以一个神经元输出的数据分布发生改变一定会使其他神经元跟着改变，这样相互影响的调参很容易使调参过程乱套，这个被称作Internal Covariate shift（ICS）。此外，还有其他问题，比如仿射层的输出值太大或太小，其经过sigmoid激活函数时会落在饱和区域，反向传播会有梯度消失的问题。这里先试图说明一下ICS问题及其解决方法。 Internal Covariate shift (ICS)关于ICS，有个形象的比喻：当你有个射击目标时，如果这个目标是静止的，它就会比较容易击中；而当这个目标是在不停的移动时，它就很难被击中。深度学习的训练过程中就类似于后者： 训练数据被输入到神经网络时，一般会先被normalization一下，因为输入的数据的每个维度的量纲可能会不一样，通过normalization可以消除量纲不一致的问题； 数据经过{Wx+b}和activation后，进入hidden layer，数据集的分布（均值和标准差）就会发生变化，而且每经过一层，输出数据集的分布都会变化； 由于每一层都发现自己的input数据集的分布在不停的变化，而反向传播更新参数时，想要适应训练数据集的分布就是一件非常困难的事情。 如果上面的理解是正确的，那么训练过程收敛速度非常慢就很容易理解了，尤其是对于深度较深的网络（隐含层比较多）。 ICS问题的解决虽然ICS的问题很棘手，但也不是无解的。 其中一个比较常见的解法就是减小learning rate，因为学习率一旦降低，学习训练数据集分布的过程就能够通过持续微小的调整来慢慢接近目标；但是，这也带来了一个问题，学习率太小容易使学习/训练的速度变慢，此外，还可能使学习过程陷入局部极小值。 BN算法之所以work的比较好，最主要的原因一直被认为是其解决了ICS的问题。Batch Normalization将每一层的输出都经过了“变换”，每一层的输出数据集（batch）都会重新将数据集的分布归一化到标准的分布形态上（均值为0，标准差为1）。这样一来，“目标分布” 在每一层的传递过程中变化就不会很大了，也即目标被固定住了。 下图对比了使用BN和不使用BN时，训练收敛的变化趋势，可以看到使用BN可以在更少的训练步数内达到同等的准确率，此外使用BN还可以达到更高的准确率，也即训练收敛速度更快，效果更佳。 BN算法的有效性分析除了ICS的问题，BN算法还一并解决了深度学习训练过程中遇到的各种小问题。下面以问答的形式，说明一下几个小问题的解决。 Q1: 为什么先做BN再做activation？其实仅仅考虑ICS的问题，先做activation再做BN也不是不可以；但是，先做BN还是有好处的，BN将仿射层的输出标准化后，数值基本分布在0附近，对于sigmoid激活函数来说，值大都落在非饱和区了，就不太会造成梯度消失的现象了。 Q2: mini-batch的大小设置多大比较合适？mini batch的大小稍微大一点其实会比较合理，因为算法中需要使用mini-batch内的数据去估计整个样本的均值和方差，所以大一点会比较接近总体样本的分布；但是，太大又会导致training比较慢，所以，batch的大小和算力需要去权衡一下。 Q3: scale和shift参数的加入有什么作用？scale、shift是两个独立的参数，也即和数据是没有依赖关系的，它们完全有可能将BN的作用给抵消掉，然而这恰好也是这个方法的优势，可以根据具体情况由网络自身在训练过程中来决定需不需要BN。 Q4: 训练好的模型如何使用，因为已经没有batch的概念了？一种方法是真的去估计整个样本在每一层的输出值的均值和方差，这个计算量太大。另一种比较常用的方法是，对训练集数据中的每一个均值和方差都保留下来，最后做移动平均来估计总体样本的均值和方差。 新的理解这是一篇投稿于NeurIPS 2018的会议论文（参考文献2），文章以新的观点阐述了BN算法的有效性。主要涉及了两个实验（公式太多，没有细看）： 在BN层之后添加Noise在BatchNorm层之后加上一个随机噪音，噪音的分布异于BatchNorm层的输出（均值非0，方差非1），并且每次传播的时候，噪音都不一样。也就是说，在BatchNorm层之后故意加了一个ICS，结果发现训练并没有因此而明显变差（如下图粉红色所示），虽然隐含层的输出分布会随着迭代次数的增加（时间的推移）而变得不太稳定。 梯度更新前后的Loss和其梯度的变化作者使用量化的方式定量的说明了BN算法并不能减少ICS。实际上，作者认为BN算法减少了Lipschitz常数（也即loss函数变得更加连续/光滑），使得梯度变得跟加”可预测”（如下图所式），才是BN算法有效性的关键。 这里”可预测“我的理解是表示变量的变化范围比较小，更加可控。 总结 ICS问题的解决使深度神经网络的收敛速度变快，另外，此时的learning rate也可以设置大一些，则加快了学习的速率； BN的引入极大的降低了sigmoid和tanh这样的激活函数梯度消失的风险； 使用了Batch Normalization，初始化参数对神经网络的影响减小； BN算法降低了过拟合的风险，训练过程不需要太多的正则化，也可以不需要drop out了； 新的观点认为ICS的解决并非BN算法有效的根本原因，loss变得平滑了才是主要原因； 国外发表的论文还做了一个几分钟的小视频放在youtube上，我觉得国内也可以校仿一下。 Reference Ioffe, S., &amp; Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167. Santurkar, S., Tsipras, D., Ilyas, A., &amp; Madry, A. (2018). How does batch normalization help optimization?. In Advances in Neural Information Processing Systems (pp. 2488-2498). Batch Normalization — Speed up Neural Network Training]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>深度学习</tag>
        <tag>CNN</tag>
        <tag>人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络之Batch Normalization（一）：How？]]></title>
    <url>%2F2019%2F03%2F06%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8BBatch-Normalization%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9AHow%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[本文主要介绍深度学习里的一个常用的trick，主要用于加速收敛算法，这篇主要介绍一下怎么做的（How），下篇再介绍Why和该算法的一些好处，本来想着根据自己的理解写一下，看了大神写的之后我就决定”抄袭了”，（大神就是大神啊…）。原文使用MXNet实现的算法（原文查看文末的原文链接），这里改成使用TensorFlow实现一下这个例子。 批量归一化这一节我们介绍批量归一化（batch normalization）层，它能让较深的神经网络的训练变得更加容易。在“实战 Kaggle 比赛：预测房价”一节里，我们对输入数据做了标准化处理：处理后的任意一个特征在数据集中所有样本上的均值为 0、标准差为 1。标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。 通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但对于深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。 批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使得整个神经网络在各层的中间输出的数值更稳定。批量归一化和下一节将要介绍的残差网络为训练和设计深度模型提供了两类重要思路。 批量归一化层对全连接层和卷积层做批量归一化的方法稍有不同。下面我们将分别介绍这两种情况下的批量归一化。 对全连接层做批量归一化我们先考虑如何对全连接层做批量归一化。通常，我们将批量归一化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为$\boldsymbol{u}$，权重参数和偏差参数分别为 \boldsymbol{W} 和 \boldsymbol{b}，激活函数为 \phi。设批量归一化的操作符为 \text{BN}。那么，使用批量归一化的全连接层的输出为 \phi(\text{BN}(\boldsymbol{x})),其中批量归一化输入 \boldsymbol{x} 由仿射变换 \boldsymbol{x} = \boldsymbol{W\boldsymbol{u} + \boldsymbol{b}}得到。考虑一个由 m 个样本组成的小批量，仿射变换的输出为一个新的小批量 \mathcal{B} = \{\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(m)} \}。它们正是批量归一化层的输入。对于小批量 \mathcal{B} 中任意样本 \boldsymbol{x}^{(i)} \in \mathbb{R}^d, 1 \leq i \leq m，批量归一化层的输出同样是 d 维向量 \boldsymbol{y}^{(i)} = \text{BN}(\boldsymbol{x}^{(i)}),并由以下几步求得。首先，对小批量 \mathcal{B} 求均值和方差： \boldsymbol{\mu}_\mathcal{B} \leftarrow \frac{1}{m}\sum_{i = 1}^{m} \boldsymbol{x}^{(i)},\boldsymbol{\sigma}_\mathcal{B}^2 \leftarrow \frac{1}{m} \sum_{i=1}^{m}(\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B})^2,其中的平方计算是按元素求平方。接下来，我们使用按元素开方和按元素除法对 \boldsymbol{x}^{(i)} 标准化： \hat{\boldsymbol{x}}^{(i)} \leftarrow \frac{\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B}}{\sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}},这里 \epsilon > 0 是一个很小的常数，保证分母大于 0。在上面标准化的基础上，批量归一化层引入了两个可以学习的模型参数，拉伸（scale）参数 \boldsymbol{\gamma} 和偏移（shift）参数 \boldsymbol{\beta}。这两个参数和 \boldsymbol{x}^{(i)} 形状相同，皆为 d 维向量。它们与 \boldsymbol{x}^{(i)} 分别做按元素乘法（符号 \odot）和加法计算： {\boldsymbol{y}}^{(i)} \leftarrow \boldsymbol{\gamma} \odot \hat{\boldsymbol{x}}^{(i)} + \boldsymbol{\beta}.至此，我们得到了 \boldsymbol{x}^{(i)} 的批量归一化的输出 \boldsymbol{y}^{(i)}。值得注意的是，可学习的拉伸和偏移参数保留了不对 \hat{\boldsymbol{x}}^{(i)} 做批量归一化的可能：此时只需学出 \boldsymbol{\gamma} = \sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon} 和 \boldsymbol{\beta} = \boldsymbol{\mu}_\mathcal{B}。我们可以对此这样理解：如果批量归一化无益，理论上学出的模型可以不使用批量归一化。 对卷积层做批量归一化对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数，且均为标量。设小批量中有 m 个样本。在单个通道上，假设卷积计算输出的高和宽分别为 p 和 q。我们需要对该通道中 m \times p \times q 个元素同时做批量归一化。对这些元素做标准化计算时，我们使用相同的均值和方差，即该通道中 m \times p \times q 个元素的均值和方差。 测试/预测时的批量归一化使用批量归一化训练时，我们可以将批量大小设的大一点，从而使批量内样本的均值和方差的计算都较为准确。将训练好的模型用来预测/测试时，我们希望模型对于任意输入都有确定的输出。因此，单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。可见，和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。 tensorflow调用根据上面的讲解，运算流程应该是，输入神经元（neural）的数据先做w \cdot u+b得到x_i，再按照上面的公式对一个batch内的x_i进行normalization并接着scale和shift，之后再对其进行激活得到z_i。 u_{i} -仿射变换-> x_i -标准化-> \hat{x_i} -拉伸和偏移-> y_i --> activation --> z_i下面，使用mnist手写数字识别为例，按照这个流程走一遍吧，在tensorflow中调用使用的是tf.layers.batch_normalization，完整代码请看这里(使用jupter-notebook查看)：12345epsilon = 0.001Wx_plus_b = tf.layers.batch_normalization(Wx_plus_b, mean, var, shift, scale, epsilon)# similar with this two steps:# Wx_plus_b = (Wx_plus_b - fc_mean) / tf.sqrt(fc_var + 0.001)# Wx_plus_b = Wx_plus_b * scale + shift 转载自：动手学深度学习原文网址：https://zh.gluon.ai/chapter_convolutional-neural-networks/batch-norm.html作者：阿斯顿·张、李沐、扎卡里 C. 立顿、亚历山大 J. 斯莫拉]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>深度学习</tag>
        <tag>CNN</tag>
        <tag>人工智能</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go语言为网站生成二维码]]></title>
    <url>%2F2019%2F03%2F05%2Fgo%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90%E7%BD%91%E5%9D%80%E4%BA%8C%E7%BB%B4%E7%A0%81%2F</url>
    <content type="text"><![CDATA[二维码有点意思，想着把俺的博客地址用二维码展示出来，比较来看还是go语言比较强大啊… 搭建golang环境安装go1234# ubuntusudo apt install golang-go# CentOSsudo yum install go 设置GOPATH将GOPATH添加至环境变量12345echo export GOPATH=/root/go &gt;&gt; ~/.bashrc# 设置当前终端生效source ~/.bashrc # 查看GOPATHgo env 创建所需文件夹 12cd /root/gomkdir bin &amp;&amp; mkdir pkg &amp;&amp; mkdir src GOPATH的目录结构: bin 编译后生成的可执行文件 pkg 编译后生成的文件（比如：.a） src 存放源代码（比如：.go .c .h .s等） 运行代码导入第三方包：go get -u github.com/yeqown/go-qrcode 新建文件夹makeqrcode，进入该文件夹后，新建文件 makeqrforwebsite.go 1234567891011121314151617package main import ( "fmt" qrcode "github.com/yeqown/go-qrcode" // 给后面的包一个简称)func main() &#123; qrc, err := qrcode.New("https://kiddie92.github.io/") if err != nil&#123; fmt.Printf("could not generate QRCode: %v", err) &#125; // 保存二维码 if err := qrc.Save("."); err != nil &#123; fmt.Printf("could not save image: %v", err) &#125;&#125; 直接运行：go run makeqrforwebsite.go，生成本博客地址对应的二维码，扫描一下试试。 Referencehttps://github.com/yeqown/go-qrcode/]]></content>
      <categories>
        <category>go</category>
        <category>Just for Fun</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈深度学习算法 -- 能不能学物理定律?]]></title>
    <url>%2F2019%2F03%2F03%2F%E6%B5%85%E8%B0%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95-%E8%83%BD%E4%B8%8D%E8%83%BD%E5%AD%A6%E7%89%A9%E7%90%86%E5%AE%9A%E5%BE%8B%2F</url>
    <content type="text"><![CDATA[本篇博客以一个物理问题为出发点，试图从数学的角度来理解一下深度学习算法。主要围绕着深度学习算法（未讨论非监督学习）能否学习出物理定律这个问题进行阐述。 先看一个物理问题 如上图所示，使用一个锤子敲击地面，会给地面造成一个冲击力，冲击造成的震源可以大致表示成图中（a）所显示的信号，震源使地面开始震动形成波场，（a）信号随着地面震动而传播，位于远处的检测器（倒三角形所示）感受到检测器所在位置的地面震动，并记录下来就得到了途中（b）所示的信号。这个问题可以简单理解成声音的传播，(a)是声源，经过大地传播之后（滤波作用），使接收声波的一方接收到信号（b）。 显然，不同的地质环境，大地的”材质”也会不同，那么一定会影响到（b）信号的最终形态。这就好像，人在水里说话和在空气中说话听到的声音肯定也会不一样。那么，如何描述材料的性质和接收信号（b）的关系呢？（我们把这一关系表达为以下映射，在物理中称为正演问题）下面给出两种解法。$公式$ Model Parameterts --> Data物理方法物理学家根据力学定律以及材料的弹性性质、密度等参数推导出Model和Data之间有着定量的关系，可以描述成下图的物理方程，有了这个方程，我们就可以建立这两者之间的关系了。 深度学习方法如今，我们还可以使用深度学习方法建立Model和Data之间的定量关系。首先我们建立一个网络架构，比如用几个卷积层、几个全链接层等，每一个神经元还有一组参数[W]，[b]通过给定Model和Data的数据对，不断的改进神经元的参数，最终在function set里面找到一个相对令人满意的函数，其可以表示Model和Data之间的映射关系。 问题的提出和回答上面给出了两种方法来解这个问题，我们可以看到如果两种方法都能解这个正演问题（深度学习实际上是在从数据反推回模型的过程中逐步给出正演函数的），那么是不是深度学习学习到的模型等价于物理定律呢，换句话说深度学习可以学到物理定律？ 这个问题我给它拆成两个： 深度学习建立Model到Data的映射这件事情能不能做？肯定能，因为有人已经证明了深度学习算法可以拟合任何复杂的函数，参见Universal approximation theorem。相关问题也可以看看这里神经网络为什么可以拟合任意函数？。也就是说，Model到Data的映射再复杂，深度学习也可以给你找个函数来逼近它。 建立的映射好不好用？或者说模型的泛化能力会很强吗？虽说神经网络有万能逼近的性质，但是逼近的好不好就另说了，因为毕竟没有拿所有的数据集去训练，而一个物理问题的数据集几乎可以说是无限大的。那有没有可能深度学习学习出来的模型恰好和物理定律一致呢？那就得把深度学习模型当作一个函数来研究了，看看它是不是化简完恰好就是物理方程，不过，几万甚至上亿个参数的函数，研究起来应该很头疼吧，一般人肯定会疯掉的，所以说这个问题还是交给科学家去解决吧。 这里和胡师兄讨论的时候，发现我们的理解基本一致（难道是因为大家都学地球物理的吗…） 其实，一个非线性的物理问题也可以线性化，比如使用泰勒展开就可以做到；从另外一个角度去理解就是G(m)=d转化成Gm=d的问题。但是，由于数据有限，这里的G存在0空间，所以会有G_0m=0，也就是说有限的数据集几乎是不可能约束G的。 其他策略为了让深度学习学到的模型/函数/映射更加接近”理论事实”，我们可以加一些约束，比如先做特征提取、结合比较好解释的机器学习和深度学习算法来学习出一个泛化能力更强的模型/函数/映射。 小结深度学习算法一般来说只能学习到数据集中已有的知识，它比较擅长于归纳，而不擅长演绎。对于非监督学习，情况可能比较复杂，暂不讨论。此外，世界是复杂的，但是伟大的理论通常在数学表达上都是简单优美的，这恐怕是现阶段人工智能所不能企及的。]]></content>
      <categories>
        <category>人工智能</category>
        <category>知识理解</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 特性--Graph 和 Sessions]]></title>
    <url>%2F2019%2F03%2F02%2FTensorFlow-%E7%89%B9%E6%80%A7-Graph-%E5%92%8C-Sessions%2F</url>
    <content type="text"><![CDATA[深度学习框架有很多，Google的TensorFlow市场占有率居高不下，本文的小目标是说清楚tensorflow的“图（Graph）”和“会话（Session）”机制及其优缺点，最后以一个回归问题为例实践一下。文中顺便回答一下动态图（Dynamic computation graphs）和静态图（Static computational graphs）框架的区别。 背景介绍深度学习是目前人工智能领域备受推崇的算法种类，其在计算机视觉（Computer vision）、自然语言处理（NLP）领域有比较广泛的应用，这里先挖个坑，下一篇将谈谈我对深度学习算法的理解。 目前开源市场的深度学习框架有很多，如tensorflow、pytorch、mxnet等，而tensorflow的市场占有率相对较高，那么为什么会有如此多的深度学习框架，tensorflow又有什么异于常人的地方呢？为了回答这个问题，本文首先尝试说明一下tensorflow的Graph+Session机制。 市面上的各种深度学习框架： 计算图（Graph）简单来说，计算图是由tensor和opration组成的一张工程图纸。先上图（从图中来看，这是一个分类问题…），文末还附了一张PyTorch的动态图，有兴趣可以对比一下。 张量（Tensor）先把力学里面的张量忘掉，这里的张量概念包括标量、矢量和线性算子，或者简单理解成高维矩阵。输入的数据、参数大多是高维矩阵，统一被称为tensor，此外，tensor之间经过各种计算得到的结果依然是一个张量。 输入tf.placeholder 参数tf.Variable 算子tf.matmul、tf.sqrt()等 算子（Operation）Tensor之间的各种运算统称为operation，如加减乘除、开根号等。tensor进入operation进行各种计算，输出结果到下一个operation继续计算，像是tensor在流动，TensorFlow由此得名。 会话 （Session）当tf.graph定义好后，打开一个tf.session执行Graph，简单来说，会话是指机器根据工程图纸打开计算资源进行施工。Session提供了Operation执行和Tensor求值的环境，此外其还拥有物理资源（GPUs和网络连接）。当我们不再需要该session的时候，需要调用sess.close()关闭会话，将这些资源释放。 1234567# Create a default in-process session.with tf.Session() as sess: # ...# Create a remote session.with tf.Session("grpc://example.org:2222"): # ... 数据流 （Dataflow）Dataflow是一个常见的并行计算编程模型。在一个dataflow图中（如上gif图所示），节点表示计算单元，边界则表示计算单元对数据的生产和消费。dataflow模式有几个比较大的优势： Parallelism：知道了各个operation之间的依赖关系，系统就可以比较好的使用并行计算了，比如：矩阵相乘可以并行计算、A算子的输入与B算子的输出没有依赖关系也可以并行计算。 Distributed execution：同样利用每个operation之间的依赖关系，tensorflow好让一些计算被调度到不同机器的多个设备上（CPUs, GPUs, TPUs），tensorflow还会提供必要的机器之间的通信。 Compilation：tensorflow的 XLA 编译器利用dadaflow图编译更快的机器码。 Portability：datdaflow图使模型表示是语言无关的。tf.saved_model保存的模型可以在其他语言中使用，非常便携。 实践 — 回归问题问题描述 构造一个函数/映射，y_{data} = f(x_{data})，其中 x_{data}是一个随机输入的2\times100矩阵，y_{data}是一个1\times100矩阵，由一个参数矩阵W=\begin{bmatrix} 0.1 & 0.2 \end{bmatrix} 和 x_{data}点乘后加上一个常数b=0.3构造出来。 12345# 使用Numpy生成假数据(phony data),总共100个点.x_data = np.float32(np.random.rand(2, 100)) # 随机输入print(x_data)y_data = np.dot([0.100, 0.200], x_data) + 0.300 #输出的y为[[]]的listprint(y_data) 使用x_{data}和y_{data}数据来反演/学习出参数矩阵W和常量参数b，该问题等价于由100个方程求解三个参数问题，显然是一个超定问题，求解过程就是一个优化过程。 \begin{pmatrix} W_1X_{1,1} + W_2X_{2,1} +b = y_1 \\ W_1X_{1,2} + W_2X_{2,2} +b = y_2 \\ ... \\ W_1X_{1,100} + W_2X_{2,100} +b =y_{100} \\ \end{pmatrix} 假设我们已经知道这个函数式了$Wx+b=y$，仅仅不知道给定的参数W和b是什么，根据函数式，可以使用初始化参数来构造y，并计算y和y_{data}之间的“距离”，并使用梯度下降的方式找到一个最优参数组使距离尽量减少。见代码部分10-13行。 注意： 现实世界中往往是不知道两个随机变量之间的确切关系的 这里”距离”是指向量之间的空间距离，常用的距离有欧几里得距离（2-范数）曼哈顿距离（1-范数）等。本例中使用2-范数作为距离，也即最小方差/最小二乘/Least Square方法。 运行环境由于tensorflow和cuda版本（9.0.176）兼容问题，选择安装V1.12.0GPU版本，本机tensorflow环境： 1234[conan@localhost ~]$ conda list | grep tensortensorboard 1.12.0 &lt;pip&gt;tensorflow-gpu 1.12.0 &lt;pip&gt;tensorflow-tensorboard 0.4.0 &lt;pip&gt; 代码部分这里使用一个简单的平面拟合问题来实践一下，完整代码请看这里 拟合时主要关注的参数为：tf.train.GradientDescentOptimizer(0.2)里的学习率（或者叫做步长）、和迭代次数for step in range(0, 51):，减小学习率增加迭代次数理论上会使拟合效果更好，但是会有过拟合（over fitting）的危险，并且模型的泛化能力（generalization）会比较差，控制这种风险的算法也很多，比如给目标函数加正则化（regularization）。 123456789101112131415161718192021222324252627282930313233# 构造一个线性模型# 实际问题中如果没有确切的物理关系,很难知道是否是线性模型, 也很难知道解在哪个范围b = tf.Variable(tf.zeros([1]))W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))y = tf.matmul(W, x_data) + b # y is synthetic data# 下面开始构建Graph# 最小化方差(Least Square) 定义目标函数/损失函数/misfit functionloss = tf.reduce_mean(tf.square(y - y_data))# 优化器就使用最原始的梯度下降方法，参数为learning rate/步长optimizer = tf.train.GradientDescentOptimizer(0.2) train = optimizer.minimize(loss)# 初始化变量init = tf.initialize_all_variables()# 启动会话sess = tf.Session()sess.run(init)# 拟合平面/反演参数/回归分析for step in range(0, 51): sess.run(train) # 参数train就是前面定义的dataflow graph if step % 10 == 0: print(step, sess.run(W), sess.run(b))W = sess.run(W)b = sess.run(b)sess.close() # 最终反演出来的方程y_pred = np.dot(W, x_data) + bprint('----------------')print(y_pred[0])# 得到最佳拟合结果 W: [[0.100 0.200]]\, b: [0.300] 得到的y和y_data的对比： 静态图和动态图回到最开始的问题，TensorFlow异于常人的地方：其实就是“静态图”框架。引用“hackernoon”上看到的一句话： TensorFlow is a “Define-and-Run” framework where one would define conditions and iterations in the graph structure whereas in comparison Chainer, DyNet, PyTorch are all “Define-by-Run” frameworks. 动态计算图框架使用起来就像做工程时一边设计一边施工，TensorFlow使用起来就没有“动态图”框架那样灵活、直接，容易调试，而这也是其入门门槛高的一个原因。但是，“静态图”的优点也是明显的—计算会更加高效，因为所有的步骤都定义好了再进行计算使计算机资源的调配更加合理、高效。所以说，“动态图”和“静态图”是优势互补的。 TensorFlow 2.0 推出了Eager Execution，开始支持“动态图”了 Reference Graphs and Sessions How is PyTorch different from Tensorflow?]]></content>
      <categories>
        <category>tensorflow</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 性能测试方法简介]]></title>
    <url>%2F2019%2F01%2F23%2Fkubernetes-%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E6%96%B9%E6%B3%95%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[本文主要介绍kubernetes（以下简称k8s）在性能测试中的主要关注指标以及涉及的辅助测试工具。由于各企业在私有云建设过程中使用的技术标准不尽相同，文中尽可能介绍可能涉及的通用性测试项，由于作者水平有限，以下内容仅供参考，欢迎讨论以及指正。 部署环境简要企业级k8s集群需要达到生产可用（GA），在部署上常采用高可用方案（HA），也即多台master节点和多台node节点的组合形式。生产上，k8s集群通常搭建在私有云或公有云IaaS之上，且需要较高的硬件资源支持。 集群资源从k8s集群部署需求的角度来说，集群资源应该明确给出，包括CPU、内存、系统、存储、网络以及相关的性能指标，而这些都可以由IaaS层提供，这里简单声明如下： 集群存储：采用NAS作为后端持久化存储方案 集群网络：VMware提供的NSX-T容器网络方案 集群系统、CPU、内存资源列表： 角色 角色说明 节点数 cpu 内存 系统 存储大小 master k8s master节点 3 4 16 CentOS 7.5 100G node k8s计算节点 2 8 16 CentOS 7.5 100G 集群部署架构企业级集群部署架构需要考虑很多因素，其中最主要的是需要有管理平面和业务平面，核心则是降低平台的使用复杂度和运维复杂度，所以部署上我们不仅仅需要有高可用的业务集群，还需要有相应的配套服务机制，其中包括监控（metrics）、日志（Logs）、网络管控、存储管控、负载均衡、私有云场景还需要提供yum源和镜像仓库服务等。 这里，我们的监控采用单个集群使用Prometheus作为TSDB+grafana作为数据展示，而日志方面则以ElasticSearch集群部署的方式进行存储收集。 业务集群的部署架构图则如下所示（图片来自这里） 容器网络测试容器网络简介k8s的最小调度单位为Pod，而Pod“内部”的容器会通过Linux namespace机制与infra容器共享网络栈，所以容器网络就是指Pod之间通信的网络，kubernetes以开放插件接口的形式（Container Network Interface）让第三方插件提供Pod间网络通信的能力。 目前主流的k8s容器网络插件有开源的Weave、Calico、Flannel-HostGW、Flannel-VxLAN、MacVLAN、IpVLAN…以及未开源的VMware NSX-T。 性能测试从容器网络性能测试的角度来说，关注点主要在于不同场景下带宽、计算资源消耗的情况。下面简单介绍一下相关的测试场景和测试策略以及涉及的测试工具： 由于k8s网络插件在工作过程中存在Linux的User Space和Kernel Space的交互（封包解包），这是性能损耗的主要来源之一；如果考虑网络安全，需要加上网络插件的限制隔离机制（Network Policies）的测试。 场景一：同主机Pod间通信 场景二：跨主机Pod间通信 场景三：集群内主机和主机间通信 场景四：Pod与宿主机间通信 场景五：Pod与非宿主机间通信 测试策略：固定网络带宽，固定网络类型，测试不同数据包大小对网络吞吐量的影响，例如可以测试获取文件传输量超过10G，系统在文件传输高峰时对局域网的带宽要求，并对比容器网络传输和非容器网络（Bare Metal）传输之间的CPU消耗以及内存消耗情况。 测试工具：iperf3，容器化运行在k8s集群上 相关的测试可以参考这里。 网络延迟造成容器网络延迟的主要原因是传输延迟及处理延迟，这里的测试关注点在于不同CNI插件下，不同场景的网络延迟。 场景一：通过Service的VIP或DNS进行集群内部访问 场景二：通过NodePort进行集群外部访问 场景三：通过IaaS层提供的LoadBalancer进行访问 测试策略：容器化运行qperf，依据场景的不同，通过设置yaml文件为qperf添加不同的Service访问方式，测试其在访问过程中的网络延迟。 涉及工具：qperf 需要注意的是：由于容器网络是基于IaaS层网络搭建，而IaaS层网络通常又是一个跨数据中心的“大二层”网络，虚拟机本身的物理位置对k8s集群来说已经是无感知的了，如此一来，容器网络的测试指标与IaaS网络其实是耦合在一起的，那么容器网络的测试实际上也是包含了IaaS层网络性能考量。 容器存储测试针对有状态应用的数据持久化以及容器日志存储需求，k8s设计了容器存储接口（CSI）并辅以PV、PVC的机制实现分布式应用的持久化存储，目前支持CSI实现容器持久化存储的方案有很多。存储的测试主要考量的指标是容器对数据卷的读写IO，除此之外，还需要考虑容器迁移是否依然能够实现数据持久化。 场景一：多容器实例跨主机部署，数据持久化 场景二：单个容器对数据卷进行读写IO 测试策略：k8s上使用deployment部署多个应用实例，每个Pod使用同一个PVC挂载同一个目录（Pod一般会分布在不同的主机上），再查看多个应用实例的数据是否同步写入同一后端存储；此外，在单容器内部使用dd命令在挂载目录下（本地存储或分布式存储）进行读、写以及读写测试，并使用参数iflag=direct，观察输出的平均读写时间。 涉及工具：dd k8s 并发测试对于使用go编写的k8s来说，并发能力理论上很强。性能测试上，可以使用多线程执行创建、删除、查询各类资源，由于k8s的最小调度单元为Pod，测试时可以仅使用创建deployment作为场景，主要的关注指标为错误率和平均响应时间以及硬件资源消耗： 场景一：多线程并发创建deployment，再并发删除deployment 测试策略：使用Jmeter多线程方式发送创建不同name的deployment资源的json文件至kube-apiserver，删除亦如此；同时通过Prometheus和Grafana对集群的资源和相关组件的资源使用进行监控。 涉及工具：Jmeter curl Prometheus Grafana kube-apiserver api 规范使用curl测试一下kube-apiserver的api规范：123curl -X POST \-d @filename.json -H "Content-Type: application/json" \-H "Authorization: Bearer $&#123;token&#125;" $&#123;api_url_or_ip:8080&#125;/apis/extensions/v1beta1/namespaces/$&#123;namespace_name&#125;/deployments deployment命名不可重复由于deployment的name、label不可以重复，这里可以使用jmeter设置变量，并将变量赋值到将要发送的json文件内，点击deployment.json即可查看deployment的json文件。 横向伸缩能力测试k8s的横向伸缩能力主要体现在两个层面：node扩展和Pod扩展，但是node的扩展同时需要IaaS的能力支持，我们这里仅仅考虑Pod的横向扩缩容。k8s可以开启Horizontal Pod Autoscaler功能，对接了metrics指标后，可以实现根据指标策略来自动扩缩应用副本数（Pod数）。因此，性能测试需要关注的指标有：Pod在扩缩容过程中所需的启停时间；扩缩过程中服务是否会出现中断，也即服务的错误率；以及服务的TPS变化；同时对集群资源的使用率进行监控。 场景一：对deployment进行扩容操作 场景二：对deployment进行缩容操作 测试策略：部署单个应用至k8s集群，关联的service端口暴露方式为NortPort，使用Jmeter对该服务进行多线程持续访问；修改deployment的replica参数，使用kubectl apply -f ${deployment.yaml}更新应用，观察Jmeter的TPS、Error指标数据，以及集群资源监控数据。 关于scale out/in，k8s把Pod当做”cattle”而不是”pet”去管理，这里的测试并没有使用HPA，所以手动扩缩容实际上使用的是Rolling Update，Rolling Update思路也即关闭正在运行的Pod再创建新的Pod。所以，缩容过程中可能会出现部分服务暂时中断的现象，jmeter会出现Error，如果将Pod的”优雅停”时间（默认30s）设置长一点应该能够减少Error出现的几率。 涉及工具：Jmeter Prometheus Grafana 集群高可用测试k8s集群高可用其实就是集群各组件的高可用，测试关注点则是集群部分组件甚至节点关闭（如master或node宕机），集群是否还能正常工作，以及业务应用对外提供服务的性能是否还能保持稳定。 场景一：正在对外提供服务的业务集群突然出现部分机器断网、宕机，或者kubelet等组件停止运行 测试策略：使用systemctl命令启停相关组件，模拟组件的工作中断；使用docker stop命名停止以静态Pod运行的服务组件，模拟组件的工作终止；使用ifconfig命令启停节点的网卡，模拟网络的中断；直接关闭机器模拟集群节点的突然宕机；同时观察集群应用服务及其管理是否能正常工作，业务运行相关指标是否下降。 涉及工具：systemd ifconfig docker vcenter Jmeter 总结优秀的架构一定是可扩展的，尤其是大规模集群管理这样的底层系统，k8s的扩展能力太强以至于它更像是IaaS和PaaS之间的中间层。以Kubernetes为核心的PaaS平台已在国内外众多企业内实施落地，由于kubernetes的插件化设计，各企业在落地过程中需要解决的网络方案、存储方案、负载均衡方案、监控体系、日志体系等各不相同，从而在性能测试方法上也不尽相同，本文主要介绍了部分性能测试可能需要关注的地方以及相关工具，不够全面系统，内容仅供参考。 Acknowledgment灵雀云的小伙伴们给予了文档参考和技术支持，在此致谢。 Reference Benchmark results of Kubernetes network plugins (CNI) over 10Gbit/s network Container Networking: From Docker to Kubernetes On setting up highly available Kubernetes clusters iperf3 doc qperf doc Kubernetes Performance Measurements and Roadmap]]></content>
      <categories>
        <category>容器云kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>性能测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 下基于kubernetes安装部署kubeflow]]></title>
    <url>%2F2019%2F01%2F05%2FCentOS-%E4%B8%8B%E5%9F%BA%E4%BA%8Ekubernetes%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2kubeflow%2F</url>
    <content type="text"><![CDATA[小目标：基于前期搭建的kubernetes集群，部署kubeflow，由于涉及到google的docker镜像，只好设置国外代理了 下载安装包：12wget https://github.com/ksonnet/ksonnet/releases/download/v0.13.1/ks_0.13.1_linux_amd64.tar.gzwget https://github.com/kubeflow/kubeflow/archive/v0.4.0-rc.3.tar.gz #2019.1.4 最新版本v0.4.0-rc.3 将下载好的安装包解压并归档 12345tar -vxf ks_0.13.1_linux_amd64.tar.gztar -vxf v0.4.0-rc.3.tar.gzmkdir kubeflow-kscp -r kubeflow-0.4.0-rc.3 kubeflow-kscp -r ks_0.13.1_linux_amd64 kubeflow-ks 安装ksonnetks是一个可执行文件，直接拷贝到系统可执行目录下就OK了 12cd kubeflow-ks/ks_0.13.1_linux_amd64cp ks /usr/bin 安装部署kubeflow首先定义一些临时的环境变量，安装的时候会方便很多，因为安装脚本也是需要用到这些变量的 12export KUBEFLOW_SRC=/your/path/to/kubeflow-0.4.0-rc.3export KFAPP=kubeflowconfig #随意命名 注意：KFAPP必须是将要存放配置文件的目录名称，不可以是目录的路径，否则会报以下错误：&lt;name&gt; should be the name for the deployment; not a path 安装部署只需要三个命令123$&#123;KUBEFLOW_SRC&#125;/scripts/kfctl.sh init $&#123;KFAPP&#125; --platform none # none 也可以是minikube等$&#123;KUBEFLOW_SRC&#125;/scripts/kfctl.sh generate k8s$&#123;KUBEFLOW_SRC&#125;/scripts/kfctl.sh apply k8s 查看是否运行好了：1234kubectl get pod -n kubeflow #理论上gcr.io的镜像pull不下来# 查看ImagePullBackOff等问题kubectl describe pod scheduledworkflow -n kubeflow #提示: Failed to pull image "gcr.io/ml-pipeline/scheduledworkflow:0.1.6" 所以这里需要代理了.. 设置国外代理的方法比较多，我这里使用的是VPS的方式。 想要删除 or 重新部署？直接删除kubeflow这个namespace和之前放置配置文件的文件夹就OK了 123kubectl delete ns kubeflowkubectl delete crd tfjobs.kubeflow.org # crd 不删除也行rm -rf $&#123;KFAPP&#125; 参考资料 官方文档 katacoda]]></content>
      <categories>
        <category>容器云kubeflow</category>
        <category>分布式机器学习</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>kubeflow</tag>
        <tag>分布式机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微观经济学--比较优势]]></title>
    <url>%2F2019%2F01%2F03%2F%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6-%E6%AF%94%E8%BE%83%E4%BC%98%E5%8A%BF%2F</url>
    <content type="text"><![CDATA[曼昆 《经济学原理》微观经济学分册学习小目标：说清楚绝对优势和比较优势，以及其与中美贸易战之间的关系 一、绝对优势假设，Frank和Lee都会两个技能“摊煎饼”，“做馒头”，显然，不同的人擅长做的事情也不一样，我们用“效率”来衡量这两个人分别做两件事情的擅长程度。但是，影响”效率”的因素太多了，比如：原材料的节约程度，产品最终的受欢迎程度….，为了简化衡量标准，我们假设他们做出的“煎饼”和“馒头”所付出的成本仅仅在制作时间上有差异。如果以单位时间作为单位成本的话，我们假设Frank一个小时平均可以摊6个煎饼或者做10个馒头，Lee一个小时平均可以做4个煎饼或者15个馒头，按照单位成本来计算（min/个）整理成下表则为： 生产成本（min/个） Frank Lee 煎饼 10 15 馒头 6 4 显然，成本越小，表明其优势越大，比如Frank摊煎饼的成本就比Lee小，而Lee做馒头的成本就比Frank小，这个优势在经济学里面就叫做绝对优势，比如，我们可以说，Frank在摊煎饼这项工作上具有绝对优势。 那就有人想说，干脆让Frank只做煎饼，Lee只做馒头好了，这样的话生产出来的煎饼和馒头的总和在单位时间内就会比他们分别生产煎饼和馒头要多。 可是，要达成这个目的还需要一个协议，Frank和Lee可以仅生产自己有绝对优势的产品，并且可以相互交换，而相互交换的规则则是“一个煎饼可以换10/6到15/4个馒头之间”，也就是定价规则。 因为，对于Frank来说一个煎饼至少得换10/6个馒头吧，不然还不如他自己做馒头呢，而对于Lee来说，一个煎饼最多可以换15/4个馒头，不然他就亏了。所以，煎饼和馒头的兑换比率应该是ratio（ 10/6=1.67 &lt; ratio &lt; 15/4=3.75） 此外，从“机会成本（opportunity cost）”的角度阐述这个问题也是一样的： 机会成本，生产A产品而不生产B产品，所舍弃的成本；这里Frank只生产煎饼，那么为了生产煎饼而放弃生产的馒头就是他的机会成本 - Frank Lee 1个煎饼的机会成本 10/6个馒头 15/4个馒头 1个馒头的机会成本 6/10个煎饼 4/15个煎饼 定价的规则就是保证在双方的机会成本之间。 二、比较优势专业化和贸易的好处不是基于绝对优势，而是基于比较优势。假设，Frank比较厉害，无论是摊煎饼还是做馒头都比Lee快： 生产成本（min/个） Frank Lee 煎饼 10 15 馒头 5 6 这时双方的机会成本为： - Frank Lee 1个煎饼的机会成本 12/6个馒头 10/4个馒头 1个馒头的机会成本 6/12个煎饼 4/10个煎饼 显然，如果一方在生产一种产品的机会成本比较低时，那他在生产另一种产品的机会成本就会比较高，因为两种产品的机会成本互为倒数。所以，Frank和Lee之间依然可以贸易，这时Frank生产他的机会成本比较小的煎饼，Lee生产他机会成本比较小的馒头。而交易的价格，也就是兑换的比率一就是在两个机会成本之间，12/6=2 &lt; 煎饼/馒头 &lt; 10/4=2.5 个。 简单来说就是，Frank即使两个工作都很擅长，但他肯定也还是有更擅长的工作（擅长中的擅长^_^），他只要做他最擅长的那一个就OK了；而贸易则可以使整体的生产效率提高，从而提高大家的物质生活水平。 不过理论上，这里可以有个小问题：如果Frank和Lee的机会成本一样，那还要不要贸易啊？？？ 三、联想 人类为什么需要贸易贸易使经济生产效率提高，物质生活水平自然也会更高。有了贸易，自然就需要保护贸易正常进行的组织（不能偷、不能抢别人的，只能换），于是政府和国家的概念和角色就出现了，注意这只是理想状态下。 为什么会有中美贸易战美国各方面都领先于中国，拥有绝对优势，但是其在生产服装、玩具、制造组装手机等方面的机会成本比中国高，而在生产高科技产品、农业产品的机会成本比中国低，所以中国向美国出口服装、玩具等，并从美国进口高科技产品、农产品等。可是，美国发现每年的中美贸易都出现逆差（对中国来说就是顺差），也就是说每年都有大量美元流入中国（中国美元的外汇储备之前一直比较高，2007年还搞了一个”中投”专门”研究”怎么花这笔钱…），为了扭转事态，美国开始加征关税（减少美国国内对中国的进口），中国当然不愿意看到这些，虽然人民币与美元并不挂钩，但是外汇储备对于中国政府来说仍然至关重要，于是中美就发生了trade war. 事实上，贸易往来总会出现不平衡的状况，一百多年前的鸦片战争也正是由于中英贸易存在类似的问题而导致的。 根据中国海关总署统计，美国对中国的贸易逆差从2001年的281亿美元增长到2017年2758亿美元。中美贸易总额已经从1992年的330亿美元发展到2017年的5837亿美元。由于统计口径差距，美国商务部统计数据，对中国逆差从2001年830亿美元增长到2017年3752亿美元，贸易总额增长到2017年6360亿美元。（维基百科） 人尽其才，才尽其用的使命比较优势告诉我们，每个人去做自己最擅长的事情，就会使经济蛋糕变大，这就让人联想到人尽其才，物尽其用这句古话了。但是这都是理论上的，现实的世界不存在真正的自由贸易，也不会存在人尽其才，物尽其用这种理想状态。 参考资料 经济学原理 — 微观经济学分册，曼昆 李永乐老师视频]]></content>
      <categories>
        <category>经济学原理</category>
      </categories>
      <tags>
        <tag>金融知识</tag>
        <tag>比较优势</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS上安装部署Kubernetes注意事项]]></title>
    <url>%2F2018%2F12%2F26%2FCentOS%E4%B8%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2Kubernetes%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[小目标：不翻墙的情况下，使用kubeadm安装部署Kubernetes集群（非高可用），1个master、2个node 一、准备工作关于机器： 准备的主机可以连接外网，对于私有云场景，需要做好前期准备（例如：配置yum源、镜像仓库源等） 满足安装 Docker 项目所需的要求，比如 64 位的 Linux 操作系统、3.10 及以上的内核版本； x86 或者 ARM 架构均可； 机器之间网络互通，这是将来容器之间网络互通的前提； 关于Linux操作系统： 建议开启root权限（我这里是已经开启了root权限，以root用户登录节点） 修改各节点的hostname：打开终端输入hostnamectl set-hostname master8088，这里的命名需要有一定的规范重启后hostname失效 建议：systemd不低于234，否则执行 df 命令的时候，据说会有一定几率卡死，使用systemctl --version查看版本信息 建议关闭swap输入swapoff -a：如果不满足，据说系统会有一定几率出现 io 飙升，造成 docker 卡死 关闭防火墙，终端输入systemctl stop firewalld &amp;&amp; systemctl disable firewalld 关闭selinux:vi /etc/selinux/config 设置SELINUX=disabled 修改hosts文件，vi /etc/hosts 123192.168.0.1 node2 # ip+hostname格式192.168.0.2 node1 # ip每个节点的ip地址，可以使用ifconfig命令查看192.168.0.3 master 添加相关设置vim /etc/sysctl.conf需要修改的内容如下所示： 1234net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1vm.max_map_count=262144 输入sysctl -p使设置生效 设置三台机器之间可以使用ssh+hostname互相登录， 节点之间无密码互相访问设置：1234ssh-keygencat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys#将每个节点的id_rsa.pub写入每个节点的authorized_keys#最后生成的authorized_keys复制到集群中的每一台计算机的.ssh目录下，覆盖掉之前的authorized_keys 二、安装部署Kubernetes 安装之前需要配置一下kubernetes这个yum源，否则下面的命令可能失效 在每个节点上安装kubeadm、kubelet、kubectl，这里选择的是CentOS系统，所以使用命令1yum install -y kubelet kubeadm kubectl 安装的kubeadm、kubectl、kubelet默认都是最新的版本（1.13版本），也可以指定版本，比如目前是stable版的1.111234567# 查看版本kubeadm versionkubectl versionkubelet --version# 下载安装指定版本yum list --showduplicates | grep 'kubeadm' #查看有哪些版本yum install -y kubeadm-1.10.5-0.x86_64 # 安装指定版本，这里选择的是1.10.5 部署master节点：这里需要注意的是，直接使用kubeadm init会发现需要的镜像获取不了，因为大陆被墙了.. 不过可以指定镜像仓库源，这里选择阿里云杭州的源（感谢^_^）：123# kubernetes-version=v1.13.1指定了安装1.13.1版本的kubernetes# pod-network-cidr是为了后续安装calico这样的网络插件kubeadm init --kubernetes-version=v1.13.1 --pod-network-cidr=192.168.0.0/16 --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers 可能出现kubelet和kubeapi-server失联的情况，注意排查master部署完成后，会生成一个指令：kubeadm join ....这个是后续加入node用的kubeadm还会在部署好master后，最后提示我们第一次使用kubernetes集群需要的配置命令：mkdir... sudo cp ... sudo chown... 部署node节点：参照master部署完毕生成的kubeadm join提示，在每个node上执行以下命令kubeadm join ${master_ip}:6443 --token ${kubeadm_token} --discovery-token-ca-cert-hash ${hash_value}使用kubectl get no查看node是否已经添加，并且处于Ready状态，由于网络插件还没安装，应该不会Ready 安装CNI插件calico 12kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yamlkubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml 删除master上的Taint标签，使之也可以被调度kubectl taint nodes --all node-role.kubernetes.io/master- 至此，一个kubernetes集群已经可以使用了，接下来还可以部署Dashboard、CSI插件 三、问题记录Q1：kubeadm一次运行没有通过，但是部分static Pod已经启动？打开终端输入：kubeadm reset，即可重置集群，修改必要的参数后，再次使用kubeadm init ...命令部署K8s集群。 Q2： 需要事先下载好国内镜像源吗？不需要 由于kubeadm在部署K8s集群时，需要从k8s.gcr.io上拉取镜像，但是大陆需要翻墙，所以有些博客里提出先下载好一样的镜像再修改tag以此绕开从国外拉取镜像的问题，但实际上没有必要这样做；即便如此，还是记录一下吧… 镜像下载脚本image_download.sh1234567891011121314151617181920212223242526272829#! bin/bashsource=registry.cn-hangzhou.aliyuncs.com/google_containersimages=(kube-apiserver:v1.13.1kube-controller-manager:v1.13.1kube-scheduler:v1.13.1kube-proxy:v1.13.1pause:3.1etcd:3.2.24coredns:1.2.6)for imageName in $&#123;images[@]&#125; ; do echo $imageName echo "------------------------------------" docker pull $source/$imageName docker tag $source/$imageName k8s.gcr.io/$imageNamedone 四、参考资料 极客时间-张磊-深入剖析Kubernetes kubeadm Creating a single master cluster with kubeadm]]></content>
      <categories>
        <category>容器云kubernetes</category>
        <category>Linux软件安装</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>kubeadm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图片识别app-从keras+flask代码到kubernetes部署]]></title>
    <url>%2F2018%2F12%2F06%2F%E5%9B%BE%E7%89%87%E8%AF%86%E5%88%ABapp%E4%BB%8Ekeras-flask%E4%BB%A3%E7%A0%81%E5%88%B0kubernetes%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[使用python flask 以及 keras建立一个简单的image recognition的工具,最后使用docker和kubernetes将应用容器化部署在PaaS平台上对外提供服务。 机器学习+kubernetes 使用python flask 以及 keras建立一个简单的image recognition的工具，主要参考了这里觉得有点意思就实现了一下，里面涉及到python编程、docker、k8s的使用，image recognition模型不涉及训练，使用的是开源模型，下次自己train个模型出来看看效果^_^。代码托管在github。 测试一下代码是否可用123456# 安装依赖模块，南七技校的pip源比较好用pip install -r requirements.txt -i https://mirrors.ustc.edu.cn/pypi/web/simple/ # 运行代码python app.py # 找个图片辨认一下curl -X POST -F image=@dog.jpg 'http://127.0.0.1:2400/predict 首次运行代码后，需要等待一段时间，因为要下载图片识别的模型 制作docker镜像1sudo docker build -t keras-app:latest . 出现如下提示则镜像制作成功12345Successfully built pyyaml gast absl-py termcolor MarkupSafe......Removing intermediate container 3a21aa77c06cSuccessfully built fc03d48b4096 查看一下镜像信息：123[conan@localhost deeplearning_flask]$ sudo docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEkeras-app latest fc03d48b4096 43 minutes ago 1.7 GB Size 1.7G，Dockerfile中可以修改python基础镜像为pythom3.6-slim进行瘦身. 测试docker镜像123456# 运行sudo docker run --name image-recon -d -p 2400:2400 keras-app:latest# 测试curl -X POST -F image=@dog.jpg 'http://127.0.0.1:2400/predict'# 打包带走sudo docker save keras-app:latest &gt; keras-app.tar 在kubernetes上调度使用k8s集群部署应用，推荐使用yaml文件，前置条件是把刚刚的镜像push到k8s使用的镜像仓库中这里使用简单一点的deployment方式来部署imagerecon_deployment_test_for_fun.yaml123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: imagerecon-deployment labels: app: imagereconspec: replicas: 1 selector: matchLabels: app: imagerecon template: metadata: labels: app: imagerecon spec: containers: - name: imagerecon image: keras-app:latest ports: - containerPort: 2400 部署123kubectl create -f imagerecon_deployment_test_for_fun.yaml# 查看kubectl get pods --show-labels 参考资料https://medium.com/analytics-vidhya/deploy-your-first-deep-learning-model-on-kubernetes-with-python-keras-flask-and-docker-575dc07d9e76]]></content>
      <categories>
        <category>容器云kubernetes</category>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>python</tag>
        <tag>机器学习</tag>
        <tag>flask</tag>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[囚徒困境回顾]]></title>
    <url>%2F2018%2F11%2F07%2F%E5%9B%9A%E5%BE%92%E5%9B%B0%E5%A2%83%E5%9B%9E%E9%A1%BE%2F</url>
    <content type="text"><![CDATA[如题，博弈论常识 甲囚犯和乙囚犯分别被审讯 甲和乙都选择合作（拒不认罪或者不指认乙），那么他们会分别被判处3年；甲选择背叛（指认乙）而乙选择合作，则甲将会被无罪释放，而乙则被判处5年；甲和乙都选择背叛（相互指认）则各获刑1年 审讯的结果是， 甲\判刑时间\乙 合作 背叛 合作 3, 3 0, 5 背叛 5, 0 1, 1 那么针对以上的规则，甲理性思考会依据对方的选择来判断自己的处境： 乙背叛：那么甲选择合作就会被判5年，选择背叛就会被判1年； 乙合作：那么甲选择合作就会被判3年，选择背叛就会被判0年； 显然，无论乙怎么选，甲选择背叛都是最优解 那么就此判断在竞争中，选择背叛就一定是对自己有利的吗？不一定吧… 我们改一下游戏规则（规则二）： 审讯的结果是， 甲\判刑时间\乙 合作 背叛 合作 0，0 0, 10 背叛 10, 0 5, 5 那么甲再次理性思考同样会依据对方的选择来判断自己的处境： 乙背叛：那么甲选择合作就是被判10年，选择背叛就是被判5年； 乙合作：那么甲选择合作就是被判0年，选择背叛也是被判0年； 显然，无论乙怎么选，甲选择背叛还是是最优解 再次改一下规则（规则三）： 审讯的结果是， 甲\判刑时间\乙 合作 背叛 合作 0，0 0, 10 背叛 10, 0 15, 15 那么甲再次理性思考还是会依据对方的选择来判断自己的处境： 乙背叛：那么甲选择合作就是被判10年，选择背叛就是被判15年； 乙合作：那么甲选择合作就是被判0年，选择背叛也是被判0年； 显然，无论乙怎么选，甲选择背叛就不再是最优解了 所以说游戏规则会改变囚徒困境的结果…？ 思考假设第三条规则不那么合理，那么什么样的规则才是合理的呢？市场行为中有多方参与，比简单的两者博弈要复杂很多… 即使如此，囚徒困境的模型还是有一些指导意义的]]></content>
      <categories>
        <category>经济学原理</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>囚徒困境</tag>
        <tag>金融知识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django安装与使用]]></title>
    <url>%2F2018%2F08%2F30%2FDjango%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[一、Django简介Django是一个开源的web开发框架，该框架使用的是python语言。其集成了所有web开发需要的组件，开发测试速度极快，所以又被称为all-in-one类型的web开发框架。此外，Django还有Secure、Scalable、Maintainable等特性。Django于2017年发布2.0版本。 这里列出了很多Django的特点/优点；Django早在2003-2005年间被一个新闻网站的开发团队开发出来，于2005年7月开源，并命名为Django；可以推测出，Django在做文本型的网站开发上是有优势的，比如：博客、图书馆等。 二、Django安装Step1.安装python安装python会涉及版本的选择问题，这里我选择的是python3，因为python2和python3的语法略有不同，python3没有做向下兼容，而python2又将不再被官方更新维护了，所以对于我这种新手还是学学python3吧^_^。 python的官方网站提供了多种平台和版本的下载 对于python3的安装，我选择的是anaconda，anaconda是一个开源的python发行版，里面包含很多常用模块，安装这个省去了后续很多模块安装的麻烦；当然，如果想多了解python也可以使用python安装包进行安装。 anaconda安装过程123wget http://repo.continuum.io/archive/Anaconda3-5.2.0-Linux-x86_64.sh #在官网下载可以下3.6的版本（2018.08.30）sh Anaconda3-5.2.0-Linux-x86_64.sh #sh可以改为bash，安装的最后会让你选择安装微软的一款IDE产品，可以选择nosource ~/.bashrc #刷新一下环境变量 输入命令测试一下，出现Anaconda，Inc，成功12345zhuz@qwerty:~$ python3Python 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) [GCC 7.2.0] on linuxType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; windows下安装相对简单，不再赘述。 注意1.anaconda自带了Jupyter和Spyder，前者是notebook，后者是IDE，都挺好用的；2.Linux自带了python，如果自带的是python2.7，也可以考虑安装一个python3或者anaconda，但是需要解决一下版本管理问题；3.windows系统下，需要在安装好python后添加路径至环境变量，这样就可以在命令行内使用了 Step2.安装Django模块 2.1.使用pip或conda安装pip和conda安装方法是一样的12pip install django #pip是python的安装包管理工具,类似于yum和Centos的关系conda install django #conda是anaconda的安装包管理工具 如果使用pip3安装的时候提示pip3版本过低，需要先更新pip3的版本1pip3 install --upgrade pip #注意不是pip3 install --upgrade pip3 2.2.使用pycharm安装pycharm是一款IDE，由捷克公司JetBrains开发，也是python的主流IDE，其内置了很多功能，使用方便。打开pycharm，依次点击file-&gt;settings，出现如下界面，点击+号按钮，搜索框输入django，选中出现的模块包，右侧便会出现相应的模块信息，接下来点击Install Package即可，如下图所示: 注意1.windows系统下，需要在安装好Django后添加环境变量2.如果使用的是pycharm安装Django，则需要另外添加anaconda/pkgs/django-2.0.5-py36hd476221_0/Scripts到环境变量，这样比较麻烦，不如用conda来安装；3.conda只能安装python的官方包 2.3.添加环境变量Linux下已经自动添加了环境变量；windows10下添加环境变量的方法此电脑-&gt;右键属性-&gt;高级系统设置-&gt;高级-&gt;环境变量-&gt;用户变量中双击Path-&gt;新建如下图所示： 图中红色矩形框即为Anaconda在本机上的路径，这里需要添加上面的3个路径 三、Django实践这里使用Django创建一个简单的web项目，一方面是学习一下Django的框架，另一方面测试一下Django是否可用。 框架理论Django框架是MTV模式（model+template+view），和MVC的模式基本一样，可以参考阮一峰的博客学习一下下面直接看图： 图中显示了HTTP Request到HTTP Response的过程，首先通过URLS来找到View的路径，而View是由Template（比如一个html模板）+data合并得到的，data是由Model得来（所以Model是用来操作数据库的）。具体说来，Django分为几大块（摘自书签1）： URLs: While it is possible to process requests from every single URL via a single function, it is much more maintainable to write a separate view function to handle each resource. A URL mapper is used to redirect HTTP requests to the appropriate view based on the request URL. The URL mapper can also match particular patterns of strings or digits that appear in an URL, and pass these to a view function as data. View: A view is a request handler function, which receives HTTP requests and returns HTTP responses. Views access the data needed to satisfy requests via models, and delegate the formatting of the response to templates. Models: Models are Python objects that define the structure of an application’s data, and provide mechanisms to manage (add, modify, delete) and query records in the database. Templates: A template is a text file defining the structure or layout of a file (such as an HTML page), with placeholders used to represent actual content. A view can dynamically create an HTML page using an HTML template, populating it with data from a model. A template can be used to define the structure of any type of file; it doesn’t have to be HTML! 项目测试pycharm新建一个项目：依次点击file-&gt;New Project-&gt;Django-&gt;Location(命名)-&gt;Create即可命令行新建一个项目：12python3 -m django --version #先看一下版本django-admin startproject mysite #创建一个名为mysite的项目 看一下项目的目录：1234567891011zhuz@qwerty:~/Just_test$ tree mysite/mysite/├── manage.py└── mysite ├── __init__.py ├── settings.py ├── urls.py └── wsgi.py1 directory, 5 fileszhuz@qwerty:~/Just_test$ 创建一个app1python3 manage.py startapp catalog 看一下app的目录123456789101112zhuz@qwerty:~/Just_test/mysite$ tree catalog/catalog/├── admin.py├── apps.py├── __init__.py├── migrations│ └── __init__.py├── models.py├── tests.py└── views.py1 directory, 7 files 项目 V.S. 应用应用（app）是一个专门做某件事的网络应用程序——比如博客系统，或者公共记录的数据库，或者简单的投票程序。项目（project）则是一个网站使用的配置和应用的集合。项目可以包含很多个应用；应用可以被很多个项目使用。 在/mysite/mysite/setting.py文件内“注册”一下刚刚创建的app，写入app的Config路径：123456789INSTALLED_APPS = [ 'polls.apps.PollsConfig', 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'catalog.apps.CatalogConfig', #CatalogConfig在catalog下的apps.py文件内，这个文件下app的名字已经写好了] 看到setting.py中的DATABASES默认使用的是sqlite3，这里就不用改了；需要改的是时区，还是在setting.py文件内，找到TIME_ZONE修改：1TIME_ZONE = 'Asia/Shanghai' 接下来写一下URL映射，我们将project中的url导向app自己的url，打开mysite/mysite/urls.py写入url的相对路径：123456from django.urls import path, include #默认没有添加includeurlpatterns = [ path('admin/', admin.site.urls), #原本自带的信息 path('catalog/', include('catalog.urls')), #app的相对路径 path('', RedirectView.as_view(url='/catalog/', permanent=True)),] + static(settings.STATIC_URL, document_root=settings.STATIC_ROOT) 这样一来，我们就需要在catalog这个app下新建一个urls.py文件1234567from django.urls import path from catalog import views #导入的这两个包以后会用到的urlpatterns = [] OK啦，接下来可以测试运行一下了12python3 manage.py makemigrationspython3 manage.py migrate migrate：迁移是非常强大的功能，它能让你在开发过程中持续的改变数据库结构而不需要重新删除和创建表 - 它专注于使数据库平滑升级而不会丢失数据。 运行1python3 manage.py runserver 接下来就可以访问http://127.0.0.1:8000/了，注意Chrome浏览器使用时，需要先检查一下代理，有些同学在翻墙的时候将代理端口改了一下导致不能连接… 生产环境下web搭建一般会需要LEMP(LNMP)或者LAMP环境，即由Nginx或者Apache作为前端提供http服务，mysql提供数据库服务；但Django自带了一个轻量级的webserver进行开发调试，如需调整为生产环境，可以调换自带的webserver 四、总结 1.Django是一个python的web开发框架，因此需要先搭建好python环境； 2.可以使用Anaconda安装python环境，需要修改一下系统的环境变量； 3.windows下使用pycharm添加的Django包需要单独添加环境变量，所以不建议这样安装； 4.Django自带轻量级webserver，所以无需其他组件即可搭建web app； 五、书签 Mozilla Django Tutorial Django官方中文文档 Django测试与生产环境讨论1 Django测试与生产环境讨论2 MVC模式]]></content>
      <categories>
        <category>python</category>
        <category>web开发框架</category>
      </categories>
      <tags>
        <tag>web 框架</tag>
        <tag>python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单机版SuSe搭建Confluence Wiki]]></title>
    <url>%2F2018%2F08%2F15%2F%E5%8D%95%E6%9C%BA%E7%89%88SuSe%E6%90%AD%E5%BB%BAConfluence-Wiki%2F</url>
    <content type="text"><![CDATA[单机 SuSe confluence Wiki Wiki，作为知识管理的工具，实现团队成员之间的协作和知识共享。下面主要介绍三部分内容： 一.安装过程主要参考的博客地址这篇博客每一个步骤都讲的非常清楚，按照步骤来，肯定没有问题。可能出现的问题我将写在第二部分里面。 二.安装过程的注意事项安装软件之前需要添加SuSe的源，尽量避免下载源码自己编译；另外，如果每次都让管理员那边挂载CD源，效率也会比较低。 下载配置SuSe源 点击这里即可进入软件源镜像站点选择一个镜像地址，推荐国内的USTC源：）建议同时下载leap和tumbleweed镜像，每个镜像大小大概4G多一点 1.将下载好的iso文件上传至目标主机，低于4G的文件可以使用sz命令（前提是lszrz软件已经安装好了），超过4G的文件需要使用ftp进行上传，注意ftp端口的设置 鉴于公司网络安全方面的设置，我习惯用自己的电脑连接外网，公司电脑连接内网，这样查询资料或者下载文件的同时也可以使用内网比较方便。这样就会涉及到两台电脑传输数据的问题，如果公司电脑USB有加密传输设置则建议使用网线连接进行文件传输。另外，查询资料建议使用Google。 2.挂载和配置Suse源1234mkdir -p /mnt/localsuse01 #新建一个存放文件的挂载点mount -o loop imagefilename.iso /mnt/localsuse01 #挂载到刚刚建立的文件夹zypper ar file:///mnt/localsuse01 localsuse01 #添加源zypper lr #查看添加的源 这样的设置会导致电脑重启后需要重新手动挂载，所以可以设置成开机自动挂载。 mysql安装和配置 1.使用suse安装软件12zypper se mysql #查看源里面是否有mysql的包zypper install mysql #或者mariadb 2.安装完成后，启动mysql服务systemctl start mariadb或者systemctl start mysqld3.登录mysql：mysql -uname -p先设置数据库密码按照参考的博客输入内容：1234create database confluence default character set utf8;grant all on confluence.* to 'confluenceuser'@'%' identified by 'confluencepasswd' with grant option;grant all on confluence.* to 'confluenceuser'@localhost identified by 'confluencepasswd' with grant option;flush privileges; 修改mysql的配置文件，SuSe默认安装mysql到/usr/share/mysql文件夹，12zhuz@qwerty:~$ mysql --help|grep 'my.cnf'#查看默认配置文件的位置/etc/my.cnf /etc/mysql/my.cnf ~/.my.cnf #这个是按照优先级排列的 所以在/etc/my.cnf中添加：binlog_format=mixed4.重启mysql服务：12service mariadb stopservice mariadb start 关闭防火墙 1SuSEfirewall2 stop 配置VNC 由于后续安装步骤需要用到windowX图形界面服务，而我们是通过CRT或者putty远程连接登录服务器进行操作的，这里推荐使用VNC viewer在服务器电脑上安装vnc服务，并开启即可使用，其间可能会遇到各式各样的问题，需要自行Google一下 正式安装confluence这里的安装部分全程比较简单，按照前面提到的blog内容step by step即可成功，需要注意的是安装的目录可以自行选择。再贴下blog地址https://segmentfault.com/a/1190000008753391#articleHeader10 三.运行维护1.安装后的文件位于/confluence/atlassian/confluence文件夹下，将该目录下的bin目录加入系统的环境变量echo &#39;export PATH=$PATH:/confluence/atlassian/confluence/bin&#39; &gt;&gt; /etc/profile2.由于Wiki服务没有手动加入守护进程，所以可能会由于一些原因导致其停止工作，比如电脑重启，这时需要重启Wiki服务：123sudo systemctl start mariadb #1.启动mysqlSuSEfirewall2 stop #2.关闭防火墙start_up.sh #3.最后开启wiki 四.书签CentOS7搭建Confluence Wikihttps://segmentfault.com/a/1190000008753391#articleHeader10]]></content>
      <categories>
        <category>Linux软件安装</category>
      </categories>
      <tags>
        <tag>Confluence Wiki</tag>
        <tag>Linux</tag>
        <tag>Suse</tag>
      </tags>
  </entry>
</search>
