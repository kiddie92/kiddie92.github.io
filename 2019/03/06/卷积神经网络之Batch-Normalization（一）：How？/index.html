<!DOCTYPE html>
<html lang="zh-cn">
  <head><meta name="generator" content="Hexo 3.8.0">
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">

<meta name="theme-color" content="#f8f5ec">
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">



  <meta name="description" content="卷积神经网络之Batch Normalization（一）：How？">




  <meta name="keywords" content="算法, 深度学习, CNN, 人工智能, 神经网络, kiddie92">










  <link rel="alternate" href="https://feedity.com/github-io/UlJXUVFQUg.rss" title="kiddie92">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.10.2">



<link rel="canonical" href="https://kiddie92.github.io/2019/03/06/卷积神经网络之Batch-Normalization（一）：How？/">



  <link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css">




  <link rel="stylesheet" type="text/css" href="/lib/nprogress/nprogress.min.css">



<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.10.2">



  


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-91728997-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-91728997-1');
</script>


  <script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>





  <script src="//cdn1.lncld.net/static/js/3.1.1/av-min.js"></script>
  <script id="leancloud">
    AV.init({
      appId: "n8MGlOWz83zS9G0m0PGinErj-gzGzoHsz",
      appKey: "bSOcrmi2W53fTzNXQA3UQ2z3"
    });
  </script>





<script>
  window.config = {"leancloud":{"app_id":"n8MGlOWz83zS9G0m0PGinErj-gzGzoHsz","app_key":"bSOcrmi2W53fTzNXQA3UQ2z3"},"toc":true,"fancybox":true,"pjax":true};
</script>

    <title> 卷积神经网络之Batch Normalization（一）：How？ - kiddie92 </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="mobile-header-logo">
    <a href="/." class="logo">kiddie92</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    
      <a href="/">
        <li class="mobile-menu-item">
          
          
            首页
          
        </li>
      </a>
    
      <a href="/archives/">
        <li class="mobile-menu-item">
          
          
            归档
          
        </li>
      </a>
    
      <a href="/tags">
        <li class="mobile-menu-item">
          
          
            标签
          
        </li>
      </a>
    
      <a href="/categories">
        <li class="mobile-menu-item">
          
          
            分类
          
        </li>
      </a>
    
      <a href="/Open-source">
        <li class="mobile-menu-item">
          
          
            开源项目
          
        </li>
      </a>
    
      <a href="/links">
        <li class="mobile-menu-item">
          
          
            链接
          
        </li>
      </a>
    
      <a href="/about">
        <li class="mobile-menu-item">
          
          
            关于
          
        </li>
      </a>
    
  </ul>
</nav>

    <div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">kiddie92</a>
</div>

<nav class="site-navbar">
  
    <ul id="menu" class="menu">
      
        <li class="menu-item">
          <a class="menu-item-link" href="/">
            
            
              首页
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            
            
              归档
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/tags">
            
            
              标签
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/categories">
            
            
              分类
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/Open-source">
            
            
              开源项目
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/links">
            
            
              链接
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/about">
            
            
              关于
            
          </a>
        </li>
      
    </ul>
  
</nav>

      </header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            
  
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          卷积神经网络之Batch Normalization（一）：How？
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-03-06
        </span>
        
          <span class="post-category">
            
              <a href="/categories/人工智能/">人工智能</a>
            
          </span>
        
        
        <span class="post-visits" data-url="/2019/03/06/卷积神经网络之Batch-Normalization（一）：How？/" data-title="卷积神经网络之Batch Normalization（一）：How？">
          阅读次数 0
        </span>
        
      </div>
    </header>

    
    
  <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">文章目录</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#批量归一化"><span class="toc-text">批量归一化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#批量归一化层"><span class="toc-text">批量归一化层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#对全连接层做批量归一化"><span class="toc-text">对全连接层做批量归一化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#对卷积层做批量归一化"><span class="toc-text">对卷积层做批量归一化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#测试-预测时的批量归一化"><span class="toc-text">测试/预测时的批量归一化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#tensorflow调用"><span class="toc-text">tensorflow调用</span></a></li></ol>
    </div>
  </div>



    <div class="post-content">
      
        <blockquote>
<p>本文主要介绍深度学习里的一个常用的trick，主要用于加速收敛算法，这篇主要介绍一下怎么做的（How），下篇再介绍Why和该算法的一些好处，本来想着根据自己的理解写一下，看了大神写的之后我就决定”抄袭了”，（大神就是大神啊…）。原文使用MXNet实现的算法（原文查看文末的<strong>原文链接</strong>），这里改成使用TensorFlow实现一下这个例子。<br><a id="more"></a></p>
</blockquote>
<h1 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h1><p>这一节我们介绍批量归一化（batch normalization）层，它能让较深的神经网络的训练变得更加容易。在“实战 Kaggle 比赛：预测房价”一节里，我们对输入数据做了标准化处理：处理后的任意一个特征在数据集中所有样本上的均值为 0、标准差为 1。标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。</p>
<p>通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但对于深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。</p>
<p>批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使得整个神经网络在各层的中间输出的数值更稳定。批量归一化和下一节将要介绍的残差网络为训练和设计深度模型提供了两类重要思路。</p>
<h2 id="批量归一化层"><a href="#批量归一化层" class="headerlink" title="批量归一化层"></a>批量归一化层</h2><p>对全连接层和卷积层做批量归一化的方法稍有不同。下面我们将分别介绍这两种情况下的批量归一化。</p>
<h3 id="对全连接层做批量归一化"><a href="#对全连接层做批量归一化" class="headerlink" title="对全连接层做批量归一化"></a>对全连接层做批量归一化</h3><p>我们先考虑如何对全连接层做批量归一化。通常，我们将<strong>批量归一化层置于全连接层中的仿射变换和激活函数之间</strong>。设全连接层的输入为$\boldsymbol{u}$，权重参数和偏差参数分别为 <script type="math/tex">\boldsymbol{W}</script> 和 <script type="math/tex">\boldsymbol{b}</script>，激活函数为 <script type="math/tex">\phi</script>。设批量归一化的操作符为 <script type="math/tex">\text{BN}</script>。那么，使用批量归一化的全连接层的输出为</p>
<script type="math/tex; mode=display">\phi(\text{BN}(\boldsymbol{x})),</script><p>其中批量归一化输入 <script type="math/tex">\boldsymbol{x}</script> 由仿射变换</p>
<script type="math/tex; mode=display">\boldsymbol{x} = \boldsymbol{W\boldsymbol{u} + \boldsymbol{b}}</script><p>得到。考虑一个由 <script type="math/tex">m</script> 个样本组成的小批量，仿射变换的输出为一个新的小批量 <script type="math/tex">\mathcal{B} = \{\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(m)} \}</script>。它们正是批量归一化层的输入。对于小批量 <script type="math/tex">\mathcal{B}</script> 中任意样本 <script type="math/tex">\boldsymbol{x}^{(i)} \in \mathbb{R}^d, 1 \leq  i \leq m</script>，批量归一化层的输出同样是 <script type="math/tex">d</script> 维向量</p>
<script type="math/tex; mode=display">\boldsymbol{y}^{(i)} = \text{BN}(\boldsymbol{x}^{(i)}),</script><p>并由以下几步求得。首先，对小批量 <script type="math/tex">\mathcal{B}</script> 求均值和方差：</p>
<script type="math/tex; mode=display">\boldsymbol{\mu}_\mathcal{B} \leftarrow \frac{1}{m}\sum_{i = 1}^{m} \boldsymbol{x}^{(i)},</script><script type="math/tex; mode=display">\boldsymbol{\sigma}_\mathcal{B}^2 \leftarrow \frac{1}{m} \sum_{i=1}^{m}(\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B})^2,</script><p>其中的平方计算是按元素求平方。接下来，我们使用按元素开方和按元素除法对 <script type="math/tex">\boldsymbol{x}^{(i)}</script> 标准化：</p>
<script type="math/tex; mode=display">\hat{\boldsymbol{x}}^{(i)} \leftarrow \frac{\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B}}{\sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}},</script><p>这里 <script type="math/tex">\epsilon > 0</script> 是一个很小的常数，保证分母大于 0。在上面标准化的基础上，<strong>批量归一化层引入了两个可以学习的模型参数，拉伸（scale）参数 <script type="math/tex">\boldsymbol{\gamma}</script> 和偏移（shift）参数 <script type="math/tex">\boldsymbol{\beta}</script></strong>。这两个参数和 <script type="math/tex">\boldsymbol{x}^{(i)}</script> 形状相同，皆为 <script type="math/tex">d</script> 维向量。它们与 <script type="math/tex">\boldsymbol{x}^{(i)}</script> 分别做按元素乘法（符号 <script type="math/tex">\odot</script>）和加法计算：</p>
<script type="math/tex; mode=display">{\boldsymbol{y}}^{(i)} \leftarrow \boldsymbol{\gamma} \odot \hat{\boldsymbol{x}}^{(i)} + \boldsymbol{\beta}.</script><p>至此，我们得到了 <script type="math/tex">\boldsymbol{x}^{(i)}</script> 的批量归一化的输出 <script type="math/tex">\boldsymbol{y}^{(i)}</script>。<br>值得注意的是，可学习的拉伸和偏移参数保留了不对 <script type="math/tex">\hat{\boldsymbol{x}}^{(i)}</script> 做批量归一化的可能：此时只需学出 <script type="math/tex">\boldsymbol{\gamma} = \sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}</script> 和 <script type="math/tex">\boldsymbol{\beta} = \boldsymbol{\mu}_\mathcal{B}</script>。我们可以对此这样理解：<strong>如果批量归一化无益，理论上学出的模型可以不使用批量归一化。</strong></p>
<h3 id="对卷积层做批量归一化"><a href="#对卷积层做批量归一化" class="headerlink" title="对卷积层做批量归一化"></a>对卷积层做批量归一化</h3><p>对卷积层来说，批量归一化<strong>发生在卷积计算之后、应用激活函数之前</strong>。如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数，且均为标量。设小批量中有 <script type="math/tex">m</script> 个样本。在单个通道上，假设卷积计算输出的高和宽分别为 <script type="math/tex">p</script> 和 <script type="math/tex">q</script>。我们需要对该通道中 <script type="math/tex">m \times p \times q</script> 个元素同时做批量归一化。对这些元素做标准化计算时，我们使用相同的均值和方差，即该通道中 <script type="math/tex">m \times p \times q</script> 个元素的均值和方差。</p>
<h3 id="测试-预测时的批量归一化"><a href="#测试-预测时的批量归一化" class="headerlink" title="测试/预测时的批量归一化"></a>测试/预测时的批量归一化</h3><p>使用批量归一化训练时，我们<strong>可以将批量大小设的大一点</strong>，从而使批量内样本的均值和方差的计算都较为准确。将训练好的模型用来预测/测试时，我们希望模型对于任意输入都有确定的输出。因此，单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差。一种常用的方法是<strong>通过移动平均估算整个训练数据集的样本均值和方差</strong>，并在预测时使用它们得到确定的输出。可见，和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。</p>
<h1 id="tensorflow调用"><a href="#tensorflow调用" class="headerlink" title="tensorflow调用"></a>tensorflow调用</h1><p>根据上面的讲解，运算流程应该是，输入神经元（neural）的数据先做<script type="math/tex">w \cdot u+b</script>得到<script type="math/tex">x_i</script>，再按照上面的公式对一个batch内的<script type="math/tex">x_i</script>进行normalization并接着scale和shift，之后再对其进行激活得到<script type="math/tex">z_i</script>。</p>
<script type="math/tex; mode=display">
u_{i} -仿射变换-> x_i -标准化-> \hat{x_i} -拉伸和偏移-> y_i --> activation --> z_i</script><p>下面，使用mnist手写数字识别为例，按照这个流程走一遍吧，在tensorflow中调用使用的是<code>tf.layers.batch_normalization</code>，完整代码请看<a href="https://github.com/kiddie92/Learning_Tech/tree/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8BBatch-Normalization%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9AHow%EF%BC%9F" target="_blank" rel="noopener">这里</a>(<strong>使用jupter-notebook查看</strong>)：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">epsilon = <span class="number">0.001</span></span><br><span class="line">Wx_plus_b = tf.layers.batch_normalization(Wx_plus_b, mean, var, shift, scale, epsilon)</span><br><span class="line"><span class="comment"># similar with this two steps:</span></span><br><span class="line"><span class="comment"># Wx_plus_b = (Wx_plus_b - fc_mean) / tf.sqrt(fc_var + 0.001)</span></span><br><span class="line"><span class="comment"># Wx_plus_b = Wx_plus_b * scale + shift</span></span><br></pre></td></tr></table></figure></p>
<p><strong>转载自：</strong>动手学深度学习<br><strong>原文网址：</strong><a href="https://zh.gluon.ai/chapter_convolutional-neural-networks/batch-norm.html" target="_blank" rel="noopener">https://zh.gluon.ai/chapter_convolutional-neural-networks/batch-norm.html</a><br><strong>作者：</strong>阿斯顿·张、<strong>李沐</strong>、扎卡里 C. 立顿、亚历山大 J. 斯莫拉</p>

      
    </div>

    
      
      



      
      
  <div class="post-reward">
    <input type="checkbox" name="reward" id="reward" hidden>
    <label class="reward-button" for="reward">赞赏支持</label>
    <div class="qr-code">
      
      
        <label class="qr-code-image" for="reward">
          <img class="image" src="/images/reward/wechat.png" title="微信打赏">
        </label>
      
      
        <label class="qr-code-image" for="reward">
          <img class="image" src="/images/reward/alipay.png" title="支付宝打赏">
        </label>
      
    </div>
  </div>

    

    
      <footer class="post-footer">
        
          <div class="post-tags">
            
              <a href="/tags/算法/">算法</a>
            
              <a href="/tags/深度学习/">深度学习</a>
            
              <a href="/tags/CNN/">CNN</a>
            
              <a href="/tags/人工智能/">人工智能</a>
            
              <a href="/tags/神经网络/">神经网络</a>
            
          </div>
        
        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2019/03/09/卷积神经网络之Batch-Normalization（二）：Why？/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">卷积神经网络之Batch-Normalization（二）：Why？</span>
        <span class="prev-text nav-mobile">上一篇</span>
      </a>
    
    
      <a class="next" href="/2019/03/05/go语言生成网址二维码/">
        <span class="next-text nav-default">go语言为网站生成二维码</span>
        <span class="prev-text nav-mobile">下一篇</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>


      </footer>
    

  </article>


          </div>
          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
      </main>

      <footer id="footer" class="footer">

  <div class="social-links">
    
      
        
          <a href="mailto:kiddiezzh@gmail.com" class="iconfont icon-email" title="email"></a>
        
      
    
      
    
      
        
          <a href="https://twitter.com/kiddiezzh" class="iconfont icon-twitter" title="twitter"></a>
        
      
    
      
    
      
    
      
    
      
        
          <a href="https://github.com/kiddie92" class="iconfont icon-github" title="github"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    

    
      <a href="https://feedity.com/github-io/UlJXUVFQUg.rss" class="iconfont icon-rss" title="rss"></a>
    
  </div>



<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://hexo.io/">Hexo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">
    
    &copy; 
     
      2018 - 
    
    2019

    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">kiddie92</span>
  </span>
</div>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>

    
  
  <script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'https://kiddie92.github.io/2019/03/06/卷积神经网络之Batch-Normalization（一）：How？/';
        this.page.identifier = '2019/03/06/卷积神经网络之Batch-Normalization（一）：How？/';
        this.page.title = '卷积神经网络之Batch Normalization（一）：How？';
    };
    (function() {
    var d = document, s = d.createElement('script');

    s.src = '//https-kiddie92-github-io.disqus.com/embed.js';

    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();  
  </script>

  

  



    
  



  
  





  
    <script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  

  
    <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  

  
    <script type="text/javascript" src="/lib/pjax/jquery.pjax.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/nprogress/nprogress.min.js"></script>
  


    <script type="text/javascript" src="/js/src/even.js?v=2.10.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  </body>
</html>
