<!DOCTYPE html>
<html lang="zh-cn">
  <head><meta name="generator" content="Hexo 3.8.0">
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">

<meta name="theme-color" content="#f8f5ec">
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">



  <meta name="description" content="TensorFlow 特性--Graph 和 Sessions">




  <meta name="keywords" content="tensorflow, 机器学习, Mun*">










  <link rel="alternate" href="/atom.xml" title="Mun*">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.10.2">



<link rel="canonical" href="https://kiddie92.github.io/2019/03/02/TensorFlow-特性-Graph-和-Sessions/">



  <link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css">




  <link rel="stylesheet" type="text/css" href="/lib/nprogress/nprogress.min.css">



<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.10.2">



  


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-91728997-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-91728997-1');
</script>


  <script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>





  <script src="//cdn1.lncld.net/static/js/3.1.1/av-min.js"></script>
  <script id="leancloud">
    AV.init({
      appId: "n8MGlOWz83zS9G0m0PGinErj-gzGzoHsz",
      appKey: "bSOcrmi2W53fTzNXQA3UQ2z3"
    });
  </script>





<script>
  window.config = {"leancloud":{"app_id":"n8MGlOWz83zS9G0m0PGinErj-gzGzoHsz","app_key":"bSOcrmi2W53fTzNXQA3UQ2z3"},"toc":true,"fancybox":true,"pjax":true};
</script>

    <title> TensorFlow 特性--Graph 和 Sessions - Mun* </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="mobile-header-logo">
    <a href="/." class="logo">Mun*</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    
      <a href="/">
        <li class="mobile-menu-item">
          
          
            首页
          
        </li>
      </a>
    
      <a href="/archives/">
        <li class="mobile-menu-item">
          
          
            归档
          
        </li>
      </a>
    
      <a href="/tags">
        <li class="mobile-menu-item">
          
          
            标签
          
        </li>
      </a>
    
      <a href="/categories">
        <li class="mobile-menu-item">
          
          
            分类
          
        </li>
      </a>
    
      <a href="/links">
        <li class="mobile-menu-item">
          
          
            链接
          
        </li>
      </a>
    
      <a href="/about">
        <li class="mobile-menu-item">
          
          
            关于
          
        </li>
      </a>
    
  </ul>
</nav>

    <div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Mun*</a>
</div>

<nav class="site-navbar">
  
    <ul id="menu" class="menu">
      
        <li class="menu-item">
          <a class="menu-item-link" href="/">
            
            
              首页
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            
            
              归档
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/tags">
            
            
              标签
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/categories">
            
            
              分类
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/links">
            
            
              链接
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/about">
            
            
              关于
            
          </a>
        </li>
      
    </ul>
  
</nav>

      </header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            
  
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          TensorFlow 特性--Graph 和 Sessions
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-03-02
        </span>
        
          <span class="post-category">
            
              <a href="/categories/tensorflow/">tensorflow</a>
            
              <a href="/categories/tensorflow/机器学习/">机器学习</a>
            
          </span>
        
        
        <span class="post-visits" data-url="/2019/03/02/TensorFlow-特性-Graph-和-Sessions/" data-title="TensorFlow 特性--Graph 和 Sessions">
          阅读次数 0
        </span>
        
      </div>
    </header>

    
    
  <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">文章目录</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#背景介绍"><span class="toc-text">背景介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#计算图（Graph）"><span class="toc-text">计算图（Graph）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#张量（Tensor）"><span class="toc-text">张量（Tensor）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#算子（Operation）"><span class="toc-text">算子（Operation）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#会话-（Session）"><span class="toc-text">会话 （Session）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据流-（Dataflow）"><span class="toc-text">数据流 （Dataflow）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实践-—-回归问题"><span class="toc-text">实践 — 回归问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#问题描述"><span class="toc-text">问题描述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#运行环境"><span class="toc-text">运行环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#代码部分"><span class="toc-text">代码部分</span></a></li></ol></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#静态图和动态图"><span class="toc-text">静态图和动态图</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Reference"><span class="toc-text">Reference</span></a>
    </li></div>
  </div>



    <div class="post-content">
      
        <blockquote>
<p>深度学习框架有很多，Google的TensorFlow市场占有率居高不下，本文的小目标是说清楚tensorflow的<strong>“图（Graph）”</strong>和<strong>“会话（Session）”</strong>机制及其优缺点，最后以一个回归问题为例实践一下。文中顺便回答一下<strong>动态图（Dynamic computation graphs）</strong>和<strong>静态图（Static computational graphs）</strong>框架的区别。<br><a id="more"></a></p>
</blockquote>
<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p><strong>深度学习</strong>是目前人工智能领域备受推崇的算法种类，其在计算机视觉（Computer vision）、自然语言处理（NLP）领域有比较广泛的应用，这里先挖个坑，<a href="xxx">下一篇</a>将谈谈我对深度学习算法的理解。</p>
<p>目前开源市场的深度学习框架有很多，如<code>tensorflow</code>、<code>pytorch</code>、<code>mxnet</code>等，而tensorflow的市场占有率相对较高，那么为什么会有如此多的深度学习框架，tensorflow又有什么异于常人的地方呢？为了回答这个问题，本文首先尝试说明一下tensorflow的<code>Graph+Session</code>机制。</p>
<p>市面上的各种深度学习框架：</p>
<p><img src="DL-framework.png" alt="DeepLearning frameworks"></p>
<h2 id="计算图（Graph）"><a href="#计算图（Graph）" class="headerlink" title="计算图（Graph）"></a>计算图（Graph）</h2><p>简单来说，计算图是由tensor和opration组成的一张工程图纸。先上图（从图中来看，这是一个分类问题…），文末还附了一张PyTorch的动态图，有兴趣可以对比一下。</p>
<p><img src="tensors_flowing.gif" alt="Tensor and Flow"></p>
<h3 id="张量（Tensor）"><a href="#张量（Tensor）" class="headerlink" title="张量（Tensor）"></a>张量（Tensor）</h3><p>先把力学里面的张量忘掉，这里的张量概念包括标量、矢量和线性算子，或者简单理解成高维矩阵。输入的数据、参数大多是高维矩阵，统一被称为tensor，此外，tensor之间经过各种计算得到的结果依然是一个张量。</p>
<ul>
<li>输入<code>tf.placeholder</code></li>
<li>参数<code>tf.Variable</code></li>
<li>算子<code>tf.matmul</code>、<code>tf.sqrt()</code>等</li>
</ul>
<h3 id="算子（Operation）"><a href="#算子（Operation）" class="headerlink" title="算子（Operation）"></a>算子（Operation）</h3><p>Tensor之间的各种运算统称为operation，如加减乘除、开根号等。tensor进入operation进行各种计算，输出结果到下一个operation继续计算，像是tensor在流动，TensorFlow由此得名。</p>
<h2 id="会话-（Session）"><a href="#会话-（Session）" class="headerlink" title="会话 （Session）"></a>会话 （Session）</h2><p>当<code>tf.graph</code>定义好后，打开一个<code>tf.session</code>执行Graph，简单来说，会话是指机器根据工程图纸打开计算资源进行施工。Session提供了Operation执行和Tensor求值的环境，此外其还拥有物理资源（GPUs和网络连接）。当我们不再需要该session的时候，需要调用<code>sess.close()</code>关闭会话，将这些资源释放。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a default in-process session.</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a remote session.</span></span><br><span class="line"><span class="keyword">with</span> tf.Session(<span class="string">"grpc://example.org:2222"</span>):</span><br><span class="line">  <span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<h2 id="数据流-（Dataflow）"><a href="#数据流-（Dataflow）" class="headerlink" title="数据流 （Dataflow）"></a>数据流 （Dataflow）</h2><p>Dataflow是一个常见的并行计算编程模型。在一个dataflow图中（如上gif图所示），节点表示计算单元，边界则表示计算单元对数据的生产和消费。dataflow模式有几个比较大的优势：</p>
<ul>
<li><strong>Parallelism</strong>：知道了各个operation之间的依赖关系，系统就可以比较好的使用并行计算了，比如：矩阵相乘可以并行计算、A算子的输入与B算子的输出没有依赖关系也可以并行计算。</li>
<li><strong>Distributed execution</strong>：同样利用每个operation之间的依赖关系，tensorflow好让一些计算被调度到不同机器的多个设备上（CPUs, GPUs, TPUs），tensorflow还会提供必要的机器之间的通信。</li>
<li><strong>Compilation</strong>：tensorflow的 XLA 编译器利用dadaflow图编译更快的机器码。</li>
<li><strong>Portability</strong>：datdaflow图使模型表示是语言无关的。<code>tf.saved_model</code>保存的模型可以在其他语言中使用，非常便携。</li>
</ul>
<h2 id="实践-—-回归问题"><a href="#实践-—-回归问题" class="headerlink" title="实践 — 回归问题"></a>实践 — 回归问题</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><ol>
<li><p>构造一个函数/映射，<script type="math/tex">y_{data} = f(x_{data})</script>，其中 <script type="math/tex">x_{data}</script>是一个随机输入的<script type="math/tex">2\times100</script>矩阵，<script type="math/tex">y_{data}</script>是一个<script type="math/tex">1\times100</script>矩阵，由一个参数矩阵<script type="math/tex">W=\begin{bmatrix} 0.1 & 0.2 \end{bmatrix}</script> 和 <script type="math/tex">x_{data}</script>点乘后加上一个常数<script type="math/tex">b=0.3</script>构造出来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用Numpy生成假数据(phony data),总共100个点.</span></span><br><span class="line">x_data = np.float32(np.random.rand(<span class="number">2</span>, <span class="number">100</span>))      <span class="comment"># 随机输入</span></span><br><span class="line">print(x_data)</span><br><span class="line">y_data = np.dot([<span class="number">0.100</span>, <span class="number">0.200</span>], x_data) + <span class="number">0.300</span>  <span class="comment">#输出的y为[[]]的list</span></span><br><span class="line">print(y_data)</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用<script type="math/tex">x_{data}</script>和<script type="math/tex">y_{data}</script>数据来反演/学习出参数矩阵<script type="math/tex">W</script>和常量参数<script type="math/tex">b</script>，该问题等价于<strong>由100个方程求解三个参数问题</strong>，显然是一个<strong>超定问题</strong>，求解过程就是一个<strong>优化</strong>过程。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
     W_1X_{1,1} + W_2X_{2,1} +b = y_1 \\
     W_1X_{1,2} + W_2X_{2,2} +b = y_2 \\
      ... \\
     W_1X_{1,100} + W_2X_{2,100} +b =y_{100} \\
\end{pmatrix}</script></li>
<li><p>假设我们已经知道这个函数式了：$Wx+b=y$，仅仅不知道给定的参数$W$和$b$是什么，根据函数式，可以使用初始化参数来构造$y$，并计算$y$和$y_{data}$之间的<em>“距离”</em>，并使用梯度下降的方式找到一个最优参数组使<strong>距离</strong>尽量减少。见代码部分10-13行。</p>
</li>
</ol>
<blockquote>
<p><strong>注意</strong>：</p>
<ol>
<li>现实世界中往往是不知道两个随机变量之间的确切关系的</li>
<li>这里”距离”是指向量之间的空间距离，常用的距离有欧几里得距离（2-范数）曼哈顿距离（1-范数）等。本例中使用2-范数作为距离，也即最小方差/最小二乘/Least Square方法。</li>
</ol>
</blockquote>
<h3 id="运行环境"><a href="#运行环境" class="headerlink" title="运行环境"></a>运行环境</h3><p>由于tensorflow和cuda版本（9.0.176）兼容问题，选择安装V1.12.0GPU版本，本机tensorflow环境：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[conan@localhost ~]$ conda list | grep tensor</span><br><span class="line">tensorboard               1.12.0                    &lt;pip&gt;</span><br><span class="line">tensorflow-gpu            1.12.0                    &lt;pip&gt;</span><br><span class="line">tensorflow-tensorboard    0.4.0                     &lt;pip&gt;</span><br></pre></td></tr></table></figure>
<h3 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h3><p>这里使用一个简单的平面拟合问题来实践一下，完整代码请看<a href="https://github.com/kiddie92/Learning_Tech/blob/master/TensorFlow%20%E7%89%B9%E6%80%A7--Graph%20%E5%92%8C%20Sessions/Demo1_tensor.py" target="_blank" rel="noopener">这里</a></p>
<ul>
<li>拟合时主要关注的参数为：<code>tf.train.GradientDescentOptimizer(0.2)</code>里的学习率（或者叫做步长）、和迭代次数<code>for step in range(0, 51):</code>，减小学习率增加迭代次数理论上会使拟合效果更好，但是会有过拟合（over fitting）的危险，并且模型的泛化能力（generalization）会比较差，控制这种风险的算法也很多，比如给目标函数加正则化（regularization）。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造一个线性模型</span></span><br><span class="line"><span class="comment"># 实际问题中如果没有确切的物理关系,很难知道是否是线性模型, 也很难知道解在哪个范围</span></span><br><span class="line"></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">1</span>]))</span><br><span class="line">W = tf.Variable(tf.random_uniform([<span class="number">1</span>, <span class="number">2</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">y = tf.matmul(W, x_data) + b   <span class="comment"># y is synthetic data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面开始构建Graph</span></span><br><span class="line"><span class="comment"># 最小化方差(Least Square) 定义目标函数/损失函数/misfit function</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y - y_data))</span><br><span class="line"><span class="comment"># 优化器就使用最原始的梯度下降方法，参数为learning rate/步长</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.2</span>)   </span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化变量</span></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"><span class="comment"># 启动会话</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合平面/反演参数/回归分析</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">51</span>):</span><br><span class="line">    sess.run(train)   <span class="comment"># 参数train就是前面定义的dataflow graph</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        print(step, sess.run(W), sess.run(b))</span><br><span class="line">W = sess.run(W)</span><br><span class="line">b = sess.run(b)</span><br><span class="line">sess.close()  </span><br><span class="line"><span class="comment"># 最终反演出来的方程</span></span><br><span class="line">y_pred = np.dot(W, x_data) + b</span><br><span class="line">print(<span class="string">'----------------'</span>)</span><br><span class="line">print(y_pred[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 得到最佳拟合结果 W: [[0.100 0.200]]\, b: [0.300]</span></span><br></pre></td></tr></table></figure>
<p>得到的y和y_data的对比：<br><img src="Comparation.jpg" alt="Comparation"></p>
<h1 id="静态图和动态图"><a href="#静态图和动态图" class="headerlink" title="静态图和动态图"></a>静态图和动态图</h1><p>回到最开始的问题，TensorFlow异于常人的地方：其实就是“<strong>静态图</strong>”框架。引用“hackernoon”上看到的一句话：</p>
<blockquote>
<p>TensorFlow is a <strong>“Define-and-Run”</strong> framework where one would define conditions and iterations in the graph structure whereas in comparison Chainer, DyNet, PyTorch are all <strong>“Define-by-Run”</strong> frameworks.</p>
</blockquote>
<p><strong>动态计算图框架</strong>使用起来就像做工程时一边设计一边施工，TensorFlow使用起来就没有“<strong>动态图</strong>”框架那样灵活、直接，容易调试，而这也是其入门门槛高的一个原因。但是，<strong>“静态图”</strong>的优点也是明显的—计算会更加高效，因为所有的步骤都定义好了再进行计算使计算机资源的调配更加合理、高效。所以说，“动态图”和“静态图”是优势互补的。</p>
<blockquote>
<p>TensorFlow 2.0 推出了Eager Execution，开始支持“动态图”了</p>
</blockquote>
<p><img src="dynamic_graph_pytorch.gif" alt="Dynamic Graph of PyTorch"></p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol>
<li><a href="https://www.tensorflow.org/guide/graphs?hl=en" target="_blank" rel="noopener">Graphs and Sessions</a></li>
<li><a href="https://hackernoon.com/how-is-pytorch-different-from-tensorflow-2c90f44747d6" target="_blank" rel="noopener">How is PyTorch different from Tensorflow?</a></li>
</ol>

      
    </div>

    
      
      

  <div class="post-copyright">
    <p class="copyright-item">
      <span>原文作者: </span>
      <a href="https://kiddie92.github.io">Mun*</a>
    </p>
    <p class="copyright-item">
      <span>原文链接: </span>
      <a href="https://kiddie92.github.io/2019/03/02/TensorFlow-特性-Graph-和-Sessions/">https://kiddie92.github.io/2019/03/02/TensorFlow-特性-Graph-和-Sessions/</a>
    </p>
    <p class="copyright-item">
      <span>许可协议: </span>
      
      <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用4.0国际许可协议</a>
    </p>
  </div>



      
      
  <div class="post-reward">
    <input type="checkbox" name="reward" id="reward" hidden>
    <label class="reward-button" for="reward">赞赏支持</label>
    <div class="qr-code">
      
      
        <label class="qr-code-image" for="reward">
          <img class="image" src="/images/reward/wechat.png" title="微信打赏">
        </label>
      
      
        <label class="qr-code-image" for="reward">
          <img class="image" src="/images/reward/alipay.png" title="支付宝打赏">
        </label>
      
    </div>
  </div>

    

    
      <footer class="post-footer">
        
          <div class="post-tags">
            
              <a href="/tags/tensorflow/">tensorflow</a>
            
              <a href="/tags/机器学习/">机器学习</a>
            
          </div>
        
        
        
  <nav class="post-nav">
    
    
      <a class="next" href="/2019/01/23/kubernetes-性能测试方法简介/">
        <span class="next-text nav-default">kubernetes 性能测试方法简介</span>
        <span class="prev-text nav-mobile">下一篇</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>


      </footer>
    

  </article>


          </div>
          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
      </main>

      <footer id="footer" class="footer">

  <div class="social-links">
    
      
        
          <a href="mailto:kiddiezzh@gmail.com" class="iconfont icon-email" title="email"></a>
        
      
    
      
    
      
        
          <a href="https://twitter.com/kiddiezzh" class="iconfont icon-twitter" title="twitter"></a>
        
      
    
      
    
      
    
      
    
      
        
          <a href="https://github.com/kiddie92" class="iconfont icon-github" title="github"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    

    
      <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    
  </div>



<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://hexo.io/">Hexo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">
    
    &copy; 
     
      2018 - 
    
    2019

    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Mun*</span>
  </span>
</div>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>

    
  
  <script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'https://kiddie92.github.io/2019/03/02/TensorFlow-特性-Graph-和-Sessions/';
        this.page.identifier = '2019/03/02/TensorFlow-特性-Graph-和-Sessions/';
        this.page.title = 'TensorFlow 特性--Graph 和 Sessions';
    };
    (function() {
    var d = document, s = d.createElement('script');

    s.src = '//https-kiddie92-github-io.disqus.com/embed.js';

    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();  
  </script>

  

  



    
  



  
  





  
    <script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  

  
    <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  

  
    <script type="text/javascript" src="/lib/pjax/jquery.pjax.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/nprogress/nprogress.min.js"></script>
  


    <script type="text/javascript" src="/js/src/even.js?v=2.10.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  </body>
</html>
