<!DOCTYPE html>
<html lang="zh-cn">
  <head><meta name="generator" content="Hexo 3.8.0">
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">

<meta name="theme-color" content="#f8f5ec">
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">



  <meta name="description" content="反向传播算法（BP）">




  <meta name="keywords" content="算法, 机器学习, 人工智能, Mun*">










  <link rel="alternate" href="/atom.xml" title="Mun*">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.10.2">



<link rel="canonical" href="https://kiddie92.github.io/2019/06/22/反向传播/">



  <link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css">




  <link rel="stylesheet" type="text/css" href="/lib/nprogress/nprogress.min.css">



<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.10.2">



  


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-91728997-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-91728997-1');
</script>


  <script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>





  <script src="//cdn1.lncld.net/static/js/3.1.1/av-min.js"></script>
  <script id="leancloud">
    AV.init({
      appId: "n8MGlOWz83zS9G0m0PGinErj-gzGzoHsz",
      appKey: "bSOcrmi2W53fTzNXQA3UQ2z3"
    });
  </script>





<script>
  window.config = {"leancloud":{"app_id":"n8MGlOWz83zS9G0m0PGinErj-gzGzoHsz","app_key":"bSOcrmi2W53fTzNXQA3UQ2z3"},"toc":true,"fancybox":true,"pjax":true};
</script>

    <title> 反向传播算法（BP） - Mun* </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="mobile-header-logo">
    <a href="/." class="logo">Mun*</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    
      <a href="/">
        <li class="mobile-menu-item">
          
          
            首页
          
        </li>
      </a>
    
      <a href="/archives/">
        <li class="mobile-menu-item">
          
          
            归档
          
        </li>
      </a>
    
      <a href="/tags">
        <li class="mobile-menu-item">
          
          
            标签
          
        </li>
      </a>
    
      <a href="/categories">
        <li class="mobile-menu-item">
          
          
            分类
          
        </li>
      </a>
    
      <a href="/Open-source">
        <li class="mobile-menu-item">
          
          
            开源项目
          
        </li>
      </a>
    
      <a href="/links">
        <li class="mobile-menu-item">
          
          
            链接
          
        </li>
      </a>
    
      <a href="/about">
        <li class="mobile-menu-item">
          
          
            关于
          
        </li>
      </a>
    
  </ul>
</nav>

    <div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Mun*</a>
</div>

<nav class="site-navbar">
  
    <ul id="menu" class="menu">
      
        <li class="menu-item">
          <a class="menu-item-link" href="/">
            
            
              首页
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            
            
              归档
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/tags">
            
            
              标签
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/categories">
            
            
              分类
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/Open-source">
            
            
              开源项目
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/links">
            
            
              链接
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/about">
            
            
              关于
            
          </a>
        </li>
      
    </ul>
  
</nav>

      </header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            
  
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          反向传播算法（BP）
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-06-22
        </span>
        
          <span class="post-category">
            
              <a href="/categories/人工智能/">人工智能</a>
            
          </span>
        
        
        <span class="post-visits" data-url="/2019/06/22/反向传播/" data-title="反向传播算法（BP）">
          阅读次数 0
        </span>
        
      </div>
    </header>

    
    
  <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">文章目录</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Back-Propagation-tag"><span class="toc-text">Back Propagation $tag$</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码实现"><span class="toc-text">代码实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RNN怎么做BP"><span class="toc-text">RNN怎么做BP</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-text">Reference</span></a></li></ol>
    </div>
  </div>



    <div class="post-content">
      
        <blockquote>
<p>人工智能领域的算法真是日新月异啊，最近CMU和Google Brain又提出了XLNet。<br>这篇博文还是从基础算法入手，介绍一下Back Propagation（简称BP），主要分为两个部分：反向传播的基本原理和RNN的反向传播算法，最后给出代码实现。<br><a id="more"></a></p>
</blockquote>
<h2 id="Back-Propagation-tag"><a href="#Back-Propagation-tag" class="headerlink" title="Back Propagation $tag$"></a>Back Propagation $tag$</h2><p>之前介绍过<a href="https://kiddie92.github.io/2019/03/17/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-%E7%89%9B%E9%A1%BF%E8%BF%AD%E4%BB%A3%E6%B3%95/">牛顿迭代法</a>：当我们想要优化一个目标函数时，需要在每次迭代的时候对变量/参数进行更新，而更新参数的最重要的部分就是<strong>求目标函数对参数的导数</strong>了。<br>下面对最简单的神经网络进行BP算法的公式推导：<br>下图是一个只有两个神经元的网络（图片修改自<a href="./BP.pptx">李弘毅老师的PPT</a>），输入样本为一个二维的向量<script type="math/tex">\vec x</script>，输出为一个数<script type="math/tex">y</script>，现在假设loss函数是<script type="math/tex">l=|| y - \hat y ||_2^2</script>，那么要求的便是<script type="math/tex">\frac {\partial l}{\partial w_i}</script> 以及<script type="math/tex">\frac {\partial l}{\partial b}</script> （把<script type="math/tex">w</script>和<script type="math/tex">b</script>合并成一个向量<script type="math/tex">W</script>也是可以的）。</p>
<p><img src="./simple_NN.png" width="75%" height="75%"></p>
<p>图中，<script type="math/tex">\sigma</script>表示sigmiod函数。</p>
<blockquote>
<p>首先思考一下，这个导数如果让你计算应该怎么做呢？</p>
</blockquote>
<p>因为loss函数里与<script type="math/tex">w</script>, <script type="math/tex">b</script>参数相关的就只有<script type="math/tex">y</script>了，而<script type="math/tex">y</script>由<script type="math/tex">z'</script> 得出，<script type="math/tex">z'</script> 由线性函数<script type="math/tex">wx+b</script>得到…那自然会想到用chain rule来求导了。</p>
<p>整理一下思路：</p>
<script type="math/tex; mode=display">y=\sigma(z')  \\ z'=w_{21}a+b_2 \\ a=\sigma(z) \\ z=w_{11}x_1+w_{12}x_2+b_1 \tag {1}</script><p>那么求导如下：</p>
<script type="math/tex; mode=display">\frac {\partial l}{\partial y} =  2(y-\hat y) \\ \frac {\partial y}{\partial w_{11}} = \frac {\partial y}{\partial z'} \cdot \frac {\partial z'}{\partial a}  \cdot \frac {\partial a}{\partial z} \cdot \frac {\partial z}{\partial w_{11}} = \sigma(z')[1-\sigma(z')]\cdot w_{21} \cdot \sigma(z)[1-\sigma(z)] \cdot x_1 \\ \frac {\partial y}{\partial b_1} = \frac {\partial y}{\partial z'} \cdot \frac {\partial z'}{\partial a}  \cdot \frac {\partial a}{\partial z} \cdot \frac {\partial z}{\partial b_1} = \sigma(z')[1-\sigma(z')]\cdot w_{21} \cdot \sigma(z)[1-\sigma(z)] \cdot 1 \tag{2}</script><p>这么求解貌似是可以的，however, 没有任何技巧可言，当网络结构有变化的时候，算法变的异常难写，比如：一层多个神经元的情况，两层之间非全连接的情况，某些神经元共享参数的情况等，想想都可怕，想要写个general的算法几乎不可能，另外还有计算量需要被考虑。</p>
<blockquote>
<p>还好聪明人总是有的，考虑到<script type="math/tex">l</script>对<script type="math/tex">w</script>和对<script type="math/tex">b</script>求导过程其实是一样的，下面就仅介绍<script type="math/tex">y</script>对<script type="math/tex">w</script>的BP求导过程。</p>
</blockquote>
<p>对于上面的问题，先只看第一层神经元的参数更新（其实对于任意层任意神经元都有下式的关系）：</p>
<script type="math/tex; mode=display">\frac {\partial l}{\partial w} =  \frac {\partial l}{\partial z} \frac {\partial z}{\partial w}</script><p>接下来分别计算<script type="math/tex">\frac {\partial l}{\partial z}</script>和<script type="math/tex">\frac {\partial z}{\partial w}</script>:<br><code>正向计算</code>：</p>
<script type="math/tex; mode=display">\frac {\partial z}{\partial w_{11}} =  x_1 \\  \frac {\partial z}{\partial w_{12}} = x_2 \tag {3}</script><p>从式（3）可以看出正向计算有个非常好的性质，<script type="math/tex">x_1, x_1</script>就是神经元的值，这都已经在正向传播时计算过了！</p>
<p><code>反向计算</code>：</p>
<script type="math/tex; mode=display">\frac {\partial l}{\partial z} = \frac {\partial l}{\partial a}  \frac {\partial a}{\partial z} \tag {4}</script><p>这其中，<script type="math/tex">\frac {\partial a}{\partial z}</script>很简单，sigmiod求导就ok了，而且<script type="math/tex">\sigma(x)'=\sigma(x)[1-\sigma(x)]</script>，nice，又可以用现成的结果了。</p>
<p>而对于<script type="math/tex">\frac {\partial l}{\partial a}</script>的计算比较棘手了，因为：</p>
<script type="math/tex; mode=display">\frac {\partial l}{\partial a} =\frac {\partial l}{\partial z'} \frac {\partial z'}{\partial a} =  \frac {\partial l}{\partial z'}\cdot w_{21} \tag {5}</script><p>其中<script type="math/tex">w_{21}</script>就是神经元间的连接权重参数，对于比较复杂的网络，计算<script type="math/tex">\frac {\partial l}{\partial a}</script> 可以进行递归计算。如果被计算的<script type="math/tex">\frac {\partial l}{\partial z'}</script>是神经网络的最后一层（output layer），问题就变的简单了：</p>
<script type="math/tex; mode=display">\frac {\partial l}{\partial z'}=  \frac {\partial l}{\partial y} \frac {\partial y}{\partial z'} \tag{6}</script><p>结束了，，</p>
<script type="math/tex; mode=display">\frac {\partial l}{\partial z} = \sigma(z)'[\frac {\partial l}{\partial z'}\cdot w_{21}]</script><p>再看一下<code>反向计算</code><script type="math/tex">\frac {\partial l}{\partial z}</script>：因为<script type="math/tex">\frac {\partial a}{\partial z}</script>比较好求，是一个常数，sigmoid函数在该神经元上的求导就得到了；<script type="math/tex">\frac {\partial l}{\partial a}</script>呢则可以使用递归方法，只要求出output layer的<script type="math/tex">\frac {\partial l}{\partial z'}</script>就ok。从下图来看，就是input为<script type="math/tex">\frac {\partial l}{\partial z'}</script>，然后按照同样的网络反方向计算一次，和正向传播的不同之处仅仅在于sigmoid（非线性转换/激活函数）变成先求导再相乘，结构清晰，计算量也大幅度下降。</p>
<p><img src="./Back_pass.png" width="50%" height="50%"></p>
<p>最后总结一下：<code>正向传播</code>和<code>反向传播</code> 相乘就可以计算出所有的参数更新梯度。<br><img src="./BP.png" width="60%" height="60%"></p>
<blockquote>
<p>插句话：<br>神经网络中常用的优化算法是<code>随机梯度下降法</code>（SGD），实践中常用的是<code>adam</code>优化器（一种自适应步长/学习率的优化算法），理论上梯度下降做自适应步长也很简单，不过需要求Hessian矩阵，而对于上千万甚至上亿个参数的目标函数来说，计算量实在是太大了。</p>
</blockquote>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>这里对基础版的反向传播算法进行代码实现，代码来自<a href="https://medium.com/@a.mirzaei69/implement-a-neural-network-from-scratch-with-python-numpy-backpropagation-e82b70caa9bb" target="_blank" rel="noopener">这里</a>，你也可以点击<a href="https://github.com/kiddie92/Learning_Tech/tree/master/BP" target="_blank" rel="noopener">这里</a>进行查看。核心代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self, x)</span>:</span>  <span class="comment"># 正向计算</span></span><br><span class="line">    <span class="comment"># return the feedforward value for x</span></span><br><span class="line">    a = np.copy(x)</span><br><span class="line">    z_s = []</span><br><span class="line">    a_s = [a]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.weights)):</span><br><span class="line">        activation_function = self.getActivationFunction(self.activations[i])</span><br><span class="line">        z_s.append(self.weights[i].dot(a) + self.biases[i])</span><br><span class="line">        a = activation_function(z_s[<span class="number">-1</span>])</span><br><span class="line">        a_s.append(a)</span><br><span class="line">    <span class="keyword">return</span> (z_s, a_s)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backpropagation</span><span class="params">(self,y, z_s, a_s)</span>:</span>  <span class="comment"># 反向计算</span></span><br><span class="line">    dw = []  <span class="comment"># dl/dW</span></span><br><span class="line">    db = []  <span class="comment"># dl/db</span></span><br><span class="line">    deltas = [<span class="literal">None</span>] * len(self.weights)  <span class="comment"># delta = dl/dz  known as error for each layer</span></span><br><span class="line">    <span class="comment"># insert the last layer error</span></span><br><span class="line">    deltas[<span class="number">-1</span>] = ((y-a_s[<span class="number">-1</span>])*(self.getDerivitiveActivationFunction(self.activations[<span class="number">-1</span>]))(z_s[<span class="number">-1</span>]))</span><br><span class="line">    <span class="comment"># Perform BackPropagation</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(len(deltas)<span class="number">-1</span>)):</span><br><span class="line">        deltas[i] = self.weights[i+<span class="number">1</span>].T.dot(deltas[i+<span class="number">1</span>])*(self.getDerivitiveActivationFunction(self.activations[i])(z_s[i]))</span><br><span class="line">    <span class="comment">#a= [print(d.shape) for d in deltas]</span></span><br><span class="line">    batch_size = y.shape[<span class="number">1</span>]</span><br><span class="line">    db = [d.dot(np.ones((batch_size,<span class="number">1</span>)))/float(batch_size) <span class="keyword">for</span> d <span class="keyword">in</span> deltas]</span><br><span class="line">    dw = [d.dot(a_s[i].T)/float(batch_size) <span class="keyword">for</span> i,d <span class="keyword">in</span> enumerate(deltas)]</span><br><span class="line">    <span class="comment"># return the derivitives respect to weight matrix and biases</span></span><br><span class="line">    <span class="keyword">return</span> dw, db</span><br></pre></td></tr></table></figure>
<h2 id="RNN怎么做BP"><a href="#RNN怎么做BP" class="headerlink" title="RNN怎么做BP"></a>RNN怎么做BP</h2><p>RNN的结构和<code>前馈神经网络</code>有所不同，反向传播和上述也略有不同，这里要介绍的算法叫做<strong>Backpropagation Through Time (BPTT)</strong>。这里假设读者对RNN、LSTM、GRU等算法已经有所了解了，就提纲挈领的介绍一下<code>BPTT</code>特别之处。</p>
<ul>
<li>主要的不同点还是要看公式，RNN的在<script type="math/tex">t</script>时刻输出神经元的值可以用下式表示：<script type="math/tex; mode=display">a_{k,t} = \sigma (W_{k}^{h}a_{k,t-1} + W_{k}^{x}a_{k-1,t} +b_{k})</script>式中<script type="math/tex">W_{k}^{h}</script>与上一时刻的计算结果相关</li>
<li>所以<script type="math/tex">\frac {\partial l}{\partial W_{k}^{h}}</script>的计算算是新内容，其他的和之前说的没有什么不一样</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>反向传播算法1974年被提出，不过当时没有受到重视，2010年开始，得益于该算法和GPU算力的提升，人工智能开始迅速发展。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Werbos, P. (1974). Beyond Regression:” New Tools for Prediction and Analysis in the Behavioral Sciences. Ph. D. dissertation, Harvard University.</li>
<li><a href="https://medium.com/@a.mirzaei69/implement-a-neural-network-from-scratch-with-python-numpy-backpropagation-e82b70caa9bb" target="_blank" rel="noopener">https://medium.com/@a.mirzaei69/implement-a-neural-network-from-scratch-with-python-numpy-backpropagation-e82b70caa9bb</a></li>
<li><a href="https://www.youtube.com/watch?v=UTojaCzMoOs" target="_blank" rel="noopener">https://www.youtube.com/watch?v=UTojaCzMoOs</a></li>
</ol>

      
    </div>

    
      
      

  <div class="post-copyright">
    <p class="copyright-item">
      <span>原文作者: </span>
      <a href="https://kiddie92.github.io">Mun*</a>
    </p>
    <p class="copyright-item">
      <span>原文链接: </span>
      <a href="https://kiddie92.github.io/2019/06/22/反向传播/">https://kiddie92.github.io/2019/06/22/反向传播/</a>
    </p>
    <p class="copyright-item">
      <span>许可协议: </span>
      
      <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用4.0国际许可协议</a>
    </p>
  </div>



      
      
  <div class="post-reward">
    <input type="checkbox" name="reward" id="reward" hidden>
    <label class="reward-button" for="reward">赞赏支持</label>
    <div class="qr-code">
      
      
        <label class="qr-code-image" for="reward">
          <img class="image" src="/images/reward/wechat.png" title="微信打赏">
        </label>
      
      
        <label class="qr-code-image" for="reward">
          <img class="image" src="/images/reward/alipay.png" title="支付宝打赏">
        </label>
      
    </div>
  </div>

    

    
      <footer class="post-footer">
        
          <div class="post-tags">
            
              <a href="/tags/算法/">算法</a>
            
              <a href="/tags/机器学习/">机器学习</a>
            
              <a href="/tags/人工智能/">人工智能</a>
            
          </div>
        
        
        
  <nav class="post-nav">
    
    
      <a class="next" href="/2019/06/15/分层softmax/">
        <span class="next-text nav-default">分层softmax</span>
        <span class="prev-text nav-mobile">下一篇</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>


      </footer>
    

  </article>


          </div>
          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
      </main>

      <footer id="footer" class="footer">

  <div class="social-links">
    
      
        
          <a href="mailto:kiddiezzh@gmail.com" class="iconfont icon-email" title="email"></a>
        
      
    
      
    
      
        
          <a href="https://twitter.com/kiddiezzh" class="iconfont icon-twitter" title="twitter"></a>
        
      
    
      
    
      
    
      
    
      
        
          <a href="https://github.com/kiddie92" class="iconfont icon-github" title="github"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    

    
      <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    
  </div>



<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://hexo.io/">Hexo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">
    
    &copy; 
     
      2018 - 
    
    2019

    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Mun*</span>
  </span>
</div>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>

    
  
  <script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'https://kiddie92.github.io/2019/06/22/反向传播/';
        this.page.identifier = '2019/06/22/反向传播/';
        this.page.title = '反向传播算法（BP）';
    };
    (function() {
    var d = document, s = d.createElement('script');

    s.src = '//https-kiddie92-github-io.disqus.com/embed.js';

    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();  
  </script>

  

  



    
  



  
  





  
    <script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  

  
    <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  

  
    <script type="text/javascript" src="/lib/pjax/jquery.pjax.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/nprogress/nprogress.min.js"></script>
  


    <script type="text/javascript" src="/js/src/even.js?v=2.10.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  </body>
</html>
