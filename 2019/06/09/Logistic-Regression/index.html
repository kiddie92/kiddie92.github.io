<!DOCTYPE html>
<html lang="zh-cn">
  <head><meta name="generator" content="Hexo 3.8.0">
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">

<meta name="theme-color" content="#f8f5ec">
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">



  <meta name="description" content="Logistic Regression">




  <meta name="keywords" content="逻辑回归算法, Mun*">










  <link rel="alternate" href="https://feedity.com/github-io/UlJXUVFQUg.rss" title="Mun*">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.10.2">



<link rel="canonical" href="https://kiddie92.github.io/2019/06/09/Logistic-Regression/">



  <link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css">




  <link rel="stylesheet" type="text/css" href="/lib/nprogress/nprogress.min.css">



<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.10.2">



  


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-91728997-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-91728997-1');
</script>


  <script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>





  <script src="//cdn1.lncld.net/static/js/3.1.1/av-min.js"></script>
  <script id="leancloud">
    AV.init({
      appId: "n8MGlOWz83zS9G0m0PGinErj-gzGzoHsz",
      appKey: "bSOcrmi2W53fTzNXQA3UQ2z3"
    });
  </script>





<script>
  window.config = {"leancloud":{"app_id":"n8MGlOWz83zS9G0m0PGinErj-gzGzoHsz","app_key":"bSOcrmi2W53fTzNXQA3UQ2z3"},"toc":true,"fancybox":true,"pjax":true};
</script>

    <title> Logistic Regression - Mun* </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="mobile-header-logo">
    <a href="/." class="logo">Mun*</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    
      <a href="/">
        <li class="mobile-menu-item">
          
          
            首页
          
        </li>
      </a>
    
      <a href="/archives/">
        <li class="mobile-menu-item">
          
          
            归档
          
        </li>
      </a>
    
      <a href="/tags">
        <li class="mobile-menu-item">
          
          
            标签
          
        </li>
      </a>
    
      <a href="/categories">
        <li class="mobile-menu-item">
          
          
            分类
          
        </li>
      </a>
    
      <a href="/Open-source">
        <li class="mobile-menu-item">
          
          
            开源项目
          
        </li>
      </a>
    
      <a href="/links">
        <li class="mobile-menu-item">
          
          
            链接
          
        </li>
      </a>
    
      <a href="/about">
        <li class="mobile-menu-item">
          
          
            关于
          
        </li>
      </a>
    
  </ul>
</nav>

    <div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Mun*</a>
</div>

<nav class="site-navbar">
  
    <ul id="menu" class="menu">
      
        <li class="menu-item">
          <a class="menu-item-link" href="/">
            
            
              首页
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            
            
              归档
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/tags">
            
            
              标签
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/categories">
            
            
              分类
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/Open-source">
            
            
              开源项目
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/links">
            
            
              链接
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/about">
            
            
              关于
            
          </a>
        </li>
      
    </ul>
  
</nav>

      </header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            
  
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          Logistic Regression
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-06-09
        </span>
        
          <span class="post-category">
            
              <a href="/categories/人工智能/">人工智能</a>
            
          </span>
        
        
        <span class="post-visits" data-url="/2019/06/09/Logistic-Regression/" data-title="Logistic Regression">
          阅读次数 0
        </span>
        
      </div>
    </header>

    
    
  <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">文章目录</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#基础知识回顾"><span class="toc-text">基础知识回顾</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Bernoulli-distribution"><span class="toc-text">Bernoulli distribution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sigmoid-函数及其导函数"><span class="toc-text">sigmoid 函数及其导函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#softmax函数"><span class="toc-text">softmax函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#公式推导"><span class="toc-text">公式推导</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#概率模型"><span class="toc-text">概率模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最大似然估计（Maximum-Likelihood）"><span class="toc-text">最大似然估计（Maximum Likelihood）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#交叉熵（Cross-Entropy-and-KL-divergence"><span class="toc-text">交叉熵（Cross Entropy and KL-divergence)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#优化"><span class="toc-text">优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#从二分类到多分类"><span class="toc-text">从二分类到多分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#方法局限性（limitation）"><span class="toc-text">方法局限性（limitation）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#非线性可分的解法"><span class="toc-text">非线性可分的解法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#问答-amp-理解"><span class="toc-text">问答&amp;理解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#为什么使用Cross-Entropy计算loss？"><span class="toc-text">为什么使用Cross Entropy计算loss？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#用Square-Error作为Loss行不行？"><span class="toc-text">用Square Error作为Loss行不行？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#为什么叫Regression？"><span class="toc-text">为什么叫Regression？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#和信息论（information-theory）的关系？"><span class="toc-text">和信息论（information theory）的关系？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#和Generation-Model-生成模型的关系"><span class="toc-text">和Generation Model/生成模型的关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sigmoid-V-S-softmax"><span class="toc-text">sigmoid V.S. softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Logistic-Regression-V-S-Neural-Network"><span class="toc-text">Logistic Regression V.S. Neural Network</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-text">Reference</span></a></li></ol>
    </div>
  </div>



    <div class="post-content">
      
        <blockquote>
<p>对数几率回归/逻辑回归/逻辑斯蒂回归/最大熵模型也即Logistic Regression是深度学习的基础，算法的重要性不言而喻。Logistic Regression虽然叫“Rgression”，但其实与之前介绍的<a href="https://kiddie92.github.io/2019/05/10/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89/">SVM分类（svc）方法</a>一样，同属<strong>分类算法</strong>。本篇博文对该算法的介绍流程基本参考了李宏毅老师的机器学习课程，文章后半部分主要以问答的形式给出了关于逻辑斯蒂回归的部分理解。<br><a id="more"></a></p>
</blockquote>
<h2 id="基础知识回顾"><a href="#基础知识回顾" class="headerlink" title="基础知识回顾"></a>基础知识回顾</h2><p>$  $</p>
<h3 id="Bernoulli-distribution"><a href="#Bernoulli-distribution" class="headerlink" title="Bernoulli distribution"></a>Bernoulli distribution</h3><p>伯努利分布/两点分布/0-1分布是一个非常简单的概率模型，事件只有两种，常见于二分类问题，其概率密度函数（PDF）如下：</p>
<script type="math/tex; mode=display">f_X(x)=p^x(1-p)^{1-x}=\begin{cases} p, & \text {if x=1,} \\ q, & \text{if x=0.} \end{cases}  \tag{1}</script><h3 id="sigmoid-函数及其导函数"><a href="#sigmoid-函数及其导函数" class="headerlink" title="sigmoid 函数及其导函数"></a>sigmoid 函数及其导函数</h3><p>Sigmoid 函数意思就是<strong>S型函数</strong>，因为长相就是S形的，公式如图左上角所示，下图蓝色表示sigmoid函数，绿色表示其导函数。sigmiod在学习机器学习和深度学习时会时常遇到。</p>
<p><img src="./sigmoid.png" width="60%" height="60%"></p>
<h3 id="softmax函数"><a href="#softmax函数" class="headerlink" title="softmax函数"></a>softmax函数</h3><p>softmax可以将一组数字转换成一组概率，并且突出较大数值的优势（使其在概率上有很大的值），如下图所示：</p>
<p><img src="./softmax.png" width="60%" height="60%"></p>
<h2 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h2><h3 id="概率模型"><a href="#概率模型" class="headerlink" title="概率模型"></a>概率模型</h3><p>对于<strong>二分类</strong>问题，假设现在有一笔training data（下表中<script type="math/tex">C_1</script>表示数据<script type="math/tex">x^i</script>属于Class 1，<script type="math/tex">C_2</script>表示数据<script type="math/tex">x^i</script>属于Class 2）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">数据</th>
<th style="text-align:center"><script type="math/tex">x^1</script></th>
<th style="text-align:center"><script type="math/tex">x^2</script></th>
<th style="text-align:center"><script type="math/tex">x^3</script></th>
<th style="text-align:center">……</th>
<th style="text-align:center"><script type="math/tex">x^N</script></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">类别</td>
<td style="text-align:center"><script type="math/tex">C_1</script></td>
<td style="text-align:center"><script type="math/tex">C_1</script></td>
<td style="text-align:center"><script type="math/tex">C_2</script></td>
<td style="text-align:center">……</td>
<td style="text-align:center"><script type="math/tex">C_1</script></td>
</tr>
</tbody>
</table>
</div>
<p>现已知一个<strong>新数据</strong><script type="math/tex">x</script>，我们可以构造一个<strong>概率密度函数（PDF）</strong>来表示<script type="math/tex">x</script>属于<script type="math/tex">C_1</script>的概率<script type="math/tex">P(\left. C_1 \right |x)</script>：</p>
<script type="math/tex; mode=display">f_{w,b}(x) = \sigma (z) = P_{w,z}(\left. C_1 \right |x)\\ z=\sum_iw_ix_i+b \tag{2}</script><p>这里引入的<script type="math/tex">w_i, b</script>参数和sigmoid函数<script type="math/tex">\sigma(z)</script>并不是凭空捏造出来的，实际上，是可以根据<strong>生成模型</strong>推导出这个公式的，这部分的内容可以参考<a href="https://www.youtube.com/watch?v=fZAZUYEeIMg&amp;list=LLoCiddVQCg9ZvZoEa5d5wYw&amp;index=1" target="_blank" rel="noopener">这里</a>。<br>如此一来，<strong>假设“<script type="math/tex">x_i</script>属于<script type="math/tex">C_1或C_2</script>”这件事情和“<script type="math/tex">x_j</script>属于<script type="math/tex">C_1或C_2</script>”这件事情是相互独立的</strong>，那么就可以得出，training data中的所有事件<strong>全部发生</strong>的概率为：</p>
<script type="math/tex; mode=display">L(w,b) = f_{w,b}(x^1) \cdot f_{w,b}(x^2) \cdot (1-f_{w,b}(x^3)) …… f_{w,b}(x^N)  \tag{3}</script><blockquote>
<ol>
<li>对于二分类问题，由于<script type="math/tex">x_3</script>属于<script type="math/tex">C_2</script>，所以<script type="math/tex">P_{w,z}(\left. C_2 \right |x^3)=1-P_{w,z}(\left. C_1 \right |x^3)</script>也即<script type="math/tex">1-f_{w,b}(x^3)</script>；</li>
<li>实际数据往往是离散的，这里使用概率密度函数（PDF）也许不准确。</li>
</ol>
</blockquote>
<h3 id="最大似然估计（Maximum-Likelihood）"><a href="#最大似然估计（Maximum-Likelihood）" class="headerlink" title="最大似然估计（Maximum Likelihood）"></a>最大似然估计（Maximum Likelihood）</h3><p>为了让PDF可以表示真实数据的distribution，也即让<script type="math/tex">f_{w,b}(x)</script>最为准确，我们希望计算得到的<script type="math/tex">L(w,b)</script>尽可能大，当然，最大值就是1了。接下来目标便是找到一组<script type="math/tex">w^*, b^*</script>使得：</p>
<script type="math/tex; mode=display">\underset{\vec w,b}{\operatorname{arg max}} L(w,b)  \tag{4}</script><p>为了方便（Mathmatic Convenient），将上式改写：</p>
<script type="math/tex; mode=display">\underset{\vec w,b}{\operatorname{arg min}} { -ln[L(w,b)] }  \tag{5}</script><p>并且引入参数<script type="math/tex">\hat y</script>（也是关于x的一个两点分布）：</p>
<script type="math/tex; mode=display">\hat y = \begin{cases} 1, \space \space if \space x\in C_1  \\  0, \space \space if \space x\in C_2  \end{cases}  \tag{6}</script><p>结合式(5)和式(6)，有：</p>
<script type="math/tex; mode=display">-\ln{f_{w,b}(x^n)}= \hat y^n \ln{f_{w,b}(x^n)} + (1-\hat y^n) \ln{(1-f_{w,b}(x^n))}  \tag{7}</script><p>代入式(5)，得</p>
<script type="math/tex; mode=display">-ln[L(w,b)] = - \sum_n [ \hat y^n \ln{f_{w,b}(x^n)} + (1-\hat y^n) \ln{(1-f_{w,b}(x^n)}) ]  \tag{8}</script><p>式(8)的右边便是<strong>两个二项分布的Cross Entropy</strong>，也就是logistic regression的loss function/object function接下来要做的便是对式(8)进行优化/求最小值。</p>
<h3 id="交叉熵（Cross-Entropy-and-KL-divergence"><a href="#交叉熵（Cross-Entropy-and-KL-divergence" class="headerlink" title="交叉熵（Cross Entropy and KL-divergence)"></a>交叉熵（Cross Entropy and KL-divergence)</h3><p><code>熵</code>：在信息论中（information theroy）离散概率分布函数的熵为</p>
<script type="math/tex; mode=display">H(p) = - \sum_i{p_i \log{p_i}} \tag{9}</script><p><code>KL-divergence</code>: 为了衡量两个离散分布函数的相似度，如果使用传统的二范数距离或者向量的夹角距离，显然不合适，这里定义</p>
<script type="math/tex; mode=display">D_{KL}(\left. p \right |q) = \sum_i{p_i \log{\frac{p_i}{q_i}}}  \tag{10}</script><p>来衡量两个离散分布函数有多接近/相似，<strong>注意</strong><script type="math/tex">D_{KL}(\left. p \right |q) \ne D_{KL}(\left. q \right |p)</script>，当<script type="math/tex">D_{KL}(\left. p \right |q) =0</script>时，<script type="math/tex">p</script>同<script type="math/tex">q</script>分布。<br><code>Cross Entropy</code>:</p>
<script type="math/tex; mode=display">H(p,q) = H(p)+D_{KL}(\left. p \right |q)=\sum_i{p_i \log{\frac {1}{q_i}}}= -\sum_i{p_i \log{q_i}} \tag{11}</script><p>所以对应到式(8)中，对于<script type="math/tex">\forall x^n</script>，有：</p>
<script type="math/tex; mode=display">p=\begin{cases}  \hat y^n,  \space \space \space \space \space \space \space for \space x \in C_1 \\  1-\hat y^n, \space  for \space x \in C_2 \end{cases}, \space \space \space  q=\begin{cases}  f_{w,b}(x^n),  \space \space \space \space \space \space \space for \space x \in C_1 \\  1-f_{w,b}(x^n), \space  for \space x \in C_2 \end{cases} \tag{12}</script><script type="math/tex; mode=display">-ln[L(w,b)] = \sum_n  H(p,q) = \sum_n  H(\hat y^n,f(x^n))  \tag{13}</script><h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><p>有了多个Corss Entropy求和作为loss，接下来便是对目标函数/损失函数进行求解，求解目标是使该函数取<strong>最小值</strong>，这里可以使用Gradient Descent方法，牛顿迭代法或者其他启发式优化算法理论上也是可以的。</p>
<h3 id="从二分类到多分类"><a href="#从二分类到多分类" class="headerlink" title="从二分类到多分类"></a>从二分类到多分类</h3><p>上面的推导都是基于<code>binary classification</code>的，如果要进行<code>multiple classification</code>，就需要对上面的情况进行扩展了。这里先给出做法（how），关于<code>sigmoid</code>和<code>softmax</code>分析请<strong>继续</strong>往下看。</p>
<p>首先<script type="math/tex">\vec x</script>不再是只做一次“<script type="math/tex">\vec w \cdot \vec x + b</script>”变换了，而是做<script type="math/tex">n</script>次，<script type="math/tex">n</script>就是需要分类的<strong>类别数</strong>，目标的分布也不再是0-1这样，而是多维的0-1分布，比如：对于三分类问题，<script type="math/tex">x^n \in C_1</script>，则<script type="math/tex">\hat y^n=[1,0,0]</script>，而计算概率的函数也由sigmiod函数转变为softmax函数。</p>
<p><img src="./图1.png" alt="binary VS multiple classification"></p>
<h3 id="方法局限性（limitation）"><a href="#方法局限性（limitation）" class="headerlink" title="方法局限性（limitation）"></a>方法局限性（limitation）</h3><p>逻辑回归的局限性在于，分类时分割线只是<strong>线性的</strong>（直线、平面或者超平面），所以对于非线性分类问题，需要另辟蹊径。<br>到这里，有些同学可能有疑问，明明使用了sigmoid或者softmax函数处理仿射变换结果，怎么会还是线性的？？？ 首先，判断一个函数/系统是不是线性的，可以从输入和输出入手来探究，如图一所示，样本<script type="math/tex">x</script>如果增大，显然<script type="math/tex">z1, z2, z3</script>也会成比例增大，而Softmax和Sigmoid在这里的作用仅仅是将结果转换成0-1之间的数值（概率），所以它还是一个线性分类的场景。</p>
<h3 id="非线性可分的解法"><a href="#非线性可分的解法" class="headerlink" title="非线性可分的解法"></a>非线性可分的解法</h3><p>对于非线性分类问题，可以采用类似于SVM的kernel方法，先对样本做feature map，再进行分类，但是这个map需要依靠人类的智慧去找了，具体的例子可以查看李宏毅的机器学习课。</p>
<h2 id="问答-amp-理解"><a href="#问答-amp-理解" class="headerlink" title="问答&amp;理解"></a>问答&amp;理解</h2><h3 id="为什么使用Cross-Entropy计算loss？"><a href="#为什么使用Cross-Entropy计算loss？" class="headerlink" title="为什么使用Cross Entropy计算loss？"></a>为什么使用Cross Entropy计算loss？</h3><p>这里的机器学习，关键部分可以理解成<strong>学习样本数据的分布</strong>，由于本质上是使用最大似然方法，要求最终得到的概率密度函数（PDF）可以给出<strong>一个新的数据属于某一类的可能性</strong>。而对于<strong>两个分布的相似度</strong>衡量，<code>Cross Entropy</code>目前来看应该还是不二之选。</p>
<h3 id="用Square-Error作为Loss行不行？"><a href="#用Square-Error作为Loss行不行？" class="headerlink" title="用Square Error作为Loss行不行？"></a>用Square Error作为Loss行不行？</h3><p>可以，不过训练过程将极其痛苦，按照李宏毅老师课程中所述，使用传统的二范数距离来度量两个分布的相似度的损失函数将会<strong>非常平坦</strong>，使用梯度下降算法进行优化时，往往不太可能得出比较好的结果。而使用Cross Entropy所得到的损失函数则具有较大的梯度，搜索速度将会比使用Square Error要快很多。</p>
<h3 id="为什么叫Regression？"><a href="#为什么叫Regression？" class="headerlink" title="为什么叫Regression？"></a>为什么叫Regression？</h3><p>对比Linear Regression方法的计算过程，发现Logistics Regression与其一模一样，包括模型参数的更新公式，形式上都是一样的，虽然Logistics Regression是用来所分类的，还是叫逻辑回归比较”亲切”。</p>
<h3 id="和信息论（information-theory）的关系？"><a href="#和信息论（information-theory）的关系？" class="headerlink" title="和信息论（information theory）的关系？"></a>和信息论（information theory）的关系？</h3><p>以上的推导过程都是基于概率的方法，使用了最大似然估计，但是这也完全可以从Information Theory的角度推导出逻辑回归的损失函数，所以，信息论只是认识这个问题的另外一个角度。</p>
<h3 id="和Generation-Model-生成模型的关系"><a href="#和Generation-Model-生成模型的关系" class="headerlink" title="和Generation Model/生成模型的关系"></a>和Generation Model/生成模型的关系</h3><p>逻辑回归是一个判别模型（Discrimination Model），我觉得和生成模型比，它是一个不好进行数学解释的模型。实际上，从生成模型到判别模型，参数被抽样化了，底层所依赖的数学理论也同样被抽象化了，一旦模型复杂了，判别模型就基本解释不了了。<br>一般来说，生成模型会对现有数据做一些假设并对样本数据内的信息进行抽象总结，从而给出模型对样本数据的<strong>新认识</strong>（样本数据里面没有的信息），而判别模型则一般仅会对样本数据之内的信息进行挖掘。所以，对于样本数足够多的时候，使用判别模型一般会由于使用生成模型。</p>
<h3 id="sigmoid-V-S-softmax"><a href="#sigmoid-V-S-softmax" class="headerlink" title="sigmoid V.S. softmax"></a>sigmoid V.S. softmax</h3><p>前面已经介绍过，从二分类到多分类，计算概率的函数就会从sigmoid转换为softmax，但实际上softmax就是sigmoid的“扩展板”。如果对于二分类问题也使用softmax函数，可以推导出概率模型和sigmoid给出的一模一样。</p>
<h3 id="Logistic-Regression-V-S-Neural-Network"><a href="#Logistic-Regression-V-S-Neural-Network" class="headerlink" title="Logistic Regression V.S. Neural Network"></a>Logistic Regression V.S. Neural Network</h3><p>到这里，其实已经很清楚了，DNN中的每一个神经元（Neural）就是一个sigmoid形式的逻辑回归，而对于softmax形式的逻辑回归，它其实就是一个<strong>没有hidden layer</strong>的NN，一层神经网络。<br>此外，之前讨论的<strong>非线性分类问题</strong>的解法，就是对原始样本数据先做特征映射，而NN算法在softmax之前都可以看成一个比较复杂的特征映射，并且这个映射的参数是自学习的。这样，也顺便解释了NN算法好于逻辑回归，并对非线性问题有很好的解决。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li>介绍了逻辑回归算法的推导过程</li>
<li>说明了cross entropy的适用场景</li>
<li>对比了sigmoid和softmax函数</li>
<li>问答的形式给出了一些对逻辑回归算法的理解</li>
</ol>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a href="https://www.youtube.com/watch?v=hSXFuypLukA&amp;list=LLoCiddVQCg9ZvZoEa5d5wYw&amp;index=5&amp;t=0s" target="_blank" rel="noopener">https://www.youtube.com/watch?v=hSXFuypLukA&amp;list=LLoCiddVQCg9ZvZoEa5d5wYw&amp;index=5&amp;t=0s</a></li>
<li><a href="https://www.youtube.com/watch?v=fZAZUYEeIMg&amp;list=LLoCiddVQCg9ZvZoEa5d5wYw&amp;index=1" target="_blank" rel="noopener">https://www.youtube.com/watch?v=fZAZUYEeIMg&amp;list=LLoCiddVQCg9ZvZoEa5d5wYw&amp;index=1</a></li>
<li><a href="https://tdhopper.com/blog/cross-entropy-and-kl-divergence/" target="_blank" rel="noopener">cross-entropy-and-kl-divergence</a></li>
</ol>

      
    </div>

    
      
      

  <div class="post-copyright">
    <p class="copyright-item">
      <span>原文作者: </span>
      <a href="https://kiddie92.github.io">kiddie92</a>
    </p>
    <p class="copyright-item">
      <span>原文链接: </span>
      <a href="https://kiddie92.github.io/2019/06/09/Logistic-Regression/">https://kiddie92.github.io/2019/06/09/Logistic-Regression/</a>
    </p>
    <p class="copyright-item">
      <span>许可协议: </span>
      
      <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用4.0国际许可协议</a>
    </p>
  </div>



      
      
  <div class="post-reward">
    <input type="checkbox" name="reward" id="reward" hidden>
    <label class="reward-button" for="reward">赞赏支持</label>
    <div class="qr-code">
      
      
        <label class="qr-code-image" for="reward">
          <img class="image" src="/images/reward/wechat.png" title="微信打赏">
        </label>
      
      
        <label class="qr-code-image" for="reward">
          <img class="image" src="/images/reward/alipay.png" title="支付宝打赏">
        </label>
      
    </div>
  </div>

    

    
      <footer class="post-footer">
        
          <div class="post-tags">
            
              <a href="/tags/逻辑回归算法/">逻辑回归算法</a>
            
          </div>
        
        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2019/06/10/Matrix-Factorization简介/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">基于矩阵分解的推荐算法</span>
        <span class="prev-text nav-mobile">上一篇</span>
      </a>
    
    
      <a class="next" href="/2019/06/03/k8s-对外服务之ingress/">
        <span class="next-text nav-default">k8s 对外服务之ingress</span>
        <span class="prev-text nav-mobile">下一篇</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>


      </footer>
    

  </article>


          </div>
          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
      </main>

      <footer id="footer" class="footer">

  <div class="social-links">
    
      
        
          <a href="mailto:kiddiezzh@gmail.com" class="iconfont icon-email" title="email"></a>
        
      
    
      
    
      
        
          <a href="https://twitter.com/kiddiezzh" class="iconfont icon-twitter" title="twitter"></a>
        
      
    
      
    
      
    
      
    
      
        
          <a href="https://github.com/kiddie92" class="iconfont icon-github" title="github"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    

    
      <a href="https://feedity.com/github-io/UlJXUVFQUg.rss" class="iconfont icon-rss" title="rss"></a>
    
  </div>



<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://hexo.io/">Hexo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">
    
    &copy; 
     
      2018 - 
    
    2019

    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">kiddie92</span>
  </span>
</div>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>

    
  
  <script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'https://kiddie92.github.io/2019/06/09/Logistic-Regression/';
        this.page.identifier = '2019/06/09/Logistic-Regression/';
        this.page.title = 'Logistic Regression';
    };
    (function() {
    var d = document, s = d.createElement('script');

    s.src = '//https-kiddie92-github-io.disqus.com/embed.js';

    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();  
  </script>

  

  



    
  



  
  





  
    <script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  

  
    <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  

  
    <script type="text/javascript" src="/lib/pjax/jquery.pjax.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/nprogress/nprogress.min.js"></script>
  


    <script type="text/javascript" src="/js/src/even.js?v=2.10.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  </body>
</html>
